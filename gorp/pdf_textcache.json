[["C:\\Users\\mjols\\Python39\\gorp\\testDir\\silly_html_example.pdf", ["silly_html_example\n\nFebruary 5, 2021\n\nhello world\n\nYO DAWG WHAT WE DO???!!!\n\nok, I'm calm, i'm calm</li>\n\n1\n\n"]], ["C:\\Users\\mjols\\Python39\\gorp\\testDir\\walnut\\ln1 proofs.pdf", ["Massachusetts Institute of Technology \n\n6.042J/18.062J, Fall ’05: Mathematics for Computer Science \n\nProf. Albert R. Meyer and Prof. Ronitt Rubinfeld \n\nCourse Notes, Week 1 \n\nSeptember 7 \n\nrevised September 1, 2005, 856 minutes \n\nProofs \n\n1  What is a Proof? \n\nA proof is a method of ascertaining truth. But what constitutes a proof differs among ﬁelds. \n\n•\t Legal truth is ascertained by a jury based on allowable evidence presented at trial. \n\n•\t Authoritative truth is ascertained by a trusted person or organization. \n\n•\t Scientiﬁc truth is hypothesized, and the hypothesis is conﬁrmed or refuted by experiments. \n\n•\t Probable truth is obtained from statistical analysis of sample data. For example, public opin­\n\nion is ascertained by polling a small random sample of people. \n\n•\t Philosophical  proof  involves  careful  exposition  and  persuasion  based  on  consistency  and \n\nplausibility.  The best example is “Cogito ergo sum,”  a Latin sentence that translates as “I \n\nthink, therefore I am.”  It comes from the beginning of a 17th century essay by the Mathe­\n\nmatician/Philospher, Ren´e Descartes, and it is one of the most famous quotes in the world: \n\ndo a web search on the phrase and you will be ﬂooded with hits. \n\nDeducing your existence from the fact that you’re thinking about your existence is a pretty \n\ncool and persuasive­sounding ﬁrst axiom.  However, with just a few more lines of proof in \n\nthis vein, Descartes goes on to conclude that there is an inﬁnitely beneﬁcent God. This ain’t \n\nMath. \n\nMathematics also has a speciﬁc notion of “proof.” \n\nDeﬁnition.  A formal proof  of a proposition is a chain of logical deductions leading to the proposition \n\nfrom a base set of axioms. \n\nThe three key ideas in this deﬁnition are highlighted:  proposition, logical deduction, and axiom. \n\nIn  the  next  sections,  we’ll  discuss  these  three  ideas  along  with  some  basic  ways  of  organizing \n\nproofs. \n\nCopyright © 2005, Prof. Albert R. Meyer. \n\n\n\n", "2 \n\n2  Propositions \n\nCourse Notes, Week 1: Proofs \n\nDeﬁnition.  A proposition is a statement that is either true or false. \n\nThis deﬁnition sounds very general, but it does exclude sentences such as, “Wherefore art thou \n\nRomeo?” and “Give me an A!”. \n\nBut not all propositions are mathematical. For example, “Albert’s wife’s name is ‘Irene’ ” happens \n\nto be true, and could be proved with legal documents and testimony of their children, but it’s not \n\na mathematical statement. \n\nMathematically  meaningful  propositions  must  be  about  well­deﬁned  mathematical  objects  like \n\nnumbers, sets, functions, relations, etc., and they must be stated using mathematically meaning­\n\nful terminology, like ‘AND’ and ‘FORALL’. It’s best to illustrate this with a few examples about \n\nnumbers and planar maps that are all mathematically meaningful. \n\nProposition 2.1.  2 + 3 = 5. \n\nThis proposition is true. \n\nProposition 2.2.  Let p(n) ::=  n2  +  n +  41. \n\n∀n ∈ N.  p(n) is a prime number. \n\nThe symbol ∀ is read “for all”.  The symbol N stands for the set of natural numbers, which are 0, \n\n1, 2, 3, . . . (ask your TA for the complete list).  The period after the N is just a separator between \n\nphrases. \n\nA prime is a natural number greater than one that is not divisible by any other natural number \n\nother than 1 and itself, for example, 2, 3, 5, 7, 11, . . . . \n\nLet’s try some numerical experimentation to check this proposition:  p(0)  = 41  which is prime. \n\np(1)  =  43  which is prime.  p(2)  =  47  which is prime.  p(3)  =  53  which is prime. . . . p(20)  =  461 \n\nwhich is prime. Hmmm, starts to look like a plausible claim. In fact we can keep checking through \n\nn = 39  and conﬁrm that p(39)  =  1601  is prime. \n\nBut  if  n  =  40,  then  p(n)  = 402  +  40  +  41  =  41  · 41,  which  is  not  prime.  Since  the  expression \n\nis not prime for all n,  the proposition is false!  In fact,  it’s not hard to show that no nonconstant \n\npolynomial can map all natural numbers into prime numbers.  The point is that in general you \n\ncan’t check a claim about an inﬁnite set by checking a ﬁnite set of its elements,  no matter how \n\nlarge the ﬁnite set. Here are two even more extreme examples: \n\nProposition 2.3.  a4  + b4  + c4  =  d4  has no solution when a, b, c, d are positive integers. In logical notation, \n\nletting Z+  denote the positive integers, we have \n\n∀a ∈ Z+∀b ∈ Z+∀c ∈ Z+∀d ∈ Z+ .  a 4  +  b4  +  c  =  d4 . \n\n4\n\nStrings of ∀’s like this are usually abbreviated for easier reading: \n\n∀a, b, c, d ∈ Z+ .  a 4  +  b4  +  c  =  d4 . \n\n4\n\nEuler (pronounced “oiler”) conjectured this 1769.  But the proposition was proven false 218 years \n\nlater by Noam Elkies at a liberal arts school up Mass Ave.  He found the solution a  =  95800, b = \n\n217519, c =  414560, d =  422481. \n\n�\n\n�\n\n", "Course Notes, Week 1: Proofs \n\n3 \n\nProposition 2.4.  313(x3  +  y3) =  z3  has no solution when x, y, z  ∈ N. \n\nThis proposition is also false, but the smallest counterexample has more than 1000 digits! \n\nProposition 2.5.  Every map can be colored with 4 colors so that adjacent1  regions have different colors. \n\nThis  proposition  is  true  and  is  known  as  the  “four­color  theorem”.  However,  there  have  been \n\nmany incorrect proofs,  including one that stood for 10 years in the late 19th century before the \n\nmistake was found.  An extremely laborious proof was ﬁnally found in 1976 by mathematicians \n\nAppel and Haken who used a complex computer program to categorize the four­colorable maps; \n\nthe  program  left  a  couple  of  thousand  maps  uncategorized,  and  these  were  checked  by  hand \n\nby  Haken  and  his  assistants—including  his  15­year­old  daughter.  There  was  a  lot  of  debate \n\nabout  whether  this  was  a  legitimate  proof:  the  argument  was  too  big  to  be  checked  without  a \n\ncomputer,  and  no  one  could  guarantee  that  the  computer  calculated  correctly,  nor  did  anyone \n\nhave the energy to recheck the four­colorings of thousands of maps that was done by hand.  Fi­\n\nnally, about ﬁve years ago, a humanly intelligible proof of the four color theorem was found (see \n\nhttp://www.math.gatech.edu/ thomas/FC/fourcolor.html). 2 \n\nProposition 2.6 (Goldbach).  Every even integer greater than 2 is the sum of two primes. \n\nNo one knows whether this proposition is true or false. This is the “Goldbach Conjecture,” which \n\ndates back to 1742. \n\nFor a Computer Scientist, some of the most important questions are about program and system \n\n“correctness”  –  whether  a  program  or  system  does  what  it’s  supposed  to.  Programs  are  noto­\n\nriously buggy,  and there’s a growing community of researchers and practitioners trying to ﬁnd \n\nways to prove program correctness. These efforts have been successful enough in the case of CPU \n\nchips that they are now routinely used by leading chip manufacturers to prove chip correctness \n\nand avoid mistakes like the notorious Intel division bug in the 1990’s. \n\nDeveloping  mathematical  methods  to  verify  programs  and  systems  remains  an  active  research \n\narea. We’ll consider some of these methods later in the course. \n\n3  The Axiomatic Method \n\nThe standard procedure for establishing truth in mathematics was invented by Euclid, a mathe­\n\nmatician working in Alexadria, Egypt around 300 BC. His idea was to begin with ﬁve assumptions \n\nabout geometry, which seemed undeniable based on direct experience.  (For example, “There is \n\na straight line segment between every pair of points.)  Propositions like these that are simply ac­\n\ncepted as true are called axioms. \n\nStarting from these axioms, Euclid established the truth of many additional propositions by pro­\n\nviding “proofs”. A proof  is a sequence of logical deductions from axioms and previously­proved \n\nstatements that concludes with the proposition in question.  You probably wrote many proofs in \n\nhigh school geometry class, and you’ll see a lot more in this course. \n\n1Two regions are adjacent only when they share a boundary segment of positive length. They are not considered to \n\nbe adjacent if their boundaries meet only at a few points. \n\n2The  story  of  the  four­color  proof  is  told  in  a  well­reviewed  recent  popular  (non­technical)  book:  “Four  Colors \n\nSufﬁce. How the Map Problem was Solved.” Robin Wilson. Princeton Univ. Press, 2003, 276pp. ISBN 0­691­11533­8. \n\n\n", "4\t\n\nCourse Notes, Week 1: Proofs \n\nThere are several common terms for a proposition that has been proved.  The different terms hint \n\nat the role of the proposition within a larger body of work. \n\n•\t Important propositions are called theorems. \n\n•\t A lemma is a preliminary proposition useful for proving later propositions. \n\n•\t A corollary is an afterthought, a proposition that follows in just a few logical steps from a \n\ntheorem. \n\nThe deﬁnitions are not precise. In fact, sometimes a good lemma turns out to be far more important \n\nthan the theorem it was originally used to prove. \n\nEuclid’s axiom­and­proof approach, now called the axiomatic method, is the foundation for math­\n\nematics today.  In fact, there are just a handful of axioms, called ZFC, which, together with a few \n\nlogical deduction rules, appear to be sufﬁcient to derive essentially all of mathematics. \n\n3.1  Our Axioms \n\nThe ZFC axioms are important in studying and justifying the foundations of Mathematics.  But \n\nfor practical purposes, they are much too primitive— by one reckoning, proving that 2 + 2 =  4 \n\nrequires more than 20,000 steps! So instead of starting with ZFC, we’re going to take a huge set of \n\naxioms as our foundation: we’ll accept all familiar facts from high school math! \n\nThis will give us a quick launch, but you will ﬁnd this imprecise speciﬁcation of the axioms trou­\n\nbling at times.  For example, in the midst of a proof, you may ﬁnd yourself wondering, “Must I \n\nprove this little fact or can I take it as an axiom?” Feel free to ask for guidance, but really there is no \n\nabsolute answer.  Just be upfront about what you’re assuming, and don’t try to evade homework \n\nand exam problems by declaring everything an axiom! \n\n3.2  Proofs in Practice \n\nIn principle, a proof can be any sequence of logical deductions from axioms and previously­proved \n\nstatements that concludes with the proposition in question.  This freedom in constructing a proof \n\ncan seem overwhelming at ﬁrst. How do you even start a proof? \n\nHere’s the good news: many proofs follow one of a handful of standard templates. Proofs all differ \n\nin the details, of course, but these templates at least provide you with an outline to ﬁll in. We’ll go \n\nthrough several of these standard patterns, pointing out the basic idea and common pitfalls and \n\ngiving some examples. Many of these templates ﬁt together; one may give you a top­level outline \n\nwhile others help you at the next level of detail.  And we’ll show you other, more sophisticated \n\nproof techniques later on. \n\nThe recipes below are very speciﬁc at times, telling you exactly which words to write down on \n\nyour piece of paper.  You’re certainly free to say things your own way instead; we’re just giving \n\nyou something you could say so that you’re never at a complete loss. \n\n", "Course Notes, Week 1: Proofs\n\n\n5 \n\nThe ZFC Axioms \n\nFor  the  record,  we  list  the  axioms  of  Zermelo­Frankel  Set  Theory. \n\nEssentially  all  of  mathematics  can  be  derived  from  these  axioms  to­\n\ngether with a few logical deduction rules. \n\nExtensionality.  Two sets are equal if they have the same members.  In \n\nformal logical notation, this would be stated as: \n\n(∀ z.  (z  ∈ x \n\n←→ z  ∈ y))  −→ x \n\n=  y. \n\nPairing.  For any two sets x  and y, there is a set, { x,  y} , with x  and y  as \n\nits only elements. \n\nUnion.  The union of a collection, z, of sets is also a set. \n\n∃ u∀ x.  (∃ y.  x  ∈ y  ∧ y  ∈ z) ←→ x  ∈ u. \n\nInﬁnity.  There is an inﬁnite set;  speciﬁcally,  a nonempty set,  x,  such \n\nthat for any set y  ∈ x, the set { y} is also a member of x \n\nSubset.  Given any set, x, and any proposition P (y), there is a set con­\n\ntaining precisely those elements y  ∈ x  for which P (y) holds. \n\nPower Set.  All the subsets of a set form another set. \n\nReplacement.  The image of a set under a function is a set. \n\nFoundation.  For every non­empty set, x, there is a set y  ∈ x  such that \n\nx  and y  are disjoint. (In particular, this axiom prevents a set from \n\nbeing a member of itself.) \n\nChoice.  We  can  choose  one  element  from  each  set  in  a  collection  of \n\nnonempty  sets.  More  precisely,  if  f  is  a  function  on  a  set,  and \n\nthe  result  of  applying  f  to  any  element  in  the  set  is  always \n\na  nonempty  set,  then  there  is  a  “choice”  function  g  such  that \n\ng(y) ∈ y  for every y  in the set. \n\nWe’re not going to be working with the ZFC axioms in this course. We \n\njust thought you might like to see them. \n\n\n\n\n\n", "6 \n\nCourse Notes, Week 1: Proofs \n\n4  Proving an Implication \n\nAn enormous number of mathematical claims have the form “If P , then Q” or, equivalently, “P \n\nimplies Q”. Here are some examples: \n\n•  (Quadratic Formula) If ax2  + bx  + c  = 0 and a  = 0, then x  = (−b  ±\n\nb2  − 4ac)/2a. \n\n√\n\n•  (Goldbach’s Conjecture) If n  is an even integer greater than 2, then n  is a sum of two primes. \n\n•  If 0 ≤ x  ≤ 2, then −x3  + 4x  + 1 >  0. \n\nThere are a couple standard methods for proving an implication. \n\n4.1  Method #1 \n\nIn order to prove that P  implies Q: \n\n1.  Write, “Assume P .” \n\n2.  Show that Q  logically follows. \n\nExample \n\nTheorem 4.1.  If 0 ≤ x  ≤ 2, then −x3  + 4x  + 1 >  0. \n\nBefore we write a proof of this theorem, we have to do some scratchwork to ﬁgure out why it is \n\ntrue. \n\nThe inequality certainly holds for x  =  0; then the left side is equal to 1 and 1 >  0. As x  grows, the \n\n4x  term (which is positive) initially seems to have greater magnitude than −x3  (which is negative). \n\nFor example, when x  =  1, we have 4x  =  4, but −x3  =  −1 only.  In fact, it looks like −x3  doesn’t \n\nbegin  to  dominate  until  x  >  2.  So  it  seems  the  −x3  + 4x  part  should  be  nonnegative  for  all  x \n\nbetween 0 and 2, which would imply that −x3  + 4x  + 1 is positive. \n\nSo  far,  so  good.  But  we  still  have  to  replace  all  those  “seems  like”  phrases  with  solid,  logical \n\narguments.  We can get a better handle on the critical −x3  + 4x  part by factoring it, which is not \n\ntoo hard: \n\n−x 3  + 4x  = x(2 − x)(2 + x) \n\nAha!  For x  between 0 and 2, all of the terms on the right side are nonnegative.  And a product \n\nof nonnegative terms is also nonnegative. Let’s organize this blizzard of observations into a clean \n\nproof. \n\n2\n\nProof.  Assume 0 ≤ x  ≤ 2.  Then x, 2 − x, and 2 + x  are all nonnegative.  Therefore, the product of \n\nthese terms is also nonnegative. Adding 1 to this product gives a positive number, so: \n\nMultiplying out on the left side proves that \n\nx(2 − x)(2 + x) + 1 >  0 \n\n−x 3  + 4x  + 1 >  0 \n\nas claimed. \n\n\n\n\n\n\n�\n\n", "Course Notes, Week 1: Proofs\t\n\n7 \n\nThere are a couple points here that apply to all proofs: \n\n•\t You’ll often need to do some scratchwork while you’re trying to ﬁgure out the logical steps \n\nof a proof. Your scratchwork can be as disorganized as you like— full of dead­ends, strange \n\ndiagrams,  obscene words,  whatever.  But keep your scratchwork separate from your ﬁnal \n\nproof, which should be clear and concise. \n\n•\t Proofs typically begin with the word “Proof” and end with some sort of doohickey like � or \n\n“q.e.d”. The only purpose for these conventions is to clarify where proofs begin and end. \n\n4.2  Method #2 ­ Prove the Contrapositive \n\nAn implication (“P  implies Q”) is logically equivalent to its contrapositive “not Q  implies not P ”; \n\nproving one is as good as proving the other.  And often proving the contrapositive is easier than \n\nproving the original statement. If so, then you can proceed as follows: \n\n1.  Write, “We prove the contrapositive:” and then state the contrapositive. \n\n2.  Proceed as in Method #1. \n\nExample \n\nTheorem 4.2.  If r  is irrational, then \n\nr  is also irrational. \n\n√\n\nRecall that rational numbers are equal to a ratio of integers and irrational numbers are not. So we \n\nmust show that if r  is not a ratio of integers, then \n\nr  is also not a ratio of integers.  That’s pretty \n\nconvoluted!  We can eliminate both “not”’s and make the proof straightforward by considering \n\nthe contrapositive instead. \n\n√\n\nProof.  We prove the contrapositive: if \n\nr  is rational, then r  is rational. \n\nAssume that \n\nr  is rational. Then there exists integers a  and b  such that: \n\n√\n\n√\n\n√\n\nr  = \n\na\n\nb \n\nr  = \n\n2a\n\nb2 \n\nSquaring both sides gives: \n\nSince a2  and b2  are integers, r  is also rational. \n\n5  Proving an “If and Only If” \n\nMany mathematical theorems assert that two statements are logically equivalent; that is, one holds \n\nif and only if the other does. Here are some examples: \n\n•\t An integer is a multiple of 3 if and only if the sum of its digits is a multiple of 3. \n\n•\t Two triangles have the same side lengths if and only if all angles are the same. \n\n•\t A positive integer p  ≥ 2 is prime if and only if 1 + (p − 1) · (p − 2)\n\n· · ·\n\n3 2 1 is a multiple of p.\n\n·\n\n·\n\n\n\n\n\n\n\n\n\n\n\n\n", "8 \n\nCourse Notes, Week 1: Proofs \n\n5.1  Method #1: Prove Each Statement Implies the Other \n\nThe  statement  “P  if  and  only  if  Q”  is  equivalent  to  the  two  statements  “P  implies  Q”  and  “Q \n\nimplies P ”. So you can prove an “if and only if” by proving two implications: \n\n1.  Write, “We prove P  implies Q and vice­versa.” \n\n2.  Write, “First, we show P  implies Q.” Do this by one of the methods in Section 4. \n\n3.  Write, “Now, we show Q implies P .” Again, do this by one of the methods in Section 4. \n\nExample \n\nTwo sets are deﬁned to be equal if they contain the same elements; that is, X  =  Y  means z  ∈ X if \n\nand only if z  ∈  Y .  (This is actually the ﬁrst of the ZFC axioms.)  So set equality theorems can be \n\nstated and proved as “if and only if” theorems. \n\nTheorem 5.1 (DeMorgan’s Law for Sets).  Let A, B, and C be sets. Then: \n\nA ∩ (B ∪ C) =  (A ∩ B) ∪ (A ∩ C) \n\nProof.  We show z  ∈ A ∩ (B ∪ C) implies z  ∈ (A ∩ B) ∪ (A ∩ C) and vice­versa. \n\nFirst, we show z  ∈ A ∩ (B ∪ C) implies z  ∈ (A ∩ B) ∪ (A ∩ C). Assume z  ∈ A ∩ (B ∪ C). Then z is in \n\nA and z is also in B or C. Thus, z is in either A ∩ B or A ∩ C, which implies z  ∈ (A ∩ B) ∪ (A ∩ C). \n\nNow, we show z  ∈ (A ∩ B) ∪ (A ∩ C) implies z  ∈ A ∩ (B ∪ C).  Assume z  ∈ (A ∩ B) ∪ (A ∩ C). \n\nThen z is in both A and B or else z is in both A and C. Thus, z is in A and z is also in B or C. This \n\nimplies z  ∈ A ∩ (B ∪ C). \n\n5.2  Method #2: Construct a Chain of Iffs \n\nIn order to prove that P  is true if and only if Q is true: \n\n1.  Write, “We construct a chain of if­and­only­if implications.” \n\n2.  Prove P  is equivalent to a second statement which is equivalent to a third staement and so \n\nforth until you reach Q. \n\nThis method is generally more difﬁcult than the ﬁrst, but the result can be a short, elegant proof. \n\nExample \n\nThe standard deviation of a sequence of values x1, x2, . . . , xn  is deﬁned to be: \n\n� \n\n(x1  − µ)2  + (x1  − µ)2  + . . . + (xn  − µ)\n\n2\n\nwhere µ is the average of the values: \n\nµ = \n\nx1  + x2  + . . . + xn \n\nn \n\n\n\n\n\n\n\n", "Course Notes, Week 1: Proofs \n\n9 \n\nTheorem 5.2.  The standard deviation of a sequence of values x1, . . . , xn  is zero if and only if all the values \n\nare equal to the mean. \n\nFor example, the standard deviation of test scores is zero if and only if everyone scored exactly the \n\nclass average. \n\nProof.  We construct a chain of “if and only if” implications.  The standard deviation of x1, . . . , xn \n\nis zero if and only if: \n\n� \n\n(x1  − µ)2  + (x1  − µ)2  + . . . + (xn  − µ)2  = 0 \n\nwhere µ is the average of x1, . . . , xn. This equation holds if and only if \n\n(x1  − µ)2  + (x1  − µ)2  + . . . + (xn  − µ)2  =  0 \n\nsince zero is the only number whose square root is zero.  Every term in this equation is nonnega­\n\ntive, so this equation holds if and only every term is actually 0. But this is true if and only if every \n\nvalue xi  is equal to the mean µ. \n\nProblem 1.  Reformulate the proof DeMorgan’s Law for Sets as a chain of if­and­only­if implica­\n\ntions. \n\n6  How to Write Good Proofs \n\nThe  purpose  of  a  proof  is  to  provide  the  reader  with  deﬁnitive  evidence  of  an  assertion’s  truth. \n\nTo serve this purpose effectively, more is required of a proof than just logical correctness:  a good \n\nproof  must  also  be  clear.  These  goals  are  usually  complimentary;  a  well­written  proof  is  more \n\nlikely to be a correct proof, since mistakes are harder to hide. \n\nIn practice, the notion of proof is a moving target.  Proofs in a professional research journal are \n\ngenerally unintelligible to all but a few experts who know all the lemmas and theorems assumed, \n\noften without explicit mention, in the proof.  And proofs in the ﬁrst weeks of a beginning course \n\nlike 6.042 would be regarded as tediously long­winded by a professional mathematician.  In fact, \n\nwhat we accept as a good proof later in the term will be different from what we consider good \n\nproofs in the ﬁrst couple of weeks of 6.042. But even so, we can offer some general tips on writing \n\ngood proofs: \n\nState your game plan.  A good proof begins by explaining the general line of reasoning, e.g. “We \n\nuse case analysis” or “We argue by contradiction”.  This creates a rough mental picture into \n\nwhich the reader can ﬁt the subsequent details. \n\nKeep a linear ﬂow.  We sometimes see proofs that are like mathematical mosaics, with juicy tidbits \n\nof reasoning sprinkled across the page. This is not good. The steps of your argument should \n\nfollow one another in a sequential order. \n\nA proof is an essay, not a calculation.  Many students initially write proofs the way they compute \n\nintegrals.  The  result  is  a  long  sequence  of  expressions  without  explantion.  This  is  bad. \n\nA  good  proof  usually  looks  like  an  essay  with  some  equations  thrown  in.  Use  complete \n\nsentences. \n\n\n\n\n\n\n", "10 \n\nCourse Notes, Week 1: Proofs \n\nAvoid excessive symbolism.  Your  reader  is  probably  good  at  understanding  words,  but  much \n\nless skilled at reading arcane mathematical symbols.  So use words where you reasonably \n\ncan. \n\nSimplify.  Long, complicated proofs take the reader more time and effort to understand and can \n\nmore easily conceal errors. So a proof with fewer logical steps is a better proof. \n\nIntroduce notation thoughtfully.  Sometimes an argument can be greatly simpliﬁed by introduc­\n\ning a variable,  devising a special notation,  or deﬁning a new term.  But do this sparingly \n\nsince you’re requiring the reader to remember all that new stuff. And remember to actually \n\ndeﬁne the meanings of new variables, terms, or notations; don’t just start using them! \n\nStructure long proofs.  Long programs are usually broken into a heirarchy of smaller procedures. \n\nLong proofs are much the same.  Facts needed in your proof that are easily stated, but not \n\nreadily  proved  are  best  pulled  out  and  proved  in  preliminary  lemmas.  Also,  if  you  are \n\nrepeating essentially the same argument over and over,  try to capture that argument in a \n\ngeneral lemma, which you can cite repeatedly instead. \n\nDon’t bully.  Don’t use phrases like “clearly” or “obviously” in an attempt to bully the reader into \n\naccepting something which you’re having trouble proving.  Also, go on the alert whenever \n\nyou see one of these phrases is someone else’s proof. \n\nFinish.  At some point in a proof, you’ll have established all the essential facts you need. Resist the \n\ntemptation to quit and leave the reader to draw the “obvious” conclusion.  What is obvious \n\nto you as the author is not likely to be obvious to the reader. Instead, tie everything together \n\nyourself and explain why the original claim follows. \n\nThe analogy between good proofs and good programs extends beyond structure.  The same rig­\n\norous  thinking  needed  for  proofs  is  essential  in  the  design  of  critical  computer  system.  When \n\nalgorithms and protocols only “mostly work” due to reliance on hand­waving arguments, the re­\n\nsults can range from problematic to catastrophic. An early example was the Therac 25, a machine \n\nthat provided radiation therapy to cancer victims, but occasionally killed them with massive over­\n\ndoses due to a software race condition.  More recently, in August 2004, a single faulty command \n\nto a computer system used by United and American Airlines grounded the entire ﬂeet of both \n\ncompanies— and all their passengers! \n\nIt is a certainty that we’ll all one day be at the mercy of critical computer systems designed by \n\nyou and your classmates. So we really hope that you’ll develop the ability to formulate rock­solid \n\nlogical arguments that a system actually does what you think it does! \n\n7  Propositional Formulas \n\nIt’s really sort of amazing that people manage to communicate in the English language.  Here are \n\nsome typical sentences: \n\n1.  “You may have cake or you may have ice cream.” \n\n2.  “If pigs can ﬂy, then you can understand the Chernoff bound.” \n\n", "Course Notes, Week 1: Proofs \n\n11 \n\n3.  “If you can solve any problem we come up with, then you get an A for the course.” \n\n4.  “Every American has a dream.” \n\nWhat precisely do these sentences mean? Can you have both cake and ice cream or must you choose \n\njust one desert?  If the second sentence is true, then is the Chernoff bound incomprehensible?  If \n\nyou can solve some problems we come up with but not all, then do you get an A for the course? \n\nAnd can you still get an A even if you can’t solve any of the problems?  Does the last sentence \n\nimply that all Americans have the same dream or might they each have a different dream? \n\nSome  uncertainty  is  tolerable  in  normal  conversation.  But  when  we  need  to  formulate  ideas \n\nprecisely—  as  in  mathematics—  the  ambiguities  inherent  in  everyday  language  become  a  real \n\nproblem.  We can’t hope to make an exact argument if we’re not sure exactly what the individual \n\nwords mean. (And, not to alarm you, but it is possible that we’ll be making an awful lot of exacting \n\nmathematical arguments in the weeks ahead.)  So before we start into mathematics, we need to \n\ninvestigate the problem of how to talk about mathematics. \n\nTo  get  around  the  ambiguity  of  English,  mathematicians  have  devised  a  special  mini­language \n\nfor  talking  about  logical  relationships.  This  language  mostly  uses  ordinary  English  words  and \n\nphrases such as “or”, “implies”, and “for all”.  But mathematicians endow these words with def­\n\ninitions more precise than those found in an ordinary dictionary.  Without knowing these deﬁni­\n\ntions, you could sort of read this language, but you would miss all the subtleties and sometimes \n\nhave trouble following along. \n\nSurprisingly, in the midst of learning the language of logic, we’ll come across the most important \n\nopen problem in computer science— a problem whose solution could change the world. \n\n7.1  Combining Propositions \n\nIn  English,  we  can  modify,  combine,  and  relate  propositions  with  words  such  as  “not”,  “and”, \n\n“or”, “implies”, and “if­then”. For example, we can combine three propositions into one like this: \n\nIf all humans are mortal and all Greeks are human, then all Greeks are mortal. \n\nFor  the  next  while,  we  won’t  be  much  concerned  with  the  internals  of  propositions—  whether \n\nthey involve mathematics or Greek mortality— but rather with how propositions are combined \n\nand related.  So we’ll frequently use variables such as P  and Q  in place of speciﬁc propositions \n\nsuch  as  “All  humans  are  mortal”  and  “2 + 3 =  5”.  The  understanding  is  that  these  variables, \n\nlike propositions, can take on only the values T(true) and F(false).  Such true/false variables are \n\nsometimes called Boolean variables after their inventor, George— you guessed it— Boole. \n\n7.1.1  “Not”, “And” and “Or” \n\nWe  can  precisely  deﬁne  these  special  words  using  truth  tables.  For  example,  if  P  denotes  an \n\narbitrary proposition, then the truth of the proposition “not P ” is deﬁned by the following truth \n\ntable: \n\nP  not P \n\nT \n\nF \n\nF \n\nT \n\n\n\n\n\n", "12 \n\nCourse Notes, Week 1: Proofs \n\nThe  ﬁrst  row  of  the  table  indicates  that  when  proposition  P is  true,  the  proposition  “not  P ”  is \n\nfalse (F). The second line indicates that when P is false, “not P ” is true. This is probably what you \n\nwould expect. \n\nIn general, a truth table indicates the true/false value of a proposition for each possible setting of \n\nthe variables.  For example, the truth table for the proposition “P and Q” has four lines, since the \n\ntwo variables can be set in four different ways: \n\nAccording to this table, the proposition “P and Q” is true only when P and Q are both true. This \n\nis probably reﬂects the way you think about the word “and”. \n\nThere is a subtlety in the truth table for “P or Q”: \n\nThis says that “P or Q” is true when P is true, Q is true, or both are true.  This isn’t always the \n\nintended meaning of “or” in everyday speech, but this is the standard deﬁnition in mathematical \n\nwriting.  So if a mathematician says, “You may have cake or your may have ice cream”, then you \n\ncould have both. \n\n7.1.2  “Implies” \n\nThe least intuitive connecting word is “implies”.  Mathematicians regard the propositions “P im­\n\nplies  Q”  and  “if  P then  Q”  as  synonymous,  so  both  have  the  same  truth  table.  (The  lines  are \n\nnumbered so we can refer to the them later.) \n\nP Q\n\n\nT  T \n\nT  F \n\nF  T \n\nF  F \n\nP and Q\n\n\nT \n\nF \n\nF \n\nF \n\nP Q\n\nT  T \n\nT  F \n\nF  T \n\nF  F \n\nP or Q \n\nT \n\nT \n\nT \n\nF \n\nP implies Q,\n\n\nif P then Q\n\n\nP Q\n\n\n1. T  T \n\n2. T  F \n\n3. F  T \n\n4. F  F \n\nT \n\nF \n\nT \n\nT \n\nLet’s experiment with this deﬁnition. For example, is the following proposition true or false? \n\n“If Goldbach’s Conjecture is true, then x2  ≥ 0 for every real number x.” \n\nNow, we told you before that no one knows whether Goldbach’s Conjecture is true or false.  But\n\n\nthat doesn’t prevent you from answering the question!  This proposition has the form P −→  Q\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "Course Notes, Week 1: Proofs \n\n13 \n\nwhere P is “Goldbach’s Conjecture is true” and Q is “x2  ≥ 0  for every real number x”. Since Q is \n\ndeﬁnitely true, we’re on either line 1 or line 3 of the truth table.  Either way, the proposition as a \n\nwhole is true! \n\nOne of our original examples demonstrates an even stranger side of implications. \n\n“If pigs ﬂy, then you can understand the Chernoff bound.” \n\nDon’t take this as an insult;  we just need to ﬁgure out whether this proposition is true or false. \n\nCuriously,  the answer has nothing to do with whether or not you can understand the Chernoff \n\nbound.  Pigs  do not ﬂy,  so  we’re on either line 3  or line 4  of the truth  table.  In both cases,  the \n\nproposition is true! \n\nIn contrast, here’s an example of a false implication: \n\n“If the moon shines white, then the moon is made of white cheddar.” \n\nYes, the moon shines white.  But, no, the moon is not made of white cheddar cheese.  So we’re on \n\nline 2 of the truth table, and the proposition is false. \n\nThe truth table for implications can be summarized in words as follows: \n\nAn implication is true when the if­part is false or the then­part is true. \n\nThis  sentence  is  worth  remembering;  a  large  fraction  of  all  mathematical  statements  are  of  the \n\nif­then form! \n\n7.1.3  “If and Only If” \n\nMathematicians commonly join propositions in one additional way that doesn’t arise in ordinary \n\nspeech. The proposition “P if and only if Q” asserts that P and Q are logically equivalent; that is, \n\neither both are true or both are false. \n\nP Q\n\n\nT  T \n\nT  F \n\nF  T \n\nF  F \n\nP if and only if Q\n\n\nT \n\nF \n\nF \n\nT \n\nThe following if­and­only­if statement is true for every real number x: \n\n“x2  − 4  ≥ 0  if and only if  x ≥ 2” \n\n| \n\n|\n\nFor some values of x, both inequalities are true. For other values of x, neither inequality is true . In \n\nevery case, however, the proposition as a whole is true. \n\nThe phrase “if and only if” comes up so often that it is often abbreviated “iff”. \n\n\n\n\n\n\n\n", "14 \n\nCourse Notes, Week 1: Proofs \n\n7.2  Propositional Logic in Computer Programs \n\nPropositions and logical connectives arise all the time in computer programs.  For example, con­\n\nsider the following snippet, which could be either C, C++, or Java: \n\nif  (  x  >  0  ||  (x  <=  0  &&  y  >  100)  )\n\n\n. . . \n\n(further instructions) \n\nThe symbol || denotes “or”, and the symbol && denotes “and”. The further instructions are carried \n\nout only if the proposition following the word if is true. On closer inspection, this big expression \n\nis  built  from  two  simpler  propositions.  Let  A be  the  proposition  that  x > 0,  and  let  B be  the \n\nproposition that y  >  100. Then we can rewrite the condition this way: \n\nA truth table reveals that this complicated expression is logically equivalent to “A or B”. \n\nA or ((not A) and B) \n\nA B\n\nT T \n\nT F \n\nF T \n\nF F \n\nA or ((not A) and B)  A or B \n\nT \n\nT \n\nT \n\nF \n\nT \n\nT \n\nT \n\nF \n\nThis means that we can simplify the code snippet without changing the program’s behavior: \n\nif  (  x  >  0  ||  y  >  100  )\n\n\n(further instructions) \n\nRewriting a logical expression involving many variables in the simplest form is both difﬁcult and \n\nimportant. Simplifying expressions in software might slightly increase the speed of your program. \n\nBut, more signiﬁcantly, chip designers face essentially the same challenge.  However, instead of \n\nminimizing && and || symbols in a program, their job is to minimize the number of analogous \n\nphyscial  devices  on  a  chip.  The  payoff  is  potentially  enormous:  a  chip  with  fewer  devices  is \n\nsmaller, consumes less power, has a lower defect rate, and is cheaper to manufacture. \n\n7.3  A Cryptic Notation \n\nProgramming languages use symbols like &&  and ! in place of words like “and” and “not”. Math­\n\nematicians have devised their own cryptic symbols to represent these words, which are summa­\n\nrized in the table below. \n\nEnglish \n\n“not P ” \n\n“P and Q” \n\n“P or Q” \n\n“P\n\nimplies Q” or “if P then Q” \n\n“P if and only if Q” \n\nCryptic Notation \n\n¬ P \n\n(alternatively, P ) \n\nP ∧ Q \n\nP ∨ Q \n\nP −→ Q \n\nP ←→ Q \n\n\n\n\n\n\n\n\n\n\n\n\n\n", "Course Notes, Week 1: Proofs \n\n15 \n\nFor example, using this notation, “If P and not Q, then R” would be written: \n\n(P ∧ ¬ Q) −→ R \n\nThis symbolic language is helpful for writing complicated logical expressions compactly.  But in \n\nmost  contexts  ordinary  words  such  as  “or”  and  “implies”  are  much  easier  to  understand  than \n\nsymbols such as ∨ and −→ .  So we’ll use this symbolic language sparingly, and we advise you to \n\ndo the same. \n\n7.4  Logically Equivalent Implications \n\nAre these two sentences saying the same thing? \n\nIf I am hungry, then I am grumpy.\n\n\nIf I am not grumpy, then I am not hungry.\n\n\nWe can settle the issue by recasting both sentences in terms of propositional logic.  Let P be the \n\nproposition “I am hungry”, and let Q be “I am grumpy”.  The ﬁrst sentence says “P implies Q” \n\nand the second says “(not Q) implies (not P )”.  We can compare these two statements in a truth \n\ntable: \n\nQ P implies Q (not Q) implies (not P ) \n\nP\n\nT T \n\nT F \n\nF T \n\nF F \n\nT \n\nF \n\nT \n\nT \n\nSure enough, the two statements are precisely equivalent. In general, “(not Q) implies (not P )” is \n\ncalled the contrapositive of “P implies Q”.  And, as we’ve just shown, the two are just different \n\nways of saying the same thing.  This equivalence is mildly useful in programming, mathematical \n\narguments, and even everyday speech, because you can always pick whichever of the two is easier \n\nto say or write. \n\nIn contrast, the converse of “P implies Q” is the statement “Q implies P ”. In terms of our example, \n\nthe converse is: \n\nThis sounds like a rather different contention, and a truth table conﬁrms this suspicion: \n\nIf I am grumpy, then I am hungry. \n\nQ P implies Q Q implies P \n\nP\n\nT T \n\nT F \n\nF T \n\nF F \n\nT \n\nF \n\nT \n\nT \n\nThus, an implication is logically equivalent to its contrapositive, but is not equivalent to its con­\n\nverse. \n\nOne ﬁnal relationship: an implication and its converse together are equivalent to an if and only if \n\nstatement, speciﬁcally, to these two statements together. For example, \n\nT \n\nF \n\nT \n\nT \n\nT \n\nT \n\nF \n\nT \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "16 \n\nCourse Notes, Week 1: Proofs\n\n\nIf I am grumpy, then I am hungry. \n\nIf I am hungry, then I am grumpy. \n\nare equivalent to the single statement: \n\nI am grumpy if and only if I am hungry. \n\nOnce again, we can verify this with a truth table: \n\nQ (P implies Q) and (Q implies P )  Q if and only if P \n\nP\n\nT T \n\nT F \n\nF T \n\nF F \n\nT \n\nF \n\nF \n\nT \n\nT \n\nF \n\nF \n\nT \n\n8  Logical Deductions \n\nLogical deductions or inference rules are used to prove new propositions using previously proved \n\nones. \n\nA fundamental inference rule is modus ponens.  This rule says that a proof of P together with a \n\nproof of P −→ Q is a proof of Q. \n\nInference rules are sometimes written in a funny notation. For example, modus ponens is written: \n\nWhen the statements above the line, called the antecedents, are proved, then we can consider the \n\nstatement below the line, called the conclusion or consequent, to also be proved. \n\nA key requirement of an inference rule is that it must be sound:  any assignment of truth values \n\nthat makes all the antecedents true must also make the consequent true.  So it we start off with \n\ntrue axioms and apply sound inference rules, everything we prove will also be true. \n\nThere are many other natural, sound inference rules, for example: \n\nP, P −→ Q \n\nQ \n\nP −→ Q, Q −→ R \n\nP −→ R \n\n¬ P −→ Q, ¬ Q \n\nP \n\n¬ Q\n\n¬  −→ \n\nP\n\nQ −→ P \n\nRule. \n\nRule. \n\nRule. \n\nRule. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "Course Notes, Week 1: Proofs\n\n\n17 \n\nSAT \n\nA proposition is satisﬁable if some setting of the variables makes \n\nthe  proposition  true.  For  example,  P  ∧ ¬Q  is  satisﬁable  because  the \n\nexpression is true when P  is true and Q  is false.  On the other hand, \n\nP  ∧ ¬P  is not satisﬁable because the expression as a whole is false for \n\nboth settings of P . But determining whether or not a more complicated \n\nproposition is satisﬁable is not so easy. How about this one? \n\n(P  ∨ Q ∨ R) ∧ (¬P  ∨ ¬Q) ∧ (¬P  ∨ ¬R) ∧ (¬R  ∨ ¬Q) \n\nThe general problem of deciding whether a proposition is satisﬁable \n\nis  called  SAT .  One  approach  to  SAT  is  to  construct  a  truth  table  and \n\ncheck whether or not a T  ever appears.  But this approach is not very \n\nefﬁcient; a proposition with n  variables has a truth table with 2n  lines. \n\nFor a proposition with just 30 variables, that’s already over a billion! \n\nIs there an efﬁcient solution to SAT? Is there some ingenious proce­\n\ndure  that  quickly  determines  whether  any  given  proposition  is  satiﬁ­\n\nable or not? No one knows. And an awful lot hangs on the answer. An \n\nefﬁcient solution to SAT would immediately imply efﬁcient solutions \n\nto many, many other important problems involving packing, schedul­\n\ning,  routing,  and circuit veriﬁcation.  This sounds fantastic,  but there \n\nwould also be worldwide chaos.  Decrypting coded messages would \n\nalso  become  an  easy  task  (for  most  codes).  Online  ﬁnancial  transac­\n\ntions would be insecure and secret communications could be read by \n\neveryone. \n\nAt present, though, researchers are completely stuck.  No one has a \n\ngood idea how to either solve SAT more efﬁciently or to prove that no \n\nefﬁcient solution exists.  This is the outstanding unanswered question \n\nin computer science. \n\n\n\n\n\n", "18 \n\nRule. \n\nOn the other hand, \n\n¬ P  −→ ¬ Q \n\nP  −→ Q \n\nCourse Notes, Week 1: Proofs \n\nis not sound: if P  is assigned T and Q is assigned F, then the antecedent is true and the consequent \n\nis not. \n\nProblem 2.  Prove that a propositional inference rule is sound iff the conjunction (AND) of all its \n\nantecedents implies its consequent. \n\nAs with axioms, we will not be too formal about the set of legal inference rules.  Each step in a \n\nproof should be clear and “logical”; in particular, you should state what previously proved facts \n\nare used to derive each new conclusion. \n\n\n"]], ["C:\\Users\\mjols\\Documents\\DS UWEC courses\\730 Big Data\\Course-Syllabus-for-DS-730.pdf", ["Course Syllabus for DS730: Big Data - High-Performance \n\nComputing \n\n \n\nNOTE: This syllabus document contains the basic information of this course. The most \n\ncurrent syllabus is available in the full course. \n\nCourse Description \n\nThis course teaches you how to process large datasets efficiently. You will be \n\nintroduced to non-relational databases. You will learn algorithms that allow for the \n\ndistributed processing of large data sets across clusters.  \n\nCourse Objectives By the end of this course, you will be able to: \n\n● \n\nImplement algorithms that allow for the distributed processing of large data sets \n\nacross computing clusters. \n\n●  Create parallel algorithms that can process large datasets. \n\n●  Process large datasets efficiently. \n\nYour mastery of course content is assessed using a variety of methods: \n\n \n\nActivity  Percentage of Grade \n\n \n\n \n\n \n\nGrading Policy \n\nActivities  30% \n\nProjects  70% \n\nTotal \n\n100% \n\n \n\nFinal grades are assigned using the following scale: \n\n \n\n \n\nA  90-100% \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "B  80-89% \n\nC  70-79% \n\nD  60-69% \n\nF  At or below 59% \n\n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n"]], ["C:\\Users\\mjols\\Documents\\DS UWEC courses\\730 Big Data\\ds730_lesson1-mapreduce_example.pdf", ["DS 730\n\nBig Data: High Performance Computing\n\nMapReduce Examples\n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nImportant note: Transcripts are not substitutes for textbook assignments. \n\n1 \n\n\n\n\n\n", " \n\n \n\n \n\n \n\n \n\n2 \n\n\n", " \n\n \n\nThe goal of this presentation is to go over a few different, very simple \n\nMapReduce problems. For those of you that have programmed before, the first \n\nprogram that you usually see as the Hello World program. The Hello World \n\nprogram of MapReduce is the word count problem. The problem is this. We \n\nwant to read in all of the words in a file, and we want to output how many times \n\neach word has appeared in that file.  \n\n \n\nNow, we need to get away from thinking about one single program that is going \n\nto solve everything, where you simply write all of your code in one file, and then \n\nyou execute that one file. That's not how MapReduce works. MapReduce works \n\nby having two phases, a mapper phase and a reducer phase.  \n\n \n\nAt a very high level, this is how Hadoop works. When you're using Hadoop, the \n\nmapper is executed on many machines. Your input is split up, such that one \n\nchunk of the input is sent to one of the mapper machines.  \n\n \n\nAnother chunk of the input is sent to another mapper machine. A third chunk of \n\nthe input is sent to a third mapper machine, and so on. In other words, your \n\nmapper code is running in parallel on many different CPUs, or on many different \n\nmachines. In a very extreme case, you should assume that your input is being \n\nsplit up, such that each line of your input is going to its own mapper machine, \n\nand the mapper code is running on that one line of input.  \n\n \n\nThe mapper then outputs as many key value pairs as it wants for the one line of \n\ninput that it is currently processing. The mapper could output nothing. The \n\nmapper could output one key value pair. The mapper could output 1,000 key \n\nvalue pairs for just one line of input. It can produce as much or as little output as \n\nit wants.  \n\n \n\n3 \n\n\n", " \n\nAfter the mappers are finished, there is a phase in Hadoop that looks at all of the \n\nkey value pairs that were produced by the mappers. This phase groups together \n\nall of those key value pairs that have the same key. In other words, think about a \n\nmapper that's dealing with one particular line. Let's just call it line x.  \n\n \n\nAnd this mapper might produce a key that has a-- don't want to say value, but \n\nthe key is 10 and the value is cat. So the key value pair is 10 cat. Now, think about \n\nthe map for dealing with another line, a line y. And it might produce a key value \n\npair of 10 dog. So the key is 10 and the value is dog.  \n\n \n\nThe phase between the mapper and the reducer would ensure that the 10 cat \n\nkey value pair and the 10 dog key value pair would end up on the same reducer. \n\nThey both have a key of 10. Therefore, they would end up on the same reduce.  \n\n \n\nThe reducer stage then acts very similar to the mapper phase. There may be \n\nmany machines that are running the same reducer code. The difference is that \n\nall of the key value pairs with the same key are guaranteed to be on the same \n\nreducer machine. However, you can't guarantee that any other key value pairs, \n\nthat have a different key, will be on the same machine.  \n\n \n\nSo going back to the example, the 10 cat key value pair and the 10 dog key value \n\npair will end up on the same reducer machine. However a key value pair with a \n\nkey of 20 and a value of dog may not end up on the same reducer machine. In \n\nthe extreme case, you should assume that each key is being sent to a different \n\nreducer machine. \n\n \n\n \n\n \n\n \n\n4 \n\n", " \n\n \n\nIn our first word count example, what we're going to do is read in text from the \n\ncommand line. This means that our input could come from one file or from \n\nmultiple files. We'll read in our text and we want to output a list of all of the \n\nwords from our input, along with the number of times that each word occurred \n\nin our input.  \n\n \n\nThe intermediate output from our mapper is going to be a simple key value pair. \n\nI say simple in that the key is going to be the word that we come across, and the \n\nvalue is going to be a very simple value of just 1. The 1 is representing that we've \n\nnow seen this word one time, or maybe one more time.  \n\n \n\nIf we come across the words Hello World, we will see Hello, and so we will \n\noutput Hello followed by the number 1. We'll see the word World, and so will \n\noutput the word World followed by the number 1. These are going to be the key \n\nvalue pairs that gets output by the mapper. \n\n \n\n \n\n \n\n5 \n\n\n", " \n\nAs a reminder, our mapper is stateless. This means that every line of the input \n\nmay be sent to its own mapper. We cannot assume that any two lines of the \n\ninput will end up on the same mapper. Therefore, when the mapper outputs key \n\nvalue pairs, it can only consider the current line of input. \n\n \n\n \n\n \n\n \n\n6 \n\n\n", " \n\n \n\nWhat we have here is the map function. It's a very short function, and there's \n\nreally not a whole lot to it. This map function is stored inside of a mapper file that \n\nI've called mapper.py.  \n\n \n\nIn this mapper file, we have some import statements up at the top. We need \n\nthese for reading in from our files, using regular expressions, and so on. The first \n\nthing that we actually do here is we read in one line.  \n\n \n\nWhen we say line equals the sys dot standard dot readline, it's simply saying read \n\nin the first line of text from the command line. We're not actually opening up any \n\nspecific file here. Rather, we are going to redirect text from an input file, or from \n\nmultiple input files, to our program.  \n\n \n\nThen we're going to have this thing called pattern. Pattern is basically asking, \n\nwhat do your words look like? In our example, words are simply going to be any \n\nalphanumeric thing. Anything that's not alphanumeric is going to be considered \n\na delimiter.  \n\n \n\nIf we have the word Hello followed by a comma, we don't want the comma to \n\nappear as part of the word. This is what pattern does. It's simply this regular \n\nexpression saying, look for words that are alphanumeric. Basically, letters and \n\nnumbers.  \n\n \n\nThe next line is saying, while line. What this means is this. Is there more text to \n\nprocess? If there is, then we're going to do something. And the something part \n\nstarts with the for loop that says, OK, now for every alphanumeric word that's in \n\nthe line, do something.  \n\n \n\n \n\n7 \n\n\n", "This pattern.findall(line) is saying, find all the alphanumeric words that are in this \n\nline, skipping everything else. We go through the line one word at a time. And all \n\nwe're doing is we're printing them off. We're printing off these key value pairs.  \n\n \n\nSo it looks at the first word. Maybe the first word was Hello. We'll print off the \n\nword in lowercase, which is hello in this case. And then we'll print off a tab. The \n\ntab is going to separate our key from our value.  \n\n \n\nFinally, we're going to follow it by printing off a value, which is simply going to \n\nbe a number 1. The tab between the key value pair may seem arbitrary, but it's \n\nactually quite important. Hadoop will expect that a tab separates the keys from \n\nthe values. Therefore, it's important to get into the habit of separating keys from \n\nvalues by using tabs.  \n\n \n\nAfter we've processed all of the words in the current line, then we'll go to the \n\nnext line, which is what the last statement is saying here. Get the next line from \n\nthe file and repeat this process. And this is it. This is the mapper. \n\n \n\n \n\n \n\n8 \n\n", " \n\n \n\nFinally, on this slide we have a main method. So for those of you that are familiar \n\nwith Java and other languages like that, you may really like to have a main \n\nmethod in your code. And so here I just defined this function called main that \n\ntakes in some arguments.  \n\n \n\nAnd then at the bottom, I am simply saying, OK, which method am I going to \n\ncall? Which function am I going to call? Well, I'm going to call the main function. \n\nSo that's all that's going on here is just adding a main function, for those of you \n\nthat are used to this. \n\n \n\n \n\n \n\n9 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", " \n\n \n\n \n\n \n\n \n\n10 \n\n\n", " \n\n \n\nThe reducer is also stateless, but it is different from the mapper's version of \n\nstatelessness. A stateless reducer simply means that all key value pairs that have \n\nthe exact same key will end up on the same reducer. \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n11 \n\n\n", "Stateless Reducer\n\n• Keys are not sorted, they are grouped.\n\nmoving on to another key.\n\n• Values are never sorted. \n\n• A reducer will process all (key,value) pairs with the same key before \n\n \n\n \n\nThere are a few other useful concepts that you need to know about how the reducer \n\nworks. The first is that it’s important to remember that the keys are never sorted, but \n\nrather they are groups. For example, imagine that you have four key value pairs that are \n\nsent to the same reducer. The reducer will receive all of those key value pairs that are \n\ngrouped together by key, but the keys, again, may not necessarily be sorted. \n\n \n\nAnother important concept to remember is that all of the key value pairs, with a certain \n\nkey, will be processed before a different key is processed. As an example, all of the key \n\nvalue pairs that have a key of “cat” will be processed before any of the key value pairs for \n\n“fish” are processed. But it’s important to note that the keys are not sorted. Once all of \n\nthe key value pairs for “cat” are processed, then the key value pairs for “fish” may be \n\nprocessed, and once all of the key value pairs for “fish” are processed, the reducer may \n\nprocess all of the key value pairs that have a key of “dog.” There is no order in which the \n\nkeys are processed. The only guarantee is that all key value pairs that have the same key \n\nwill be processed consecutively. \n\n \n\nThe last important thing that you need to note about the reducer is that the values are \n\nnever sorted. It is entirely possible that the key value pair of “cat” and “five” is processed \n\nbefore the value of “cat” and “two.” \n\n \n\n \n\n12 \n\n\n\n\n", " \n\n \n\nNow we're going to talk about the reduce function. We just saw how the map \n\nfunction worked by taking in the input and producing a key value pair for each \n\nword in the input. The key itself was just each of the words followed by some \n\nvalue. And that value was a simple number 1.  \n\n \n\nWhat this is saying is that we've seen this particular word one time. We are going \n\nto now take a look at the reduce function. How does the reduce function then \n\ntake these key value pairs and produce some output? \n\n \n\n \n\n \n\n13 \n\n\n", " \n\nSo the goal here is to read in the secondary key value pairs that the mapper \n\nproduced and then produce some output. So if the word Hello appeared in our \n\nfile 50 times, then we're going to print off hello followed by 50. If the word the \n\nappeared 12 times, then we're going to print off the followed by 12. This is going \n\nto be the goal of the reducer. Take that secondary key value pair and produce \n\nsome answer. \n\n \n\n \n\n \n\n \n\n14 \n\n\n", " \n\nNotes: Slides such as this show you what is happening as the reducer code runs. \n\n \n\n \n\n \n\n \n\n \n\n15 \n\n\n", " \n\n \n\nThis is the reduce function. So the other lecture, we saw how the map function \n\nworked. And it produced this key value pair. It produced a word, followed by the \n\nnumber 1, to indicate that we've seen this word one time.  \n\n \n\nSo how, then, does the reducer work? Well, the reducer starts off by creating a \n\nfew variables. So it says the current word is none. And basically, what this is \n\nsaying is, we haven't seen anything yet.  \n\n \n\nAnd so what is the current word that we're processing? Well, we're not \n\nprocessing anything. So current word is simply none.  \n\n \n\nWhat is, then, the current word count? Well, if we don't have a word yet, then \n\nobviously, our current count is equal to 0. And then we also have word being \n\nnone, which again, we haven't done anything yet. So it makes sense to have \n\nword equal to none.  \n\n \n\nThen we go through when we look at all of the key value pairs. And that's what \n\nthis for loop is doing. So it's saying for line in system.stdin, basically what that's \n\nsaying is, look at the first key value pair. \n\n \n\n \n\n \n\n16 \n\n\n", " \n\n \n\nAll of the MapReduce code that we're going to create is going to be tested on \n\nHadoop. However, Hadoop can be a little difficult to get set up and sometimes \n\nyou might want to just run your map and reduce and see if it actually works. In \n\norder to do this, I've created this test stateless code that you can download from \n\nthe activity.  \n\n \n\nAnd in order to run this code, it's a Java file, so you have to first compile the \n\ncode by entering the command that you see here. Once the above code has \n\nbeen compiled, you want to run that compiled code, then, on your mapper and \n\nyour reducer. And then define where the input files-- where do you want the \n\noutput stored, and do you want to store the intermediate key value pairs?  \n\n \n\nAnd that's what that last statement is saying. So the Java TestStateless mapper \n\nfile, the mapper file will be whatever your Python file is. Maybe it's mapper.py. \n\nThen it's followed by your reducer file, which is maybe just reducer.py.  \n\n \n\nThen it's going to be your input folder. Again, wherever your input files are being \n\nstored, the TestStateless will read in all files in that input folder one at a time and \n\nsend each line to the mapper one line at a time. And then lastly, we need an \n\noutput folder of, where do you want to store your output?  \n\n \n\nAnd you put the letter Y or the letter N, depending on whether or not you want \n\nthe-- whether you want the intermediate keys to be kept or not. More \n\ninformation on how to run this can be found in the activity. \n\n \n\n \n\n \n\n17 \n\n\n", " \n\n \n\nThe next example that we're going to consider is going to be a similar example \n\nto the word count, except this time we want to count the number of times that \n\nevery pair of characters, excluding some whitespace, appear consecutively in the \n\nfile. So as an example, consider this input file where you have hello followed by \n\nspace, followed by ella followed by a space, and followed by banana. What we \n\nwould like the output to be, then, is going to be what you see on the screen.  \n\n \n\nSo look at every pair of characters that appear in the input file. Again, where \n\nyou're excluding the whitespace. And these are the counts of how many times \n\nthose pairs appeared.  \n\n \n\nSo for instance, the \"an\" appeared twice, because in banana, it's B-A-N, which is \n\nthe first \"an.\" And then you have \"an\" again after that, so that's the second \n\noccurrence. The \"el\" in hello appears once, but it also appears once in ella, and \n\nthat gives us the two count for \"el.\"  \n\n \n\nThis is the problem that we would like to solve. And it's a little bit different from \n\nthe word count. In the word count example, all we were doing was we were \n\noutputting the word that we saw followed by a very simple number 1 to denote \n\nthat we've seen this word once.  \n\n \n\nBut that's not all that the mapper can do. The mapper can output multiple key \n\nvalue pairs for every word or for every line that it reads in. And that's what's going \n\non here. We want our mapper to read in hello, but we want it to output more \n\nthan one key value pair. \n\n \n\n \n\n \n\n \n\n18 \n\n\n", " \n\n \n\n \n\n \n\n \n\n19 \n\n\n", " \n\n \n\nIn this next example, we're considering a variation of that consecutive characters \n\nexample that we just saw. The variation is going to be this. For each word length, \n\nwhich character pairing appeared the most times?  \n\n \n\nSo for every word that has four characters, what character pairing appeared the \n\nmost times? To see a very simple example, consider this input where you have \n\nhello ella lloyd is at the lake and is a happy lazy man. The output, then, would be \n\nthis.  \n\n \n\nYou have a 2, so look at all of the words that have two characters. What was the \n\nmost common character pairing that appeared in words that had only two \n\nletters? Now, since this is a consecutive pairing example, the answer is obviously \n\ngoing to be the word count example, where I want to know which one appeared \n\nthe most, which word appeared the most times.  \n\n \n\nBut that's not always going to be the case. So think about the case where you \n\nhave three letters. Every word that has three letters, what was the most common \n\noccurrence in those three-letter words? And the three-letter words that we have \n\nhere are the, and, and man. And in those, you have \"th\" from the. You have \"he\" \n\nfrom the. You have \"an\" from and. You have \"nd\" from and. You have \"ma\" from \n\nman. And you have \"an\" from man.  \n\n \n\nIf you are to sum those up, you'd realize that \"an\" appeared twice and it was the \n\nonly one that appeared twice. Therefore, it's the answer to this question of, for \n\nall of these three-letter words, which character pairing appeared the most \n\ntimes? In this case it was \"an.\"  \n\n \n\nSo I'm not looking for the number of times that a pairing appeared. Simply, all I'm \n\n \n\n20 \n\n\n", "looking for here is, which one appeared the most? So the question that comes \n\nup is, what does the mapper output? And it's not an obvious solution. So think \n\nabout what does the mapper produce as it's reading in one line at a time? \n\n \n\n \n\n \n\n21 \n\n", " \n\n \n\nA common mistake that I will see from people with these types of problems is to \n\nhave the mapper output something that isn't going to help when you get to the \n\nreducing stage. In this case, a very simple thing that might come to most people \n\nwould be, well, let me output the two-letter pairings and have the value be the \n\nsize of the word that it came from. Because that seems like a reasonable thing to \n\noutput.  \n\n \n\nSo for instance, if you have the word and, the keys that you might output would \n\nbe \"an\" followed by the value of 3. And then you might output \"nd\" followed by a \n\nvalue of 3. So your key value pairs are the key being the pairing and the value \n\nbeing the number of letters that were in that word. And this is a common \n\nmistake that I see over and over again. And you don't want to do this, because as \n\na reminder, all key value pairs that have the same key will end up on the same \n\nreducer. So think about this, in the worst case, let's say you have the word at that \n\nappears in these four-letter words. And it appeared three times.  \n\n \n\nAnd so your intermediate key value pairs will be (at, 4), (at, 4), (at, 4) to denote \n\nthat I've seen the pairing at in a four-letter word. And I've seen it three times. \n\nThat could end up on one reducer. And you may have two key value pairs of \"en\" \n\nand 4 end up on a different reducer. So the question then becomes, how does \n\nthe \"en,\" what I'll denote here as the en-reducer, which is going to get these (en, \n\n4) key value pairs, how does it know that at is the correct answer?  \n\n \n\nIn this case, it has no idea, because both of those key value pairs went to \n\ndifferent reducers. And so therefore, having the pairing as the key is not the best \n\nidea for this particular problem. \n\n \n\n \n\n \n\n22 \n\n\n", " \n\n \n\nAnother way to attack these problems is to think about the problem from the \n\noutput's perspective. What information does each reducer need to know in order \n\nto create the output? And so think about this with a concrete example.  \n\n \n\nThink about this line of the output, where you have 3 followed by \"an.\" What \n\ndoes that mean? Well, that means all words that had three characters, the most \n\npopular character pairing that you had for those words was \"an.\"  \n\n \n\nSo that particular reducer needs to know all of the character pairings for words \n\nof size 3. Therefore, we have to ensure that all information about words of size 3 \n\nend up on the same reducer. And in order to do that, we need to make sure that \n\nour mapper will produce keys such that this is the case.  \n\n \n\nHopefully, you're seeing the pattern here of, oh, well now I need to be sure that \n\nmy keys are going to be the size of the word. And then the value will be \n\nwhatever it needs to be. And in this case, it will be the character pairings that \n\nappeared in that word. \n\n \n\n \n\n \n\n23 \n\n\n", " \n\nNow that we know what information must end up on each particular reducer, \n\nour mapper becomes pretty straightforward. We have to map our key value pairs \n\nsuch that the key is the size of the word and the value, then, is it's going to be \n\nour different character combinations. \n\n \n\n \n\n \n\n \n\n24 \n\n\n", " \n\n \n\nSimilar to the previous example, we had to create multiple key value pairs. \n\nHowever, in the previous example, we created multiple key value pairs, but we \n\nhad different keys. In this example, we are creating multiple key value pairs. They \n\nhave the same key, but they'll have different values.  \n\n \n\nIf the mapper reads in the word hello, for instance, it will output the following \n\nkey value pairs for this problem. It will output 5 representing the size of the word, \n\nfollowed by \"he.\" Then it will output 5 followed by \"el,\" and so on. And this \n\ndiffered from the previous example where we were outputting \"he\" followed by a \n\n1, and then \"el\" followed by a 1, and so on. \n\n \n\n \n\n \n\n25 \n\n\n", " \n\n \n\nThe reducer for this problem will be a little bit different from the ones that we've \n\nseen in that we're guaranteed to have all of the key value pairs that will have the \n\nsame key. And in this case, the key is going to be the word size. So every key \n\nvalue pair that has a key of 5 is coming from a word that had a length of 5.  \n\n \n\nSo we're guaranteed to have all of those key value pairs end up on the same \n\nreducer. What the reducer is going to do, then, is it's going to loop through all of \n\nthese key value pairs and simply keep track of whichever character combination \n\noccurred the most. And maybe this will be done using some sort of dictionary or \n\nhash map or whatever you want to use to determine that \"an\" occurred four \n\ntimes, or \"el\" occurred seven times, or whatever you need to do.  \n\n \n\nThe one tip that I'll give you here, which I've seen mistaken many times, is people \n\nwill not clear their dictionary before moving on to the next word size, because \n\nit's possible that multiple key value pairs that have different keys will end up on \n\nthe same reducer. You're not guaranteed to have one key per reducer. You may \n\nhave more than one. So you need to make sure that your dictionary is cleared \n\nout before moving on to the next word size or the next key value. \n\n \n\n \n\n \n\n26 \n\n\n", " \n\n \n\n27 \n\n\n"]], ["C:\\Users\\mjols\\Documents\\DS UWEC courses\\730 Big Data\\ds730_lesson1-mapreduce_intro.pdf", [" \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nImportant note: Transcripts are not substitutes for textbook assignments. \n\n \n\n1 \n\n\n\n", " \n\nIn general, parallel programming can be very difficult. Trying to figure out what threads \n\nare running, what needs to run, how do you synchronize between your threads, having \n\nto worry about livelock, deadlock, starvation. Even if you are able to create a parallel \n\nprogram to solve your problem, you may still have to worry about issues like what \n\nhappens if your processor fails? \n\n \n\nOr what happens if your computer was off the network? Or what happens if the \n\ncomputer just shuts down? Do you wait for a thread that's slow or do you simply cancel \n\nit and move on and schedule the thread on some other processor? Parallel \n\nprogramming is very nice in that you have complete control over what is going on. \n\nHowever, if you're willing to give up a bit of control for an easier solution, then \n\nMapReduce is for you. \n\n \n\n \n\n \n\n \n\n2 \n\n\n", " \n\nMapReduce is intended to work on many average machines. When people think about \n\nhigh performance computing, they probably think about these super computers that \n\nhave hundreds of CPU cores, a ton of memory, a ton of disk space. What MapReduce \n\ndoes is it takes that idea that a lot of average machines can do the work of one \n\nsupercomputer. And so there's no need to have one massive computer running the \n\ncomputation. Rather, you can have a lot of these inexpensive and very average \n\nmachines that can do about the same work that that one supercomputer can do. \n\n \n\n \n\n \n\n \n\n3 \n\n\n", " \n\n \n\n \n\n \n\n4 \n\n\n", " \n\n \n\n \n\nThe goal of this lecture is really just to give you an introduction and kind of an overview \n\nof the MapReduce model. You're going to be expected to read the MapReduce paper to \n\nget a better insight into how the entire process works. What I want to do here is give \n\nyou an overview of this entire process. \n\n \n\nAs you probably expected, there are two functions that have to be implemented in order \n\nfor MapReduce to work. There's a map function and a reduce function. The map function \n\nstarts by taking a key value pair and producing an intermediate or secondary key value \n\npair. And we're going to see an example in the next lecture. \n\n \n\nSo, this high level explanation of the mapper taking a key value pair and producing a \n\ndifferent, intermediate or secondary pair is good enough for now. One thing that's very \n\nimportant for you to know is that the mapper phase is stateless. So, what this means is \n\nthat you can't assume that the mapper has read in the previous line or anything previous \n\nin the file because that data may have been processed by a different mapper. \n\n \n\nAnd so all you have in the current mapper state is the current key value pair. You can't \n\nrely on anything that was read in before. For those of you that have programmed before \n\nyou might be asking yourself, OK, well can I store the previous values in an array or some \n\nsort of list so that I can process them later? And the answer to that is no. \n\n \n\nThat's not how the MapReduce framework works. You really need to get away from that \n\nmodel of thinking that you can just store values in an array or a list or something, and \n\nthen go back to them later. Rather, you should think about things, the way that the \n\nMapReduce framework works in that it looks at the current value and then it outputs \n\nsomething. And then we'll let the reducer take care of it after that. \n\n \n\n5 \n\n\n", "OK, so this is how the MapReduce framework works for map. It takes a key value \n\npair and produces a secondary key value pair. This may seem a little abstract right \n\nnow, but we're going to see a concrete example in the next lecture that should bring \n\nall of this together. \n\n \n\nThe reducer then takes that intermediate key value pair that was output by the mapper \n\nand it will produce some output value. The reducer itself has two main stages or phases, \n\na shuffle or sort phase and a reduce phase. The shuffle or sort phase is usually taking the \n\noutput of the mappers and sorting them in some fashion. And so this is going to be \n\nallowed in some of your code when you're writing your mapper, is that you can sort \n\nthings once the map is done executing. \n\n \n\nSo this will be allowed because we aren't going to have this sorting function that Hadoop \n\nuses, which is the implementation of MapReduce. And so at the end of map, it's going to \n\nbe fine to be able to sort things, but that will be it. So the reducer will have this shuffle \n\nor sort phase that's going to take that sorted - those sorted key value pairs from the \n\nmapper and then it's going to send those sorted key value pairs into the reduce phase. \n\nAnd so, the reduce phase takes this sorted or shuffled data and it will produce some \n\noutput. \n\n \n\nAnd the output of the producer is not necessarily sorted. So all of this high level \n\noverview of how map and reduce actually works, may seem a little abstract right \n\nnow. But the next lecture, we will go into a more concrete example and see how \n\nthis actually works. \n\n \n\n \n\n \n\n6 \n\n", " \n\n \n\n7 \n\n\n"]], ["C:\\Users\\mjols\\Documents\\DS UWEC courses\\730 Big Data\\ds730_lesson1_high-performance-intro.pdf", [" \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nImportant note: Transcripts are not substitutes for textbook assignments. \n\n1 \n\n\n\n\n", " \n\nHello, and welcome to the high performance computing course. What I want \n\nto do with this presentation is, really, just give you an overview of what to \n\nexpect in this course. Now, some of the questions that will answer today are: \n\nhow does this course it with all of the other courses? What are some of the big \n\nideas that we're going to cover in this particular course? Since this is a high \n\nperformance computing course, we're dealing with big data. So, what makes \n\ndata really big? And then, finally, we'll look at some of the analytical, and \n\ntechnical skills that you're going to develop throughout the semester in this \n\ncourse. \n\n \n\n \n\n \n\n2 \n\n\n", " \n\nWhy do you actually need this course? one of the problems that I like to tell \n\nmy undergraduate students is this. So let's say that you have an algorithm. You \n\nyou've come up with a way that's going to perfectly predict the weather for the \n\nfollowing day. But, it takes a long time to run. Let's say this algorithm takes a \n\nweek to run. So then, the algorithm isn't that good. And so, what is the \n\nproblem here? Well, we don't really have a problem collecting data or cleaning \n\nthe data. Data uncertainty really isn't the issue here, communicating \n\nconclusions. So, none of these are issues. The big problem that we're having is \n\nthat the algorithm just takes forever to run. Maybe the data size is huge, or \n\nmaybe it's just that the algorithm takes a long time. And so, these are the \n\nproblems that we're going to tackle in this particular course. \n\n \n\n \n\n \n\n3 \n\n\n", " \n\nWhat, then, is the purpose of this course? Where exactly does this course fit in \n\nwith this particular program? In some of the other courses, you're going to be \n\nlearning about collecting data, cleaning that data, preparing that data, drawing \n\nconclusions when you don't have an exact answer, dealing with how to \n\ncommunicate your results, and really, using data ethically. But a lot of these \n\nthings can't be done if the data is just too big. And so, the overarching theme \n\nof this course, then, is really, in the sentence that's at the bottom of the screen \n\nhere, and it's how do you analyze, and manage big data that either doesn't fit \n\ninto memory or takes too long to run. \n\n \n\nAnd so, these would be the two big issues that we're going to deal with here. \n\nWhat if it doesn't fit into memory? Let's say we have terabytes or petabytes of \n\ndata. Most of our machines have gigabytes of memory, and so, obviously, all of \n\nthat data isn't going to fit into memory. How do we write a program, then, \n\nthat's going to be able to process this information if it doesn't even fit in \n\nmemory. One of the other issues that we're going to deal with is the \n\nprocessing part of big data. What if it simply takes too long to run? We have an \n\nalgorithm, and the algorithm works fine, but it takes quite a long time to run. \n\nAnd so, how can we maybe split up the algorithm so that we can get it to run \n\nmuch, much faster. And so, these are going to be the two things that we're \n\ngoing to be looking at as we go throughout the semester. \n\n \n\n \n\n4 \n\n\n", " \n\nWhat exactly is big data? I guess it really depends on your perspective. 30 years \n\nago, my parents bought me a computer, a computer that was called the Tandy \n\nTRS 80. There was no hard drive on this machine. The only way that you get a \n\nprogram to run was to load it up with a floppy disk. These floppy disk could hold \n\nmaybe 100 kilobytes of data, maybe a little more, a little less. Memory was on the \n\norder of kilobytes. CPU speed was terrible, maybe a megahertz, maybe 2 \n\nmegahertz, maybe. \n\n \n\n \n\n \n\n5 \n\n\n", "Big data, back then, could have been gigabytes or terabytes of data. But now, big \n\ndata is more like petabytes and exabytes of data. So, in 30 years, those data sets \n\nwill likely get much, much larger. \n\n \n\n \n\n \n\n \n\n6 \n\n\n", " \n\nIn this course, we're going to define big data to be a data set they can't be \n\nprocessed by a normal machine in a reasonable amount of time. Now, for \n\nsomeone like myself, who is a theoretical computer scientist, these keywords \n\nof normal and reasonable are a bit bothersome. I don't care for them. I like to \n\nhave no exact things. But, if we have an exact thing, such as a terabyte of data \n\nor something that takes five days to process, those terms can change over \n\ntime. A few gigabytes of data, 30 years ago, was big. Now, it's not so much. \n\n \n\nAnd nowadays, a few terabytes of data is large, and in 10, 20 years, it may not \n\nbe so large. So, we're going to concern ourselves with the hazy definition of big \n\ndata being it takes a long time to run or the data is too large to process on a \n\nnormal machine. \n\n \n\n \n\n \n\n7 \n\n\n", " \n\nSo then, one in what ways can our data be big? We could consider a data set big \n\nif it's huge. The storage that we need to store the data is very large. The data size \n\nis just too massive to be handled by a regular machine. Now, storage is less of an \n\nissue than it was maybe 10, 15 years ago. But, it is still a concern. If it takes a \n\nnormal machine days or weeks just to read in the data, then how can we process \n\nthat data any faster? Storage capacities are getting much, much larger, but the \n\naccess time to that data is still about the same as it was 10 or 15 years ago. \n\n \n\n \n\n \n\n8 \n\n\n", " \n\nVelocity is another way that we can define what big data is. Data that comes \n\nthrough is so fast that we can handle it with a normal machine, could be \n\nconsidered big data. Is it even feasible to store all this data that we're receiving \n\nor do we just look at once, and make a decision about it, maybe added into an \n\naverage or something like that? For instance, maybe we have sensors at the \n\nsite collecting weather data. Maybe it's collecting temperature or humidity or \n\nwind or whatever. And it's doing it every few milliseconds. Do we really need \n\nto keep all that data? Maybe, maybe not. Maybe we can just read it in, keep the \n\naverage over the last few minutes or something along those lines. \n\n \n\nAs a very concrete example, there was something called the Apache telescope \n\nthat produced 80 terabytes of data over a seven year period. Now, that's a fair \n\namount of data. But recently, there was another telescope called the Large \n\nSynoptic telescope that can produce 40 terabytes of data every single day. To \n\nput that into some perspective, your camera phone, your digital camera, \n\nwhatever you have, produces photographs that are about two megabytes of \n\npicture. This telescope can produce eight million pictures every day or 92 \n\npictures every single second. That's a lot of data. \n\n \n\n \n\n \n\n9 \n\n\n", " \n\nWe won't feel too much with the variety in this course, but it is a problem for \n\nbig data. If data is coming from many different formats, how exactly do we \n\ndeal with it? Maybe our data is coming from a text file, and other data is \n\ncoming from a database, and more data is coming from an audio file. How \n\nexactly do we deal with that? How do we merge all of that data? How can we \n\nimpose a structure on data that really has no structure? Now, we'll do a bit of \n\nthis when we talk about NoSQL databases near the end of the course. \n\n \n\n \n\n \n\n10 \n\n\n", " \n\nSo, one of the big topics that we're going to be covering in this course. What \n\nI'll do right now is just give you a quick overview of what we're going to be \n\ncovering throughout the semester. The next presentation, we'll go into a little \n\nmore detail on each of these topics. An overarching theme of the course is \n\ngoing to be the ability to problem solve. I'm not terribly interested in having \n\nyou memorize facts or just regurgitate things back to me. The goal of all of my \n\ncourses, whether I do them online or in person, is really to teach you a \n\nparticular tool there or some technique, and have you apply that technique or \n\ntool to a different problem. \n\n \n\nNow, this can be hard to convince my students, my undergraduate students, \n\nthat this is a useful thing to know how to do. But, I know that many of you are \n\nin industry or have been in industry. So, I shouldn't really have to convince you \n\nthat problem solving is a very important thing. Your client, your boss, will never \n\ncome up to you and say, \"Hey, I want you to solve this particular problem by \n\ndoing this, this, and that.\" That just doesn't happen. They're going to come to \n\nyou with some sort of problem, and say, \"I have a problem. Find me a \n\nsolution.\" They don't usually care how it gets done, as long as they get a \n\nsolution. That's fine. And so, that's a big part of this course is we'll be learning \n\ncertain topics, and then, you'll be applying those tools and techniques to other \n\nproblems. \n\n \n\n \n\n11 \n\n\n", " \n\n \n\n12 \n\n\n"]], ["C:\\Users\\mjols\\Documents\\DS UWEC courses\\730 Big Data\\ds730_lesson1_high-performance-topics.pdf", [" \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nImportant note: Transcripts are not substitutes for textbook assignments.\n\n1 \n\n\n\n\n", " \n\n \n\n \n\n2 \n\n\n", " \n\n \n\n \n\n \n\n \n\nThere are many problems that are easy to solve in parallel. What this means is \n\ngetting an answer for part of the solution in no way affects the answer for \n\nanother part of the solution. A simple real-world example might be building \n\na house. Some of the things can be done in parallel and some cannot. For \n\nexample, let's assume that the house is built already. Then painting each room \n\ncan be done in parallel. One person can be painting in the dining room. \n\nAnother person can be painting in the kitchen. And another person can be \n\npainting in a bedroom. These things can be done in parallel. One person \n\npainting in one room does not affect a different person painting in \n\nanother room. \n\nHowever, there are other things that cannot be done in parallel. For instance, the \n\nconcrete being poured for the foundation has to be done before the oven is \n\ninstalled in the kitchen. Similar problems exist in the software world. Imagine \n\nsearching through a bunch of files for a specific word. One can make a parallel by \n\nsplitting up the work across multiple processors by saying something like, \n\nprocessor 1 you search files X, Y and Z. And processor 2, you search files U, V and \n\nW, and so on. The work of one processor does not affect the work of another. \n\nThey can both search their respective files without waiting for or really relying on \n\nthe other one. \n\nHowever, a problem that's not very easy to make parallel is rendering a 3D \n\nscene with lighting. Light bounces off of objects, and so, light that comes \n\n3 \n\n\n", "from a light source and bounces off of a mirror and onto, say, a cabinet, the \n\nreflectivity of the mirror will affect how bright that cabinet appears. So, you \n\ncannot render the cabinet's brightness until you've measured how well the \n\nmirror can reflect that light. \n\nNotes: \n\n \n\n \n\nWe will focus on multi-threaded applications using Java. \n\n \n\n4 \n\n", " \n\n \n\n \n\n5 \n\n\n", " \n\nMapReduce is the theory, and Hadoop is the implementation. Hadoop is open \n\nsource software that is written in Java. Hadoop uses its own file system. The \n\nbig difference between HDFS, which is the Hadoop Distributed File System, \n\nand a normal operating system's file system is the block size. Hadoop's default \n\nblock size is 64 megabytes, whereas most operating systems have a much, \n\nmuch smaller default block size on the order of kilobytes. What this allows for \n\nHadoop is very fast retrieval of files, because you don't have to move the read \n\nhead back and forth, that you may have to do with the regular operating \n\nsystem. If using this on a cluster of computers, and if it's set up correctly, HDFS \n\nwill automatically replicate the data across multiple machines. What this \n\nmeans for you is if one device crashes, the data will still be available. \n\n \n\n \n\n \n\n6 \n\n\n", " \n\nPig and Hive are frameworks that sit on top of Hadoop. In other words, you \n\ncan use Pig and Hive to solve problems, instead of having to write MapReduce \n\ncode with Java. If you're more familiar with Python or Perl, then you will \n\ndefinitely like Pig. If you're more of a database person who likes SQL, then \n\nyou'll like Hive. Both of these frameworks take your code, whether it's Python \n\nor SQL, and it will translate it into Java's map and reduce behind the scenes, so \n\nyou don't even have to worry about it. \n\n \n\n \n\n \n\n7 \n\n\n", " \n\n \n\n \n\n \n\n8 \n\n\n", " \n\n \n\n \n\n9 \n\n\n", " \n\n \n\n10 \n\n\n"]], ["C:\\Users\\mjols\\Documents\\DS UWEC courses\\730 Big Data\\ds730_lesson1_linux.pdf", [" \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nImportant note: Transcripts are not substitutes for textbook assignments. \n\n1 \n\n\n\n", " \n\n2 \n\n \n\n \n\n \n\n \n\n\n", " \n\n3 \n\n \n\n \n\n \n\n \n\n\n", " \n\nSlide 4.  \n\n \n\nAt this point, you should already have a Linux machine set up either locally or in the \n\ncloud. The majority of our interactions with the Linux machine are going to be through \n\nthe terminal window writing commands and opening up files. Now, later on in this \n\npresentation, I'll show you how to install a desktop, a more graphical experience to your \n\nLinux machine. The rest of this presentation, then, is going to be describing some of the \n\nbasic commands of navigating your file system, how do you open up files, how do you \n\nedit files, and how do you execute programs.  \n\n \n\n \n\n \n\n4 \n\n\n", " \n\nSlide 5.  \n\n \n\nThe pwd, or print working directory, is the command that you're going to execute to \n\ndisplay the name of the folder that you're currently in. To relate this to a more \n\nWindows-type approach, if you were to open up a folder and look at the highlighted \n\npart that's in the image, this tells you which folder you're currently in. pwd does the \n\nsame thing. It tells you what's the current folder.  \n\n \n\n \n\n \n\n \n\n \n\n5 \n\n\n", " \n\nSlide 6.  \n\n \n\nIf you would like to display the files that are in the current folder that you're in, you'll \n\nuse the ls command, and the ls command list the contents of the current folder. To \n\nrelate this to a more GUI type experience, if you were to open up a folder and simply \n\nview all the files, this is what the ls command is doing.  \n\n \n\n \n\n \n\n \n\n6 \n\n\n", " \n\nSlide 7.  \n\n \n\nIf you ever want to make a new folder, one way that you would do that in a more \n\nWindows-type approach would be to right click and go down to New and then click on \n\nfolder. The way that we do this in the command line is by the make directory command. \n\nIt's the mkdir, and then you give it whatever the name of the folder is. In this example, I \n\nhave make directory temp.  \n\n \n\n \n\n \n\n \n\n7 \n\n\n", " \n\nSlide 8.  \n\n \n\nIf we want to change the current folder that we're in, we use the cd, or change \n\ndirectory, command to move to a different folder. And so here in this example, I have cd \n\ntemp, and temp is simply the name of the folder. And to relate this back to Windows, \n\nthis is basically double clicking on a folder. So double clicking on the temp folder is just \n\nlike doing cd temp.  \n\n \n\n \n\n \n\n8 \n\n\n", " \n\n \n\nSlide 9.  \n\n \n\nThe commands that you've seen in this presentation are a lot of the very useful and \n\nmost common commands that you'll have to use at the terminal window. A few of the \n\nother ones are listed on this page. If you want to open up a file for editing or if you want \n\nto make a new file, that's what the Vim program is for.  \n\n \n\nIf you would like to remove or delete a file, you would use rm and then the name of the \n\nfile. If you would like to make a copy of a file, you would use the cp command. And if \n\nyou would like to move a file, you would use the mv command. There are a ton of other \n\ncommands that you want to use at the terminal, and if you're interested in what these \n\nare, you can simply look them up or post to the discussion board if there's something \n\nthat you want to do and you're wondering if there's a command to do it.  \n\n \n\n \n\n \n\n9 \n\n\n", " \n\n \n\nSlide 10.  \n\n \n\nOne of the main reasons that we're using PuTTY is because of its ability to copy and \n\npaste. Using the VirtualBox window, it's a lot more difficult to copy and paste text, but \n\ndoing it in PuTTY is actually quite easy. To copy from your local machine and paste text \n\ninto PuTTY, it's quite simple to do. You copy the text as you normally would on your \n\nregular machine.  \n\n \n\nThen you right click anywhere on the PuTTY window, and that will paste the text into \n\nwhatever you have open. To copy text from PuTTY, then, to your local machine, you \n\nsimply highlight the text that you want to copy from PuTTY, and then this text is \n\nautomatically copied when you unclick. You can paste this text on your regular machine \n\nas you normally would.  \n\n \n\n \n\n \n\n10 \n\n\n", " \n\nSlide 11.  \n\n \n\nThere's several text editors that you can use when you're at the terminal window inside \n\nof PuTTY there's Vim, Emacs, Pico, and the one that I'm going to talk about now is called \n\nNano. If you'd like to edit any kind of text documents in the terminal window, one of the \n\nprograms that you can use is Nano. To open up or to edit a file, you simply say \"nano\" \n\nand then followed by the name of the file. If that file doesn't already exist, then Nano \n\nwill create it for you.  \n\n \n\n \n\n \n\n \n\n11 \n\n\n", " \n\n \n\nSlide 12.  \n\n \n\nUsing Nano in the terminal window is very similar to using Notepad on your regular \n\nWindows machine. Once Nano is opened, you can start writing text immediately. The \n\nway that you save your file is by using the Control-O command.  \n\n \n\nAnd so Control-O will tell Nano that you want to save the file. At the bottom of this \n\nimage, you'll note that there is a confirmation of whether or not you want to save this \n\nfile. If this is the file that you would like to save it to, you simply hit Enter.  \n\n \n\n \n\n \n\n12 \n\n\n", " \n\nSlide 13.  \n\n \n\nOnce you are done editing and saving your file, you hit Control-X to quit out of Nano. If \n\nyou try and quit before you saved your changes, Nano will ask you if you would like to \n\nkeep these changes or discard them, and that's what you see at the bottom of this \n\nimage. You can choose yes or no, depending on what you want to do.  \n\n \n\n \n\n \n\n \n\n13 \n\n\n", " \n\n \n\n \n\n14 \n\n\n"]], ["C:\\Users\\mjols\\Documents\\DS UWEC courses\\730 Big Data\\Hadoop_Architecture_Notes.pdf", ["Hadoop\t\n\nArchitecture\t\n\n\n\n", "Note\t\n\nThese\tnotes\tprovide\ta\tgeneral\tintroduc�on\tto\t\n\nHadoop.\t\tThe\tcontent\tis\tpre�y\tself-explanatory\t\n\nand\tis\tmeant\tto\thelp\tyou\tunderstand\tHadoop\t\n\nconceptually.\t\tPay\ta�en�on\tto\tthe\tpage\tthat\t\n\ndiscusses\thow\tto\twrite\tpython\tcode\tso\tthat\tit\t\n\nworks\twith\tHadoop.\tThis\tis\texplained\tin\tmore\t\n\ndetail\tin\tthe\tHadoop\tIntroduc�on\tac�vity.\t\n\n\n\n", "What\tare\twe\tlearning?\t\n\nProblem\tSolving\t\n\nParallel\tProgramming\t\n\nMapReduce\t\n\nHadoop\t\n\nPig\t\n\nHive\t\n\nNoSQL\t\n\n\n", "Hadoop\tFramework\t\n\nFour\tmodules:\t\n\nHadoop\tCommon:\t\n\nLibraries\tand\tu�li�es\tneeded\t\n\nby\tother\tHadoop\tmodules\t\n\n\t\n\nHadoop\tDistributed\tFile\t\n\nSystem\t(HDFS):\t\n\nDistributed\tﬁle\tsystem\tthat\t\n\nstores\tdata\tacross\t\n\n(poten�ally)\tmany\tmachines\t\n\nHadoop\tYARN:\t\n\nResource\tmanagement\tpla�orm\t\n\nresponsible\tfor\tmanaging\t\n\ncompu�ng\tresources\tand\t\n\nscheduling\tapplica�ons.\t\n\n\t\n\nHadoop\tMapReduce:\t\n\nProgramming\tmodel\tfor\t\n\nprocessing\tbig\tdata.\t\n\n\n\n\n\n\n\n\n\n\n", "Hadoop\tFamily\t\n\nMapReduce\tcode:\t\n\n(cid:1)  wri�en\tin\tJava.\t\n\n(cid:1)  can\talso\tbe\twri�en\tin\tpython\t(see\tHadoop\tIntroduc�on\t\n\nac�vity).\t\n\nPig:\tscrip�ng\tlanguage\twri�en\tin\tPig\tLa�n.\t\n\nHive:\tSQL\tvariant.\t\n\n\n\n\n", "Hadoop\tDistributed\tFile\tSystem\t\n\nWri�en\tin\t\n\nJava\tand\tis:\t\n\n(cid:1)  Distributed\t\n\n(cid:1)  Scalable\t\n\n(cid:1)  Portable\t\n\nNameNode\t\n\nSingle,\tcontrols\tall\t\n\nDataNodes\t\t\n\nSecondary\t\n\nNameNode\t\n\nBuilds\tsnapshot\tof\t\n\nNameNode\tin\tcase\tit\tfails\t\n\nDataNode\t\n\nDataNode\t\n\nDataNode\t\n\nDataNode\t\n\nDataNode\t\n\n(cid:1)  Contains\tactual\tdata\t\n\nin\tvery\tlarge\tchunks.\t\n\n(cid:1)  Default\treplica�on\t\n\nof\tdata\tis\t3\t\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "HDFS\tAdvantages\t\n\nData\tawareness\tbetween\tthe\tdata\tand\tthe\tjob\t\n\ntracker.\t\n\n(cid:1)  If\tnode\tA\tcontained\tdata\tof\ta,b,c\tand\tnode\tZ\t\n\ncontained\tdata\tof\tx,y,z,\tthen\tthe\tjob\ttracker\t\n\nwould\tassign\tmap\tprocesses\taccordingly.\t\n\nReduces\tnetwork\ttraﬃc\tand\tdata\ttransfer.\t\n\n\n\n", "HDFS\tLimita�ons\t\n\nDesigned\tfor\timmutable\tﬁles.\t\n\nRetrieving\tdata\tand\tstoring\tdata\tmust\tgo\t\n\nthrough\tHadoop.\t\n\n\n\n", "Jobs\t\n\nIf\tcan’t\tget\tdata\ton\tsame\tnode,\t\n\ntries\tto\tget\tit\ton\tsame\track.\t\n\nJob\tTracker\tis\t“rack-aware.”\t\n\n\t\n\nApplica�ons\t\n\nSubmit\tMapReduce\tjobs\t\n\nSends\t\n\nwork\tto\t\n\navailable\t\n\nTask\t\n\nTrackers\t\n\nDataNode\t\n\nDataNode\t\n\nDataNode\t\n\nTask\tTracker\t\n\nGoal:\tKeep\tprocess\tas\tclose\t\n\nto\tdata\tas\tpossible.\t\n\nJob\tTracker\t\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "Tasks\t\n\nSends\t\n\nheartbeat\t\n\nevery\tfew\t\n\nminutes\tto\t\n\nindicate\tit\tis\t\n\ns�ll\t“alive.”\t\n\nJob\tTracker\t\n\nDataNode\t\n\nDataNode\t\n\nDataNode\t\n\nTask\tTracker\truns:\t\n\n\ton\teach\tnode\t\n\n(cid:1) \n\n(cid:1)  tasks\tgiven\tby\tJob\tTracker\t\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "Separated\tin\tHadoop\t2.0\t\n\nData\tprocessing\t\n\nMapReduce\t&\t\n\nOthers\t\n\nResource\tmanagement\t\n\nYARN\t\n\nResourceManager:\tauthority\tprocess\t\n\nthat\tarbitrates\tresources\tamong\tall\t\n\napplica�ons.\t\n\nApplica�onMaster:\tnego�ates\t\n\nresources\tfrom\tResourceManager\t\n\nand\tworks\twith\tNodeManger\tto\t\n\nexecute\tand\tmonitor\ttasks.\t\n\nMore\ton\tYARN:\t\t\n\nh�ps://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html\t\t\n\n\n\n\n\n\n\n\n\n", "\t\tWri�ng\tHadoop\tPrograms\t\n\n#!/usr/bin/env python\n\nIn\t\n\npython\t\n\n#any imports will go here\n\n#no global variables\n\n \n\n \n\ndef main(argv):\n\n    #our map or reduce code will go here\n\nif __name__ == \"__main__\":\n\n    main(sys.argv)\n\n\n\n", "Cloud\tCompu�ng\t\n\nIn\torder\tto\tsee\tthe\ttrue\tpower\tof\t\n\nHadoop,\twe\twill\tu�lize\tAmazon\tWeb\t\n\nServices.\t\n\nYou\tshould\thave\talready\tcreated\tan\t\n\naccount\tand\thave\tapplied\tfor\t\n\neduca�onal\tcredits.\t\n\nThe\tac�vity\twill\twalk\tthrough\thow\tto\t\n\nrun\tHadoop\tin\tthe\tcloud\tenvironment.\t\n\n\n\n"]], ["C:\\Users\\mjols\\Documents\\DS UWEC courses\\730 Big Data\\MapReduce_Simplified_Data_Processing_On_Large_Clusters.pdf", ["MapReduce: Simplified Data Processing \n\non Large Clusters\n\nby Jeffrey Dean and Sanjay Ghemawat\n\nAbstract\n\nMapReduce is a programming model and an associated implementation for processing\n\nand generating large datasets that is amenable to a broad variety of real-world tasks.\n\nUsers specify the computation in terms of a map and a reduce function, and the under-\n\nlying runtime system automatically parallelizes the computation across large-scale clusters of\n\nmachines, handles machine failures, and schedules inter-machine communication to make effi-\n\ncient use of the network and disks. Programmers find the system easy to use: more than ten\n\nthousand distinct MapReduce programs have been implemented internally at Google over the\n\npast four years, and an average of one hundred thousand MapReduce jobs are executed on\n\nGoogle’s clusters every day, processing a total of more than twenty petabytes of data per day.\n\n1 Introduction\n\nPrior to our development of MapReduce, the authors and many others\n\nat Google implemented hundreds of special-purpose computations that\n\nprocess large amounts of raw data, such as crawled documents, Web\n\nrequest logs, etc., to compute various kinds of derived data, such as\n\ninverted indices, various representations of the graph structure of Web\n\ndocuments, summaries of the number of pages crawled per host, and\n\nthe set of most frequent queries in a given day. Most such computa-\n\ntions are conceptually straightforward. However, the input data is usu-\n\nally large and the computations have to be distributed across hundreds\n\nor thousands of machines in order to finish in a reasonable amount of\n\ntime. The issues of how to parallelize the computation, distribute the\n\ndata, and handle failures conspire to obscure the original simple com-\n\nputation with large amounts of complex code to deal with these issues.\n\nAs a reaction to this complexity, we designed a new abstraction that\n\nallows us to express the simple computations we were trying to perform\n\nbut hides the messy details of parallelization, fault tolerance, data distri-\n\nbution and load balancing in a library. Our abstraction is inspired by the\n\nmap and reduce primitives present in Lisp and many other functional lan-\n\nguages. We realized that most of our computations involved applying a\n\nmap operation to each logical record’ in our input in order to compute a\n\nset of intermediate key/value pairs, and then applying a reduce operation\n\nto all the values that shared the same key in order to combine the derived\n\ndata appropriately. Our use of a functional model with user-specified map\n\nand reduce operations allows us to parallelize large computations easily\n\nand to use reexecution as the primary mechanism for fault tolerance.\n\nBiographies\n\nJeff Dean (jeff@google.com) is a Google Fellow and is currently work-\n\ning on a large variety of large-scale distributed systems at Google’s Moun -\n\ntain View, CA, facility.\n\nSanjay Ghemawat (sanjay@google.com) is a Google Fellow and works\n\non the distributed computing infrastructure used by most the company’s\n\nproducts. He is based at Google’s Mountain View, CA, facility.\n\nThe major contributions of this work are a simple and powerful\n\ninterface that enables automatic parallelization and distribution of\n\nlarge-scale computations, combined with an implementation of this\n\ninterface that achieves high performance on large clusters of com-\n\nmodity PCs. The programming model can also be used to parallelize\n\ncomputations across multiple cores of the same machine.\n\nSection 2 describes the basic programming model and gives several\n\nexamples. In Sec tion 3, we describe an implementation of the Map Reduce\n\ninterface  tailored  towards  our  cluster-based  computing  environment.\n\nSec tion 4 describes several refinements of the programming model that\n\nwe have found useful. Sec tion 5 has performance measurements of our\n\nimplementation for a variety of tasks. In Section 6, we explore the use of\n\nMapReduce within Google including our experiences in using it as the ba-\n\nsis for a rewrite of our production indexing system. Section 7 discusses re-\n\nlated and future work.\n\n2 Programming Model\n\nThe computation takes a set of input key/value pairs, and produces a\n\nset of output key/value pairs. The user of the MapReduce library\n\nexpresses the computation as two functions: map and reduce.\n\nMap, written by the user, takes an input pair and produces a set of\n\nintermediate key/value pairs. The MapReduce library groups together\n\nall intermediate values associated with the same intermediate key I\n\nand passes them to the reduce function.\n\nThe reduce function, also written by the user, accepts an interme-\n\ndiate key I and a set of values for that key. It merges these values\n\ntogether to form a possibly smaller set of values. Typically just zero or\n\none output value is produced per reduce invocation. The intermediate\n\nvalues are supplied to the user’s reduce function via an iterator. This\n\nallows us to handle lists of values that are too large to fit in memory.\n\n2.1 Example\n\nConsider the problem of counting the number of occurrences of each\n\nword in a large collection of documents. The user would write code\n\nsimilar to the following pseudocode.\n\nCOMMUNICATIONS OF THE ACM January  2008/Vol. 51, No. 1 107\n\n\n\n\n\n\n", "map(String key, String value):\n\n// key: document name\n\n// value: document contents\n\nfor each word w in value:\n\nEmitIntermediate(w, “1”);\n\nreduce(String key, Iterator values):\n\n// key: a word\n\n// values: a list of counts\n\nint result = 0;\n\nfor each v in values:\n\nresult += ParseInt(v);\n\nEmit(AsString(result));\n\nThe map function emits each word plus an associated count of\n\noccurrences (just 1 in this simple example). The reduce function\n\nsums together all counts emitted for a particular word.\n\nIn addition, the user writes code to fill in a mapreduce specification\n\nobject with the names of the input and output files and optional tun-\n\ning parameters. The user then invokes the MapReduce function, pass-\n\ning it to the specification object. The user’s code is linked together\n\nwith the MapReduce library (implemented in C++). Our original\n\nMapReduce paper contains the full program text for this example [8].\n\nMore than ten thousand distinct programs have been implemented\n\nusing MapReduce at Google, including algorithms for large-scale\n\ngraph processing, text processing, data mining, machine learning, sta-\n\ntistical machine translation, and many other areas. More discussion of\n\nspecific applications of MapReduce can be found elsewhere [8, 16, 7].\n\n2.2 Types\n\nEven though the previous pseudocode is written in terms of string\n\ninputs and outputs, conceptually the map and reduce functions sup-\n\nplied by the user have associated types.\n\nmap\n\nreduce\n\n(k1,v1)\n\n(k2,list(v2))\n\n→ list(k2,v2)\n\n→ list(v2)\n\nThat is, the input keys and values are drawn from a different domain\n\nthan the output keys and values. Furthermore, the intermediate keys\n\nand values are from the same domain as the output keys and values.\n\n3. Implementation\n\nMany different implementations of the MapReduce interface are pos-\n\nsible. The right choice depends on the environment. For example, one\n\nimplementation may be suitable for a small shared-memory machine,\n\nanother for a large NUMA multiprocessor, and yet another for an even\n\nlarger collection of networked machines. Since our original article, sev-\n\neral open source implementations of MapReduce have been developed\n\n[1, 2], and the applicability of MapReduce to a variety of problem\n\ndomains has been studied [7, 16].\n\nThis section describes our implementation of MapReduce that is tar-\n\ngeted to the computing environment in wide use at Google: large clusters\n\nof commodity PCs connected together with switched Gigabit Ethernet\n\n[4].  In  our  environment,  machines  are  typically  dual-processor  x86\n\nprocessors  running  Linux,  with  4-8GB  of  memory  per  machine.\n\nIndividual machines typically have 1 gigabit/second of network band-\n\nwidth, but the overall bisection bandwidth available per machine is con-\n\n108 January  2008/Vol. 51, No. 1 COMMUNICATIONS OF THE ACM\n\nsiderably less than 1 gigabit/second. A computing cluster contains many\n\nthousands of machines, and therefore machine failures are common.\n\nStorage is provided by inexpensive IDE disks attached directly to individ-\n\nual machines. GFS, a distributed file system developed in-house [10], is\n\nused to manage the data stored on these disks. The file system uses repli-\n\ncation to provide availability and reliability on top of unreliable hardware.\n\nUsers submit jobs to a scheduling system. Each job consists of a\n\nset of tasks, and is mapped by the scheduler to a set of available\n\nmachines within a cluster.\n\n3.1 Execution Overview\n\nThe map invocations are distributed across multiple machines by auto-\n\nmatically partitioning the input data into a set of M splits. The input\n\nsplits can be processed in parallel by different machines. Reduce invo-\n\ncations are distributed by partitioning the intermediate key space into R\n\npieces using a partitioning function (e.g., hash(key) mod R). The number\n\nof partitions (R) and the partitioning function are specified by the user.\n\nFigure 1 shows the overall flow of a MapReduce operation in our\n\nimplementation. When the user program calls the MapReduce func-\n\ntion, the following sequence of actions occurs (the numbered labels in\n\nFigure 1 correspond to the numbers in the following list). \n\n1. The MapReduce library in the user program first splits the input files\n\ninto M pieces of typically 16-64MB per piece (controllable by the\n\nuser via an optional parameter). It then starts up many copies of the\n\nprogram on a cluster of machines.\n\n2. One of the copies of the program—the master— is special. The rest\n\nare workers that are assigned work by the master. There are M map\n\ntasks and R reduce tasks to assign. The master picks idle workers and\n\nassigns each one a map task or a reduce task.\n\n3. A worker who is assigned a map task reads the contents of the corre-\n\nsponding input split. It parses key/value pairs out of the input data and\n\npasses each pair to the user-defined map function. The intermediate\n\nkey/value pairs produced by the map function are buffered in memory.\n\n4. Periodically, the buffered pairs are written to local disk, partitioned\n\ninto R regions by the partitioning function. The locations of these\n\nbuffered pairs on the local disk are passed back to the master who\n\nis responsible for forwarding these locations to the reduce workers.\n\n5. When a reduce worker is notified by the master about these loca-\n\ntions, it uses remote procedure calls to read the buffered data from\n\nthe local disks of the map workers. When a reduce worker has read\n\nall intermediate data for its partition, it sorts it by the intermediate\n\nkeys so that all occurrences of the same key are grouped together.\n\nThe sorting is needed because typically many different keys map to\n\nthe same reduce task. If the amount of intermediate data is too large\n\nto fit in memory, an external sort is used.\n\n6. The reduce worker iterates over the sorted intermediate data and for\n\neach unique intermediate key encountered, it passes the key and the\n\ncorresponding set of intermediate values to the user’s reduce func-\n\ntion. The output of the reduce function is appended to a final out-\n\nput file for this reduce partition.\n\n\n\n\n", "MapReduce: Simplified Data Processing on Large Clusters\n\n(1) fork\n\n(1) fork\n\nUser\n\nProgram\n\n(1) fork\n\nMaster\n\n(2)\n\nassign\n\nmap\n\n(2)\n\nassign\n\nreduce\n\n(3) read\n\nworker\n\n(4) local write\n\nworker\n\nworker\n\nsplit 0\n\nsplit 1\n\nsplit 2\n\nsplit 3\n\nsplit 4\n\nInput\n\nfiles\n\ne  \n\ne m o t\n\ne a d\n\nr\n\nr\n\n \n\n \n\n( 5 )\n\n( 5 )\n\n(6) write\n\nworker\n\noutput\n\nfile 0\n\nworker\n\noutput\n\nfile 1\n\nMap\n\nphasr\n\nIntermediate files\n\n(on local disks)\n\nReduce\n\nphase\n\nOutput\n\nfiles\n\nFig. 1. Execution overview.\n\n7. When all map tasks and reduce tasks have been completed, the mas-\n\nter wakes up the user program. At this point, the MapReduce call\n\nin the user program returns back to the user code.\n\nAfter successful completion, the output of the mapreduce execution\n\nis available in the R output files (one per reduce task, with file names\n\nspecified by the user). Typically, users do not need to combine these R\n\noutput files into one file; they often pass these files as input to another\n\nMapReduce call or use them from another distributed application that\n\nis able to deal with input that is partitioned into multiple files.\n\n3.2 Master Data Structures\n\nThe master keeps several data structures. For each map task and\n\nreduce task, it stores the state (idle, in-progress, or completed) and the\n\nidentity of the worker machine (for nonidle tasks).\n\nThe master is the conduit through which the location of interme-\n\ndiate file regions is propagated from map tasks to reduce tasks. There -\n\nfore, for each completed map task, the master stores the locations and\n\nsizes of the R intermediate file regions produced by the map task.\n\nUpdates to this location and size information are received as map tasks\n\nare completed. The information is pushed incrementally to workers\n\nthat have in-progress reduce tasks.\n\nHandling Worker Failures\n\nThe master pings every worker periodically. If no response is received\n\nfrom a worker in a certain amount of time, the master marks the worker\n\nas failed. Any map tasks completed by the worker are reset back to their\n\ninitial idle state and therefore become eligible for scheduling on other\n\nworkers. Similarly, any map task or reduce task in progress on a failed\n\nworker is also reset to idle and becomes eligible for rescheduling.\n\nCompleted map tasks are reexecuted on a failure because their out-\n\nput is stored on the local disk(s) of the failed machine and is therefore\n\ninaccessible. Completed reduce tasks do not need to be reexecuted\n\nsince their output is stored in a global file system.\n\nWhen a map task is executed first by worker A and then later exe-\n\ncuted by worker B (because A failed), all workers executing reduce\n\ntasks are notified of the reexecution. Any reduce task that has not\n\nalready read the data from worker A will read the data from worker B.\n\nMapReduce is resilient to large-scale worker failures. For example,\n\nduring one MapReduce operation, network maintenance on a running\n\ncluster was causing groups of 80 machines at a time to become unreach-\n\nable for several minutes. The MapReduce master simply re executed the\n\nwork done by the unreachable worker machines and continued to make\n\nforward progress, eventually completing the MapReduce operation.\n\n3.3 Fault Tolerance\n\nSince the MapReduce library is designed to help process very large\n\namounts of data using hundreds or thousands of machines, the library\n\nmust tolerate machine failures gracefully.\n\nSemantics in the Presence of Failures\n\nWhen the user-supplied map and reduce operators are deterministic\n\nfunctions of their input values, our distributed implementation pro-\n\nduces the same output as would have been produced by a nonfaulting\n\nsequential execution of the entire program.\n\nCOMMUNICATIONS OF THE ACM January  2008/Vol. 51, No. 1 109\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "We  rely  on  atomic  commits  of  map  and  reduce  task  outputs  to\n\nachieve this property. Each in-progress task writes its output to private\n\ntemporary files. A reduce task produces one such file, and a map task\n\nproduces R such files (one per reduce task). When a map task com-\n\npletes, the worker sends a message to the master and includes the names\n\nof the R temporary files in the message. If the master receives a comple-\n\ntion message for an already completed map task, it ignores the message.\n\nOtherwise, it records the names of R files in a master data structure.\n\nWhen a reduce task completes, the reduce worker atomically renames\n\nits temporary output file to the final output file. If the same reduce task\n\nis executed on multiple machines, multiple rename calls will be executed\n\nfor the same final output file. We rely on the atomic rename operation pro-\n\nvided by the underlying file system to guarantee that the final file system\n\nstate contains only the data produced by one execution of the reduce task.\n\nThe vast majority of our map and reduce operators are deterministic,\n\nand the fact that our semantics are equivalent to a sequential execution\n\nin this case makes it very easy for programmers to reason about their\n\nprogram’s behavior. When the map and/or reduce operators are nonde-\n\nterministic, we provide weaker but still reasonable semantics. In the\n\npresence of nondeterministic operators, the output of a particular\n\nreduce task R1 is equivalent to the output for R1 produced by a sequen-\n\ntial execution of the nondeterministic program. However, the output for\n\na different reduce task R2 may correspond to the output for R2 produced\n\nby a different sequential execution of the nondeterministic program.\n\nConsider map task M and reduce tasks R1 and R2. Let e(Ri) be the\n\nexecution of R1 that committed (there is exactly one such execution).\n\nThe weaker semantics arise because e(R1) may have read the output\n\nproduced by one execution of M, and e(R2) may have read the output\n\nproduced by a different execution of M.\n\n3.4 Locality\n\nNetwork bandwidth is a relatively scarce resource in our computing envi-\n\nronment. We conserve network bandwidth by taking advantage of the\n\nfact that the input data (managed by GFS [10]) is stored on the local\n\ndisks of the machines that make up our cluster. GFS divides each file\n\ninto 64MB blocks and stores several copies of each block (typically 3\n\ncopies) on different machines. The MapReduce master takes the loca-\n\ntion information of the input files into account and attempts to schedule\n\na map task on a machine that contains a replica of the corresponding\n\ninput data. Failing that, it attempts to schedule a map task near a replica\n\nof that task’s input data (e.g., on a worker machine that is on the same\n\nnetwork switch as the machine containing the data). When running large\n\nMapReduce operations on a significant fraction of the workers in a clus-\n\nter, most input data is read locally and consumes no network bandwidth.\n\n3.5 Task Granularity\n\nWe subdivide the map phase into M pieces and the reduce phase into\n\nR pieces as described previously. Ideally, M and R should be much\n\nlarger than the number of worker machines. Having each worker per-\n\nform many different tasks improves dynamic load balancing and also\n\nspeeds up recovery when a worker fails: the many map tasks it has\n\ncompleted can be spread out across all the other worker machines.\n\nThere are practical bounds on how large M and R can be in our imple-\n\nmentation since the master must make O(M+R) scheduling decisions\n\nand keep O(M*R) state in memory as described. (The constant factors\n\nfor memory usage are small, however. The O(M*R) piece of the state\n\nconsists of approximately one byte of data per map task/ reduce task pair.)\n\n110 January  2008/Vol. 51, No. 1 COMMUNICATIONS OF THE ACM\n\nFurthermore, R is often constrained by users because the output of\n\neach reduce task ends up in a separate output file. In practice, we tend\n\nto choose M so that each individual task is roughly 16MB to 64MB of\n\ninput data (so that the locality optimization described previously is most\n\neffective), and we make R a small multiple of the number of worker\n\nmachines we expect to use. We often perform MapReduce computa-\n\ntions with M=200,000 and R=5,000, using 2,000 worker machines.\n\n3.6 Backup Tasks\n\nOne of the common causes that lengthens the total time taken for a\n\nMapReduce operation is a straggler, that is,  a machine that takes an\n\nunusually long time to complete one of the last few map or reduce tasks\n\nin the computation. Stragglers can arise for a whole host of reasons. For\n\nexample, a machine with a bad disk may experience frequent cor-\n\nrectable errors that slow its read performance from 30MB/s to 1MB/s.\n\nThe cluster scheduling system may have scheduled other tasks on the\n\nmachine, causing it to execute the MapReduce code more slowly due\n\nto competition for CPU, memory, local disk, or network bandwidth. A\n\nrecent problem we experienced was a bug in machine initialization\n\ncode that caused processor caches to be disabled: computations on\n\naffected machines slowed down by over a factor of one hundred.\n\nWe have a general mechanism to alleviate the problem of stragglers.\n\nWhen a MapReduce operation is close to completion, the master\n\nschedules backup executions of the remaining in-progress tasks. The\n\ntask  is  marked  as  completed  whenever  either  the  primary  or  the\n\nbackup execution completes. We have tuned this mechanism so that it\n\ntypically increases the computational resources used by the operation\n\nby no more than a few percent. We have found that this significantly\n\nreduces the time to complete large MapReduce operations. As an\n\nexample, the sort program described in Section 5.3 takes 44% longer\n\nto complete when the backup task mechanism is disabled.\n\n4 Refinements\n\nAlthough the basic functionality provided by simply writing map and\n\nreduce functions is sufficient for most needs, we have found a few\n\nextensions useful. These include:\n\n(cid:129) user-specified partitioning functions for determining the mapping\n\nof intermediate key values to the R reduce shards; \n\n(cid:129) ordering guarantees: Our implementation guarantees that within\n\neach of the R reduce partitions, the intermediate key/value pairs are\n\nprocessed in increasing key order; \n\n(cid:129) user-specified combiner functions for doing partial combination of\n\ngenerated intermediate values with the same key within the same\n\nmap task (to reduce the amount of intermediate data that must be\n\ntransferred across the network); \n\n(cid:129) custom input and output types, for reading new input formats and\n\nproducing new output formats; \n\n(cid:129) a mode for execution on a single machine for simplifying debugging\n\nand small-scale testing.\n\nThe original article has more detaile d discussions of each of these\n\nitems [8].\n\n\n\n\n", "5 Performance\n\nIn this section, we measure the performance of MapReduce on two com-\n\nputations  running  on  a  large  cluster  of  machines.  One  computation\n\nsearches through approximately one terabyte of data looking for a particu-\n\nlar pattern. The other computation sorts approximately one terabyte of data.\n\nThese two programs are representative of a large subset of the real\n\nprograms written by users of MapReduce—one class of programs\n\nshuffles data from one representation to another, and another class\n\nextracts a small amount of interesting data from a large dataset.\n\n5.1 Cluster Configuration\n\nAll of the programs were executed on a cluster that consisted of approx-\n\nimately 1800 machines. Each machine had two 2GHz Intel Xeon\n\nprocessors  with  Hyper-Threading  enabled,  4GB  of  memory,  two\n\n160GB IDE disks, and a gigabit Ethernet link. The machines were\n\narranged in a two-level tree-shaped switched network with approxi-\n\nmately 100-200Gbps of aggregate bandwidth available at the root. All of\n\nthe machines were in the same hosting facility and therefore the round-\n\ntrip time between any pair of machines was less than a millisecond.\n\nOut of the 4GB of memory, approximately 1-1.5GB was reserved by\n\nother tasks running on the cluster. The programs were executed on a\n\nweekend afternoon when the CPUs, disks, and network were mostly idle.\n\n5.2 Grep\n\nThe grep program scans through 1010 100-byte records, searching for a rel-\n\natively rare three-character pattern (the pattern occurs in 92,337 records).\n\nThe input is split into approximately 64MB pieces (M = 15000), and the\n\nentire output is placed in one file (R = 1).\n\nFig. 2. Data transfer rate over time (mr-grep).\n\nFigure 2 shows the progress of the computation over time. The\n\nY-axis shows the rate at which the input data is scanned. The rate grad-\n\nually picks up as more machines are assigned to this MapReduce com-\n\nputation and peaks at over 30 GB/s when 1764 workers have been\n\nassigned. As the map tasks finish, the rate starts dropping and hits zero\n\nabout 80 seconds into the computation. The entire computation takes\n\napproximately 150 seconds from start to finish. This includes about a\n\nminute of startup overhead. The overhead is due to the propagation of\n\nthe program to all worker machines and delays interacting with GFS to\n\nopen the set of 1000 input files and to get the information needed for\n\nthe locality optimization.\n\n5.3 Sort\n\nThe sort program sorts 1010 100-byte records (approximately 1 terabyte\n\nof data). This program is modeled after the TeraSort benchmark [12].\n\nMapReduce: Simplified Data Processing on Large Clusters\n\nThe sorting program consists of less than 50 lines of user code. The\n\nfinal sorted output is written to a set of 2-way replicated GFS files (i.e.,\n\n2 terabytes are written as the output of the program).\n\nAs before, the input data is split into 64MB pieces (M = 15000). We\n\npartition the sorted output into 4000 files (R = 4000). The partitioning\n\nfunction uses the initial bytes of the key to segregate it into one of  pieces.\n\nOur partitioning function for this benchmark has built-in knowl-\n\nedge of the distribution of keys. In a general sorting program, we would\n\nadd a prepass MapReduce operation that would collect a sample of the\n\nkeys and use the distribution of the sampled keys to compute split-\n\npoints for the final sorting pass.\n\nFig. 3. Data transfer rate over time (mr-sort).\n\nFigure 3 shows the progress of a normal execution of the sort pro-\n\ngram. The top-left graph shows the rate at which input is read. The rate\n\npeaks at about 13GB/s and dies off fairly quickly since all map tasks fin-\n\nish before 200 seconds have elapsed. Note that the input rate is less\n\nCOMMUNICATIONS OF THE ACM January  2008/Vol. 51, No. 1 111\n\n\n\n\n\n\n\n\n", "than for grep. This is because the sort map tasks spend about half their\n\ntime and I/O bandwidth writing intermediate output to their local disks.\n\nThe corresponding intermediate output for grep had negligible size.\n\nA few things to note: the input rate is higher than the shuffle rate\n\nand the output rate because of our locality optimization; most data is\n\nread from a local disk and bypasses our relatively bandwidth con-\n\nstrained network. The shuffle rate is higher than the output rate\n\nbecause the output phase writes two copies of the sorted data (we\n\nmake two replicas of the output for reliability and availability reasons).\n\nWe write two replicas because that is the mechanism for reliability and\n\navailability provided by our underlying file system. Network bandwidth\n\nrequirements for writing data would be reduced if the underlying file\n\nsystem used erasure coding [15] rather than replication.\n\nThe  original  article  has  further  experiments  that  examine  the\n\neffects of backup tasks and machine failures [8].\n\n6 Experience\n\nWe wrote the first version of the MapReduce library in February of\n\n2003 and made significant enhancements to it in August of 2003,\n\nincluding the locality optimization, dynamic load balancing of task exe-\n\ncution across worker machines, etc. Since that time, we have been\n\npleasantly surprised at how broadly applicable the MapReduce library\n\nhas been for the kinds of problems we work on. It has been used\n\nacross a wide range of domains within Google, including:\n\n(cid:129) large-scale machine learning problems, \n\n(cid:129) clustering problems for the Google News and Froogle products, \n\n(cid:129) extracting data to produce reports of popular queries (e.g. Google\n\nZeitgeist and Google Trends), \n\n(cid:129) extracting properties of Web pages for new experiments and prod-\n\nucts (e.g. extraction of geographical locations from a large corpus of\n\nWeb pages for localized search), \n\n(cid:129) processing of satellite imagery data, \n\n(cid:129) language model processing for statistical machine translation, and \n\n(cid:129) large-scale graph computations.\n\nFig. 4. MapReduce instances over time.\n\n112 January  2008/Vol. 51, No. 1 COMMUNICATIONS OF THE ACM\n\nFigure 4 shows the significant growth in the number of separate\n\nMapReduce programs checked into our primary source-code manage-\n\nment system over time, from 0 in early 2003 to almost 900 in Septem -\n\nber 2004, to about 4000 in March 2006. MapReduce has been so\n\nsuccessful because it makes it possible to write a simple program and\n\nrun it efficiently on a thousand machines in a half hour, greatly speed-\n\ning up the development and prototyping cycle. Furthermore, it allows\n\nprogrammers who have no experience with distributed and/or parallel\n\nsystems to exploit large amounts of resources easily.\n\nTable I. MapReduce Statistics for Different Months.\n\nNumber of jobs (1000s)\n\nAvg. completion time (secs)\n\nMachine years used\n\nmap input data (TB)\n\nmap output data (TB)\n\nreduce output data (TB)\n\nAvg. machines per job\n\nUnique implementations\n\nmap\n\nreduce\n\nAug. ’04\n\n29\n\n634\n\n217\n\n3,288\n\n758\n\n193\n\n157\n\nMar. ’06\n\n171\n\n874\n\n2,002\n\n52,254\n\n6,743\n\n2,970 \n\n268\n\nSep. ’07\n\n2,217\n\n395\n\n11,081\n\n403,152\n\n34,774\n\n14,018\n\n394\n\n395\n\n269\n\n1958\n\n1208\n\n4083\n\n2418\n\nAt the end of each job, the MapReduce library logs statistics about\n\nthe computational resources used by the job. In Table I, we show some\n\nstatistics  for  a  subset  of  MapReduce  jobs  run  at  Google  in  various\n\nmonths, highlighting the extent to which MapReduce has grown and\n\nbecome the de facto choice for nearly all data processing needs at Google.\n\n6.1 Large-Scale Indexing\n\nOne of our most significant uses of MapReduce to date has been a\n\ncomplete rewrite of the production indexing system that produces the\n\ndata structures used for the Google Web search service. The indexing\n\nsystem takes as input a large set of documents that have been retrieved\n\nby our crawling system, stored as a set of GFS files. The raw contents\n\nfor these documents are more than 20 terabytes of data. At the time\n\nwe converted the indexing system to use MapReduce in 2003, it ran as\n\na sequence of eight MapReduce operations. Since that time, because\n\nof the ease with which new phases can be added, many new phases\n\nhave been added to the indexing system. Using MapReduce (instead\n\nof the ad-hoc distributed passes in the prior version of the indexing\n\nsystem) has provided several benefits.\n\n(cid:129) The indexing code is simpler, smaller, and easier to understand be-\n\ncause the code that deals with fault tolerance, distribution, and par-\n\nallelization is hidden within the MapReduce library. For example, the\n\nsize of one phase of the computation dropped from approximately\n\n3800 lines of C++ code to approximately 700 lines when expressed\n\nusing MapReduce.\n\n(cid:129) The performance of the MapReduce library is good enough that we\n\ncan keep conceptually unrelated computations separate instead of\n\nmixing them together to avoid extra passes over the data. This makes\n\nit easy to change the indexing process. For example, one change that\n\ntook a few months to make in our old indexing system took only a\n\nfew days to implement in the new system.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "(cid:129) The indexing process has become much easier to operate because\n\nmost of the problems caused by machine failures, slow machines,\n\nand networking hiccups are dealt with automatically by the MapRe-\n\nduce library without operator intervention. Furthermore, it is easy to\n\nimprove the performance of the indexing process by adding new ma-\n\nchines to the indexing cluster.\n\n7 Related Work\n\nMany systems have provided restricted programming models and used\n\nthe restrictions to parallelize the computation automatically. For example,\n\nan associative function can be computed over all prefixes of an N element\n\narray in log N time on N processors using parallel prefix computations [6,\n\n11, 14]. MapReduce can be considered a simplification and distillation of\n\nsome of these models based on our experience with large real-world com-\n\nputations. More significantly, we provide a fault-tolerant implementation\n\nthat scales to thousands of processors. In contrast, most of the parallel\n\nprocessing systems have only been implemented on smaller scales and\n\nleave the details of handling machine failures to the programmer.\n\nOur locality optimization draws its inspiration from techniques\n\nsuch as active disks [13, 17], where computation is pushed into pro-\n\ncessing elements that are close to local disks, to reduce the amount of\n\ndata sent across I/O subsystems or the network.\n\nThe sorting facility that is a part of the MapReduce library is simi-\n\nlar in operation to NOW-Sort [3]. Source machines (map workers)\n\npartition the data to be sorted and send it to one of R reduce workers.\n\nEach reduce worker sorts its data locally (in memory if possible). Of\n\ncourse NOW-Sort does not have the user-definable map and reduce\n\nfunctions that make our library widely applicable.\n\nBAD-FS [5] and TACC [9] are two other systems that rely on re -\n\nexecution as a mechanism for implementing fault tolerance.\n\nThe original article has a more complete treatment of related work [8].\n\nConclusions\n\nThe  MapReduce  programming  model  has  been  successfully  used  at\n\nGoogle for many different purposes. We attribute this success to several\n\nreasons. First, the model is easy to use, even for programmers without ex-\n\nperience with parallel and distributed systems, since it hides the details\n\nof parallelization, fault tolerance, locality optimization, and load balanc-\n\ning. Second, a large variety of problems are easily expressible as MapRe-\n\nduce computations. For example, MapReduce is used for the generation\n\nof data for Google’s production Web search service, for sorting, data min-\n\ning, machine learning, and many other systems. Third, we have developed\n\nan implementation of MapReduce that scales to large clusters of ma-\n\nchines comprising thousands of machines. The implementation makes\n\nefficient use of these machine resources and therefore is suitable for use\n\non many of the large computational problems encountered at Google.\n\nBy restricting the programming model, we have made it easy to par-\n\nallelize and distribute computations and to make such computations\n\nfault tolerant. Second, network bandwidth is a scarce resource. A\n\nnumber of optimizations in our system are therefore targeted at reduc-\n\ning the amount of data sent across the network: the locality optimiza-\n\ntion allows us to read data from local disks, and writing a single copy\n\nof the intermediate data to local disk saves network bandwidth. Third,\n\nredundant  execution  can  be  used  to  reduce  the  impact  of  slow\n\nmachines, and to handle machine failures and data loss.\n\nMapReduce: Simplified Data Processing on Large Clusters\n\nAcknowledgements\n\nJosh Levenberg has been instrumental in revising and extending the user-\n\nlevel MapReduce API with a number of new features. We would like to es-\n\npecially thank others who have worked on the system and all the users of\n\nMapReduce  in  Google’s  engineering  organization  for  providing  helpful\n\nfeedback, suggestions, and bug reports.\n\nReferences\n\n1. Hadoop: Open source implementation of MapReduce. http://\n\nlucene. apache.org/hadoop/.\n\n2.\n\nThe Phoenix system for MapReduce programming. http:// csl.\n\n stanford. edu/~christos/sw/phoenix/.\n\n3. Arpaci-Dusseau, A. C., Arpaci-Dusseau, R. H., Culler, D. E., Heller-\n\nstein, J. M., and Patterson, D. A. 1997. High-performance sorting on\n\nnetworks of workstations. In Proceedings of the 1997 ACM SIGMOD\n\nInternational Conference on Management of Data. Tucson, AZ.\n\n4 Barroso, L. A., Dean, J., and Urs Hölzle, U. 2003. Web search for a\n\nplanet: The Google cluster architecture. IEEE Micro 23, 2, 22-28.\n\n5. Bent, J., Thain, D., Arpaci-Dusseau, A. C., Arpaci-Dusseau, R. H.,\n\nand Livny, M. 2004. Explicit control in a batch-aware distributed file\n\nsystem. In Proceedings of the 1st USENIX Symposium on Networked\n\nSystems Design and Implementation (NSDI).\n\n6. Blelloch, G. E. 1989. Scans as primitive parallel operations. IEEE\n\nTrans. Comput. C-38, 11.\n\n7. Chu, C.-T., Kim, S. K., Lin, Y. A., Yu, Y., Bradski, G., Ng, A., and\n\nOlukotun, K. 2006. Map-Reduce for machine learning on multicore.\n\nIn Proceedings of Neural Information Processing Systems Conference\n\n(NIPS). Vancouver, Canada.\n\n8. Dean, J. and Ghemawat, S. 2004. MapReduce: Simplified data pro-\n\ncessing on large clusters. In Proceedings of Operating Systems Design\n\nand Implementation (OSDI). San Francisco, CA. 137-150.\n\n9. Fox, A., Gribble, S. D., Chawathe, Y., Brewer, E. A., and Gauthier, P.\n\n1997. Cluster-based scalable network services. In Proceedings of the\n\n16th ACM Symposium on Operating System Principles. Saint-Malo,\n\nFrance. 78-91.\n\n10. Ghemawat, S., Gobioff, H., and Leung, S.-T. 2003. The Google file\n\nsystem. In 19th Symposium on Operating Systems Principles. Lake\n\nGeorge, NY. 29-43.\n\n11. Gorlatch, S. 1996. Systematic efficient parallelization of scan and\n\nother list homomorphisms. In L. Bouge, P. Fraigniaud, A. Mignotte,\n\nand Y. Robert, Eds. Euro-Par’96. Parallel Processing, Lecture Notes in\n\nComputer Science, vol. 1124. Springer-Verlag. 401-408\n\n12. Gray, J. Sort benchmark home page. http://  research. microsoft. com/\n\nbarc/ SortBenchmark/.\n\n13. Huston, L., Sukthankar, R., Wickremesinghe, R., Satyanarayanan, M.,\n\nGanger, G. R., Riedel, E., and Ailamaki, A. 2004. Diamond: A storage\n\narchitecture for early discard in interactive search. In Proceed ings of\n\nthe 2004 USENIX File and Storage Technologies FAST Conference.\n\n14. Ladner, R. E., and Fischer, M. J. 1980. Parallel prefix computation.\n\nJACM 27, 4. 831-838.\n\n15. Rabin, M. O. 1989. Efficient dispersal of information for security,\n\nload balancing and fault tolerance. JACM 36, 2. 335-348.\n\n16. Ranger, C., Raghuraman, R., Penmetsa, A., Bradski, G., and\n\nKozyrakis, C. 2007. Evaluating mapreduce for multi-core and multi-\n\nprocessor systems. In Proceedings of 13th International Symposium on\n\nHigh-Performance Computer Architecture (HPCA). Phoenix, AZ.\n\n17. Riedel, E., Faloutsos, C., Gibson, G. A., and Nagle, D. Active disks\n\nfor large-scale data processing. IEEE Computer. 68-74.\n\nCOMMUNICATIONS OF THE ACM January  2008/Vol. 51, No. 1 113\n\n\n\n\n\n"]], ["C:\\Users\\mjols\\Documents\\DS UWEC courses\\730 Big Data\\lesson1 intro\\ACTIVITY01_High_Performance_Computing_Introduction.pdf", ["High Performance Computing\n\nIntroduction\n\nDS730\n\nBooks & Other Software\n\nHortonworks Sandbox\n\nActivity Tasks\n\nTask 0: Using Piazza\n\nTask 1: Get Registered with Amazon Web Services\n\nTask 2: Create a Linux Machine on AWS\n\nUsing the Command Line\n\nTest Connection and Stop Instance\n\nTask 3: Create a Hortonworks Sandbox in the Cloud with AWS\n\nConnect to the Hortonworks Filesystem with Cyberduck\n\nTask 4: Testing Java\n\nTask 5: Test Python\n\nRun Python program from terminal\n\nReading input from redirected file input\n\nCreate a Factorial Calculation Program\n\nOutput Average of Integers\n\nOutput Prime Numbers\n\nInterpret Dirty Data\n\nTask 6: Submitting your Work\n\nOptional Tasks\n\nOptional Task 1: Create a Hortonworks Sandbox on Azure\n\nOptional Task 2: Create a Local Hortonworks Sandbox\n\nDS 730: Activity 1 / Page 1 of 51\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "In this activity, you will be getting familiar with many of the tools we will use in this\n\ncourse. We will be using a preconfigured Linux machine in the cloud and/or locally that\n\nhas all of the software we need on it. In order to do some true high performance\n\ncomputing, we will be using a cloud service. Your organization may not have a high\n\nperformance computing cluster available and may not be willing to spend tens/hundreds\n\nof thousands of dollars setting one up. As an added benefit of taking this course,\n\nthroughout the activities, you will learn how to start up a large cluster of machines, use it\n\nto analyze your data and then shut it down when you are done. This solution allows you\n\nto only pay for the computing power that you need without paying the huge upfront cost\n\nof setting up a cluster.\n\nMany of the activities are long. However, we are assuming that you have no experience\n\nwith the command line, Linux, SSH keys, Hadoop, Spark, Hortonworks, etc. Many of the\n\ncommands are explained in detail and there are a lot of pictures. If you are more\n\nexperienced, please don’t accidentally skip a step or command. Many of the tasks need\n\nto be done in order and exactly as written or the entire setup may not work. Because of\n\nthat, you are encouraged to start task 3 and the optional tasks only if you know you’ll be\n\nable to finish them. To give you some time estimate, a person experienced with Linux,\n\nSSH keys and the command line can finish task 3 or the optional tasks in 10-15\n\nminutes. I’ve been told that someone with no experience might need 90-120 minutes to\n\nfinish each of those tasks.\n\nImportant:\n\nYou are welcome to use newer versions of the software but you do so at your own\n\nrisk. At the time of this writing, the software shown is the most appropriate software\n\navailable and everything has been tested with this setup. It’s possible a newer version\n\ncomes out between the modification of this document and when you are taking this\n\ncourse. It’s likely that everything written here will apply to future updates. However, if\n\nsomething goes wrong with newer software, you will be on your own for\n\ntroubleshooting it.\n\nIf you already have access to a cluster/machine with\n\nHadoop/Pig/Hive/Spark/Kafka/Storm installed on it, you can skip tasks 1-3 and the\n\noptional tasks. The goal of tasks 1-3 and the 2 optional tasks is to spin up a virtual\n\nmachine with the Hortonworks sandbox installed on it. If you are using a cloud\n\nsolution, always keep a backup of your code locally on your desktop/laptop just\n\nin case there are issues. Here are the benefits and disadvantages of each:\n\nDS 730: Activity 1 / Page 2 of 51\n\n\n\n\n\n\n", "Optional Task 1 - A cloud solution that allows you to connect to Hortonworks anywhere\n\nyou have the Internet. Allows you to create a machine with several cores and enough\n\nmemory to run Hortonworks. A disadvantage is that it is cloud based and therefore you\n\nare at the mercy of a 3rd party and the Internet. If Microsoft is lagging when you want to\n\nwork, it is annoying having to wait long periods for things to start up. Please don’t let the\n\nnegatives dissuade you from trying out Microsoft. It works for >90% of people with no\n\nproblems. Another disadvantage may be the credits. Azure provides $100 in credits for\n\nstudents per year. If you have used your credits in another course or for some other\n\nreason, you have no way of getting more credits. Microsoft has been quite strict about\n\ngiving out more credits if you use up your allocated amount. We cannot give out more\n\ncredits on Azure and we have yet to hear of a case where Microsoft has given extra\n\ncredits to someone. You are also on your own with setting up an Azure account.\n\nStep-by-step instructions are provided for creating an account and they work well in the\n\nvast majority of cases. However, they don’t work in about 5% of cases . We do not\n\ncontrol any accounts with respect to Microsoft/Azure. Therefore, if your account is\n\nlocked or you aren’t able to get access or your credits aren’t showing up, Microsoft\n\nsupport is your only solution . If you use Azure, be very careful to shutdown your virtual\n\nmachines when you are done using them. The credits are enough to provide 30+ hours\n\nof working time per week. But if you forget to shut down your machine, you can burn\n\nthrough credits in a few weeks.\n\n1\n\n2\n\n3\n\nOptional Task 2 - A local Hortonworks solution . You install Hortonworks locally and\n\ntherefore can use it at any time without being connected to the Internet. Your system will\n\nrun as fast as your machine will allow. It is recommended that you have about 60GB of\n\nfree space available and 8GB of memory available (not total memory, but available\n\nmemory). The disadvantage is being tied to 1 machine. Your machine may be a work\n\nmachine and may be locked down such that you cannot install software on your\n\nmachine. Some installs have required minor changes to the BIOS in order to get\n\nVirtualBox to work. If your virtual machine breaks or gets into a bad state, you will have\n\nto troubleshoot the problem as we cannot connect to your machine to see what’s going\n\non.\n\n1 This is usually true if you have signed up with or used Azure in the past. If you have exhausted your\n\ncredits or have used your free trial, there is nothing we can do to reinstate those things.\n\n2 Microsoft support may claim this is a school issue and ask you to contact your school for help.\n\nUnfortunately, this is not us (DS730 instructors). You could try contacting the helpdesk on your local\n\ncampus if Microsoft points you to back to your campus. The bottom line, if there are account issues with\n\nMicrosoft, we have no power to help.\n\n3 Please note that installing Hortonworks locally is only a good idea if you are comfortable with virtual\n\nmachines and setting them up. The instructors do not have the ability to troubleshoot a broken/failed local\n\ninstall because it is impossible to know what configuration you have and how to fix it.\n\nDS 730: Activity 1 / Page 3 of 51\n\n\n\n", "Task 1 - We strongly recommend doing this option. A cloud solution that works on\n\nAmazon. Similar advantages and disadvantages to Optional Task 1. The biggest\n\nadvantage here is that we control the credits and we can easily connect to your\n\nmachine if there is a problem. If you run out of credits, we have the ability to give you\n\nanother account such that you can create a new machine and start over. Note that we\n\ncannot give your current account more credits. If you run out of credits, you’ll have to\n\nstart over with a brand new account and a brand new virtual machine.\n\nBooks & Other Software\n\nBecause of the fast paced nature of this area, there is no required book for this course.\n\nHowever, many of the activities are quite long and detailed so one could consider the\n\nset of activities the book for this course. The advantage of this kind of “book” is that it\n\ncan be updated quickly if there is an update that needs to be made. If you come across\n\na portion of text that no longer makes sense or see an outdated image, let your\n\ninstructor know right away so we can update the document.\n\nOne of the most common questions asked in this course is why we don’t learn tool XYZ\n\nwhere XYZ is the best thing we can learn in this course. As the world of high\n\nperformance computing changes, the goal of this course is to keep up with the newest\n\ntechnologies and present you the newest versions of each software we use. At the\n\nsame time, we want to teach the core concepts of these technologies. We do not want\n\nyou to be pigeonholed into using a specific software tool to solve a problem. If you learn\n\nthe core concepts of several paradigms, then you should be able to pick up whatever\n\nthe software-du-jour is when you need it.\n\nWe have tried to make entering in commands as painless as possible. Therefore,\n\nunless otherwise noted, every command in every activity should be able to be copied\n\nand pasted. This also reduces manually copying issues like figuring out the difference\n\nbetween 0 and O and the difference between 1, I and l ← this one is a lowercase L.  If\n\nyou come across a command that does not copy and paste correctly, please inform an\n\ninstructor so we can fix the issue.\n\nHortonworks Sandbox\n\nIt is important to understand what is going on inside the sandbox. The sandbox is\n\nessentially simulating a cluster of machines that run Hadoop, Pig, Hive and other pieces\n\nof software. You do not have to install each piece individually. Rather, you can use the\n\nsoftware straight out of the box without having to deal with the administrative part of it. If\n\nDS 730: Activity 1 / Page 4 of 51\n\n\n", "you want to know more about how to install some of the software, see the optional\n\nactivities for how to setup a Linux machine, install Hadoop, Pig, Hive, etc. Note that the\n\nload times of Hortonworks are generally at least 5 minutes and sometimes closer\n\nto 20 minutes. Do not expect to start up your virtual machine and start working\n\nimmediately. It takes a while to start all of this software.\n\nIn this course, we will focus on two parts of the Hadoop environment but there are many\n\nother parts and pieces of software you can learn about.\n\nThe first part is the Hadoop Distributed File System (HDFS). HDFS allows for scalable\n\nand reliable data storage. It is designed to run on many “cheap” computers. HDFS\n\nstores data in large chunks and each chunk is stored on multiple machines (in general)\n\nto provide reliable access to the data. For example, consider chunk X is stored on\n\nmachines A, B and C. Assume you start a job that needs to use chunk X. If A and B are\n\nbusy doing other work, you can run your job on chunk X on machine C. This runs into\n\nthe classic space-vs-time problem. You can replicate your data as much as you want\n\nacross numerous machines. If you replicate a lot, your total storage capacity will go\n\ndown but your processing time will also go down. If you replicate a little, your total\n\nstorage capacity will go up but your processing time will also go up. We won’t spend\n\nmuch time looking into the innards of HDFS. A general knowledge that HDFS is\n\ndistributed storage is sufficient for this course.\n\nOne potential confusing part about the sandbox is that you will have 3 filesystems. If\n\nyou are having trouble remembering what you are connecting to, come back to this part.\n\nYou can likely skim over this part for now as most of this will not make sense until you\n\nare finished with the activity. If you create a file on one filesystem, it will not be\n\naccessible on another filesystem . You need to copy/move files from one filesystem to\n\nthe other in order to use them. There will be ways to copy/move files from filesystem to\n\nfilesystem shown in the activities. However, there are not always direct connections that\n\ncan be easily made. You may have to copy/move files from filesystem A to filesystem B\n\nand then from filesystem B to filesystem C in order to get from filesystem A to filesystem\n\nC. Here are the filesystems you’ll be dealing with:\n\n4\n\n1. Linux filesystem - this is the filesystem that you connect to if you are using port\n\n22 with CyberDuck or PuTTy. After this activity, you shouldn’t have to connect to\n\nthis filesystem.\n\n4 It’s the same concept of having two different machines. If I create a file on my machine, you can’t access\n\nit on your machine. In order for you to access the file, I need to send the file to you.\n\nDS 730: Activity 1 / Page 5 of 51\n\n\n\n", "2. Hortonworks filesystem - this is the filesystem that you see what you connect\n\nto port 4200 via the browser. You can also connect to this filesystem directly with\n\nPuTTy or CyberDuck using port 2222.\n\n3. HDFS - the hadoop distributed filesystem. This is the filesystem that you see\n\nwhen you go to the Files View in Ambari. You are able to put files here and\n\ndownload files using the Files View.\n\n4. Your laptop/desktop’s filesystem - Although not part of the Hadoop ecosystem,\n\nyour local file system is important too. When you are uploading files to canvas,\n\nthose files will need to be on your local filesystem before you submit them.\n\nThe main part we will concern ourselves with is how to access and manipulate the data.\n\nIn this course, we will use MapReduce, Pig (or Kafka), Hive and Spark. As of now, you\n\nhave the option to “open up the black box” a little bit and see how Hadoop actually\n\nworks. How does Hadoop distribute processing across multiple threads on a single\n\nmachine? The activity/project associated with threading will show how this is done. If\n\nyou are not interested in opening up the black box, there is an alternative option\n\nlearning streaming tools instead. The threading alternative introduces you to other big\n\ndata software: Storm and Redis. There are many other software titles available inside\n\nHortonworks and if there is enough interest, optional activities can be created. The final\n\nproject also gives you an opportunity to learn a brand new big data tool and create an\n\nactivity for it.\n\nActivity Tasks\n\nTask 0: Using Piazza\n\nPiazza is the only way you should be communicating with your instructors in this\n\ncourse. We know that all messages on piazza are class related. Because your piazza\n\nmessages are class related, they are important. Not all of our emails are important. We\n\nwill always check and respond to piazza first. On a related note, you should never use\n\nthe messaging system that is built into canvas. Canvas messaging is not a tool that we\n\nuse in this course and your message likely won’t be seen.\n\nGo to the online discussion board on Piazza (use the Introductions thread that is pinned\n\nat the top left of the page) and enter in some information about yourself. Since this is\n\nnot a face-to-face course, we will likely never meet most of you. However, we would still\n\nlike to know something about you. You can put whatever you want in that post. Tell us\n\nwho you are, why you are interested in our data science degree, any interests or\n\nhobbies, what you hope to learn from this course, etc. You can write as much or as little\n\nDS 730: Activity 1 / Page 6 of 51\n\n\n", "as you want. If you have questions for us, feel free to add those and we’ll answer them\n\ndirectly. We know you’ve probably had to do something similar in other courses so feel\n\nfree to copy and paste that “intro document” so we can get to know you a little bit. There\n\nare a few added requirements for the piazza introduction post that you must\n\nadhere to:\n\n1. Post a follow-up discussion to the Introductions thread that has already\n\nbeen posted on piazza. Do not start a new thread. When you are using piazza\n\nthis semester, if your question is similar/related to an already posted question,\n\nplease use the follow-up feature. This ensures that all questions related to a\n\ntask/problem are in 1 location. Please use the search feature on piazza to see if\n\nyour question has already been answered.\n\n2. Post your introductory follow-up anonymously. There may be times where\n\nyou wish you post anonymously this semester and we want to be sure you know\n\nhow to do it. See the highlighted dropdown box below. Please note that\n\nanonymous posts are anonymous to other students in the course but not for your\n\ninstructors. We know who is posting it (and therefore you will get credit for doing\n\nit).\n\n3. Add your favorite chunk of code to your introduction message. Click on the\n\ncode button (see highlighted code below). If you are ever posting code in a\n\nthread, be sure to use this feature. If you copy and paste code, it will keep your\n\nindentation and your code will look much nicer.\n\n4. Once you have posted your follow-up, click on the Resolved button above your\n\npost. Please use this feature throughout the semester when your follow-up is\n\nanswered/fixed so we know your issue is resolved.\n\nDS 730: Activity 1 / Page 7 of 51\n\n\n\n", "5. Lastly, send a private note (not a question) to the instructors on piazza.\n\nThere may be times during the semester where you wish to post a link or\n\nsomething for the entire class to see (or just the instructors) that is more of a\n\ncomment/statement instead of a question. Posting those as a note instead of a\n\nquestion helps us determine what threads need to be answered.\n\nExplore the other features that piazza offers. There is a LaTeX equation editor for those\n\nof you who know LaTeX. You can add a table. You can insert a file, an image, etc. We\n\ndon’t care what it says, just post anything. During the semester, if you want to post a lot\n\nof code, ask a question or send a comment that is only meant for the instructors, please\n\nuse a private question/note on piazza instead of sending an email.\n\nTask 1: Get Registered with Amazon Web Services\n\nUsing the Hortonworks sandbox is a nice way to simulate running your code on many\n\nmachines. The reality is, your “cluster” is simply 1 machine that is running 4 CPUs and\n\nhas 16GB of memory. Using Hortonworks on 1 machine is a nice way to test your code.\n\nIf you want to do true high performance computing to tackle terabytes (or higher) of\n\ndata, you would want to use a larger cluster. Amazon Web Services (AWS) allows you\n\nto spin up as large of a cluster as you need and shut it down whenever you are finished\n\nwith it. Please note that this task cannot be accomplished until the semester begins .5\n\nThe instructor is not provided with a list of names/emails until that time. If you start this\n\nactivity during preview week, you will not be able to do tasks 1-3 until the actual first day\n\nof the semester. Do not go to awseducate and create an account on your own. It\n\nmay cause problems in the future with accounts. Wait until you receive the email\n\nfrom step 1:\n\n1. You will receive an email from Amazon . The email I received was from AWS\n\nEducate Support at support@awseducate.com. Inside the email, there is a\n\nsentence like this: Click here to complete the AWS Educate application\n\nprocess… Simply click on the “here” link.\n\n6\n\n2. The link took me to step 2 of 3 of the AWS Educate sign up. If you see step 1,\n\nsomething went wrong and you should stop and inform an instructor. You should\n\n5 I received the email list early this semester. If you were registered by January 19, your email was\n\nuploaded to AWS for an account on January 19. Please check all of your emails and spam folders for the\n\nAWS email. If you register for the course on or after January 19, your AWS account will be set up on\n\nJanuary 26.\n\n6 Students at UW-Eau Claire may have the AWS email in quarantine. Since I am at UW-Oshkosh, I do not\n\nhave an Eau Claire account and I cannot reproduce the error. Just be aware that if you don’t see your\n\nAWS email, you may have to check your spam folder or release those emails from quarantine\n\n(https://www.uwec.edu/kb/article/managed-spam/).\n\nDS 730: Activity 1 / Page 8 of 51\n\n\n\n\n\n", "never see step 1 following these directions. Step 2 simply has you filling in your\n\ndetails. Put your home campus for the university . If there is a Promo Code field,\n\nyou can leave it blank. Once everything is filled in, click Next.\n\n7\n\n3. Read through the terms and conditions and assuming you accept them, click on\n\nthe I Agree checkbox. Click Submit. You will likely be directed to a page that\n\nlooks like this:\n\n4. Within seconds, I received an email asking me to verify my email address. I\n\nclicked on the verification link and it took me to a page saying that my email has\n\nbeen verified.\n\n5. A few minutes later, I received an email telling me that my application was\n\napproved. It is possible that your application will remain under review. If this\n\nhappens, your application will likely be approved in a few hours. Inside that email\n\nthere is a link to set up your password and login. Click on the “Click here” link to\n\ncreate a password:\n\n7 I entered in: University of Wisconsin - Oshkosh and had no problems. It wasn’t one of the prefilled\n\noptions but it worked just fine. If you replace Oshkosh with your university’s city, you should have no\n\nissues.\n\nDS 730: Activity 1 / Page 9 of 51\n\n\n\n\n", "6. Once you have created a password, you should see something like this:\n\n7. Click on the My Classrooms link at the top. This redirected me to a page that\n\nlooks like this:\n\n8. Click on the Go to classroom link. A pop-up will ask you to confirm you want to\n\ngo to a new site, read that and click Continue. You will be redirected to a terms\n\nDS 730: Activity 1 / Page 10 of 51\n\n\n\n\n\n", "and conditions page. Read those and assuming you agree, click I Agree at the\n\nbottom.\n\n9. Click on the AWS Console button on the right hand side:\n\n10. The page was a pop-up that my pop-up blocker blocked. Once I allowed the\n\npage to load up, this is what I saw:\n\n11. Once you have reached this point, you are done and everything is set up. This is\n\nthe AWS Console. In order to get back to the AWS Console, you’ll have to follow\n\nthese steps:\n\na. Go to https://aws.amazon.com/education/awseducate/ and click on the\n\nLogin to AWS Educate link.\n\nDS 730: Activity 1 / Page 11 of 51\n\n\n\n\n\n", "b. Sign in with your email address and the password you just created.\n\nc. Repeat steps 7-10 again to get back to your AWS Console.\n\nTask 2: Create a Linux Machine on AWS\n\nConnecting to a machine in the cloud is a good start for the cloud based computing we\n\nwill be using in future activities and projects. We will not connect many times to our\n\nLinux filesystem but it is something you should know how to do. Whenever you spin up\n\na cluster on AWS or some other cloud service, you should be aware of how to connect\n\nto the cluster to make any tweaks you want.\n\n1.\n\nIf you use Windows, go here:\n\nhttp://faculty.cs.uwosh.edu/faculty/krohn/ds730/putty.html. If you have a Mac, skip to\n\nstep 3 as your ssh software is built into your terminal program. If you are using\n\nLinux, odds are you don’t need instructions on how to connect to a remote server.\n\n2. Download these two files:\n\n● putty.exe\n\n● puttygen.exe\n\nNote:\n\nYou can download the Windows installer to install more than you need. However,\n\ndownloading what is specified above is sufficient. You do not need to install any\n\nsoftware to use PuTTy. It is a standalone program that should just run. If you cannot\n\nget the software to run locally, you can download PuTTy and PuTTygen and run it on\n\nthe virtual lab.\n\n3. Sign into your AWS console.\n\n4. Click Services.\n\n5.\n\nIn the Compute section, click EC2:\n\nDS 730: Activity 1 / Page 12 of 51\n\n\n\n\n\n\n\n", "6. Click the Launch Instance button.\n\nNote:\n\nAs a comment for the future, if you are looking to save some money, you should look\n\ninto Spot Requests (Instances). Everything explained in this course is with On\n\nDemand instances. Basically, when you start up an On Demand server, it will run\n\ncontinuously without being interrupted.\n\n7. Select the Ubuntu Server… option. As of now, the best option is the 20.04 version.\n\n8. Choose t2.xlarge. To ensure you don’t waste your credits, make sure to stop your\n\ninstance when you are done using it (details provided later in the task). You can find\n\nall of the pricing details here: https://aws.amazon.com/ec2/pricing/.\n\n9. Click Next: Configure Instance Details.\n\n10. All of the defaults are good here. click Next: Add Storage.\n\n11. Change the GB size from 8GB to 50GB. This can be increased later if you need\n\n12. Click Next: Add Tags.\n\n13. The default values on this page can be kept as they are. Click Next: Configure\n\n14. Make sure Create a new security group is checked. Add the following rules to your\n\nmore space.\n\nSecurity Group.\n\nSecurity Settings:\n\nDS 730: Activity 1 / Page 13 of 51\n\n\n\n\n\n\n\n\n\n\n\n\n", "15. The Type for one of the rules should be SSH and the others are Custom TCP Rule.\n\n16. The Protocol for all rules should be TCP.\n\n17. The Port Ranges should be 22, 2222, 4200, 8080 and 9995.\n\n18. Change the Source(s) from Custom to My IP. The grayed out box next to My IP will\n\ndisplay your IP address. This assumes you will be connecting to the instance from\n\nyour current machine. If you will be connecting from multiple machines, you will need\n\nto add new rules for those machines as well. You should note that if you are working\n\nfrom home, it’s unlikely your internet provider has given you a static IP address that\n\nyou will always have. Your IP address may change from day to day, even hour to\n\nhour. If you are unable to connect to your EC2 instance in the future, revisit this step\n\nand update your security settings so that your current IP can access your EC2\n\ninstance .8\n\n19. Click Review and Launch.\n\n20. Click the Launch button.\n\n21. Click Create a new key pair.\n\n22. Enter a Key pair name of whatever you want. I called mine UbuntuAWS\n\n23. Click Download Key Pair.\n\n24. Save that file somewhere to your hard drive and do not lose it!9\n\nImportant:\n\nIf you lose your key, you will not be able to access the server.\n\n25. Click Launch Instances.\n\n26. Confirm you see something like this:\n\n8 If you do not know your IP address, you can visit https://www.whatismyip.com/ to find it.\n\n9 We also recommend storing the key on a flash drive just in case your computer crashes and you lose\n\naccess to your data.\n\nDS 730: Activity 1 / Page 14 of 51\n\n\n\n\n\n\n\n\n\n", "27. Click the link that appears after The following instance launches have been\n\ninitiated.\n\n● In the previous screen shot, the link appears as i-06c458a38c77556b5. Yours\n\nwill have a different name.\n\n28. Once you click that link, confirm you see something similar to this:\n\nIf you return to this page, it may seem that your instance has disappeared. It is likely\n\nbecause of a filter that is in place. Click on the Clear Filters button to clear any\n\nfilters. Clearing the filters will display all of your EC2 instances.\n\n29. In the Instance State column, notice it says running. If it is still initializing, pending\n\nor waiting, then wait for the instance state to be in running mode before continuing.\n\nClick on the Elastic IPs link on the left hand side. You should see something like\n\nthis:\n\n● Click on the Allocate Elastic IP address button.\n\nDS 730: Activity 1 / Page 15 of 51\n\n\n\n\n\n\n\n\n\n", "● The default option chosen is fine. Click on the Allocate button.\n\n● You will now notice an IP address that is assigned to you:\n\n● This will be your IP address of your virtual machine for the remainder\n\nof the semester.\n\n● Make sure your IP is selected and click on the Actions dropdown box.\n\nClick on Associate Elastic IP address. You should see something like\n\nthis:\n\n● Inside the Instance box, click on it and you should only have one option. It\n\nshould be your running instance. Select that instance.\n\nDS 730: Activity 1 / Page 16 of 51\n\n\n\n\n", "● Click Associate. This takes us back to our Elastic IP main page.\n\n30. Write down your IP address as this will be your IP address for the remainder of the\n\nsemester. From the images, my IP address was 3.216.220.128. Yours will be\n\ndifferent.\n\n31. If you are using a Windows machine, use the following steps to connect to your EC2\n\ninstance (steps 32-48). If you are using a Mac or a Linux machine, simply open up\n\nyour Terminal window and use the following instructions to connect and then skip to\n\nstep 49. In the instructions, be sure to replace ec2-user with ubuntu. Also be sure to\n\nreplace public_dns_name with your IP address. Lastly, replace\n\n/path/my-key-pair.pem to the pem file that you downloaded. In other words, the\n\ncommand should be this:\n\nssh -i /path/my-key-pair.pem ubuntu@public_dns_name\n\nMac/Linux Instructions:\n\nhttps://docs.aws.amazon.com/quickstarts/latest/vmlaunch/step-2-connect-to-instanc\n\ne.html#sshclient\n\n32. The .pem file downloaded in Step 24 is not compatible with PuTTY. Because of this,\n\nwe need to create a key file that PuTTY can read. Open up the puttygen.exe file\n\nthat you downloaded in Step 2.\n\n33. Click the Load button.\n\n34. By default, PuTTYgen only looks for files with extensions of ppk so you must change\n\nit to look for all files. Find your .pem file that you downloaded in Step 24, which will\n\nlook something like this:\n\n35. Click open.\n\n36. PuTTYgen will give you some message about successfully importing the foreign key.\n\nSimply click OK.\n\n37. In order to save the key that PuTTY can use, click the Save private key button.\n\nDS 730: Activity 1 / Page 17 of 51\n\n\n\n\n\n\n\n\n\n\n", "38. The system will ask you if you want to save it without a passphrase. Click yes.\n\n39. Save this ppk file somewhere safe. You can name it whatever you want.\n\nImportant:\n\nIf you lose this file, you will not be able to access your server.\n\n40. Close out of PuTTygen.\n\n41. We are now ready to connect to our server. Open putty.exe.\n\n42. In the Host Name section, enter ubuntu@ followed by the IP address you wrote\n\ndown in step 30.\n\n● As an example, my Host Name was:\n\nubuntu@3.216.220.128\n\n43. Enter in a Port of 22. You should have something similar to this:\n\n44. On the left side, click the plus symbol next to SSH.\n\n45. Click the Auth option.\n\n46. Click the Browse button for the Private key file for authentication\n\nDS 730: Activity 1 / Page 18 of 51\n\n\n\n\n\n\n\n\n\n\n\n", "47. Find your ppk file that you saved in Step 39 and click on Open.\n\n48. Click the Open button.\n\n49. When the system asks you about accepting an RSA fingerprint, indicate Yes.\n\n50. Assuming everything went correctly, confirm you see something like this. As a\n\nreminder, what you are connecting on port 22 is your Linux filesystem.\n\nFor steps 51-60, all commands will be entered inside the PuTTy window.\n\n51. We will install Java as we will need it later in the course. Before we can install it, we\n\nneed to update our package list. In the PuTTy terminal window, type in :10\n\nsudo apt-get update\n\n52. Make sure your software is up to date by entering:\n\nsudo apt-get -y upgrade\n\n53. If necessary, enter ‘Y' to the upgrade.\n\n54. If you get messages about updating some kind of grub file, choose the default option\n\nof keep the local version currently installed.\n\n10 You can also copy and paste the commands. To paste the command into PuTTy, simply right click\n\nanywhere in the PuTTy window.\n\nDS 730: Activity 1 / Page 19 of 51\n\n\n\n\n\n\n\n\n", "Using the Command Line\n\nWe will have to edit files on our Linux filesystems from time to time. If you have a\n\nfavorite editor, feel free to use that. A simple one explained here is called Vim.\n\n55. To edit a file, enter\n\nvim nameOfFile\n\nThis will open up a file called nameOfFile in a program called Vim, which you can\n\nthink of as Notepad. Vim is not installed on Hortonworks by default. To use this in\n\nHortonworks, instead of typing in vim, simply type in vi.\n\n56. Press the letter i to enter Insert mode.\n\nNotice the word -- INSERT -- on the bottom. Once you are in insert mode, you can\n\ntype like you normally would.\n\n57. Add the following text to the file:\n\nHello, this is a test.\n\n58. After you have added that text to your file, Press the ESC key.\n\nYou should notice the -- INSERT -- disappear from the bottom.\n\n59. Enter the following exactly as it’s written:\n\n:wq\n\nThat’s a colon, then w, then q, then the <enter> or <return> key. This will save your\n\nfile and take you back to the command prompt.\n\n60. To ensure the file was created successfully, use the following command to display it\n\non the screen: cat nameOfFile.\n\nNote:\n\nIf you are struggling with Vim, search online for Vim tutorials such as\n\nhttp://www.openvim.com/ and read up on the commands. Once you’ve learned a few\n\nof the commands, it becomes very easy to use. Another common editor is called\n\nnano and instructions for how to use it are in the Linux presentation.\n\nTest Connection and Stop Instance\n\nImportant:\n\nYou now need to go back to the AWS console page and stop your instance. You don't\n\nwant to pay for idle time if you are not connected to your machine.\n\n61. In order to stop your instance, go back to your browser and click your instance.\n\n62. Go up to the Instance State button.\n\n63. Click the Stop instance option.\n\nDS 730: Activity 1 / Page 20 of 51\n\n\n\n\n\n\n\n\n\n\n\n", "64. You might see a box that says something about losing ephemeral storage. This is\n\nfine; click Yes. Wait on this page until the instance is stopped. You may need to\n\nrefresh the page to see the instance stopped.\n\n65. This is not a step that you should complete right now but it is being provided for\n\nfuture reference. If you want to change your instance type, this is the webpage\n\nwhere you do it (when the instance is stopped). If you so desire, you can use a\n\nt2.2xlarge to better see the power of multithreading near the end of the semester. It's\n\na CPU issue; smaller instances do not provide multiple CPUs. The t2.2xlarge\n\ninstance provides 8 CPUs and 32GB of memory . The t2.xlarge instance costs\n\nabout 18 cents an hour. The t2.2xlarge instance, by contrast, is about 37 cents an\n\nhour. In order to change your instance type, ensure that your instance is stopped.\n\nRight click on the instance you want to change. Choose Instance Settings and\n\nchoose Change Instance Type. Choose the instance type you want and hit Apply.\n\nBe very careful to choose an instance that is available to us with our starter\n\naccounts. You can view available options here. If you choose a type and that type is\n\nnot allowed because of our starter accounts, your instance will terminate and you will\n\nlose your virtual machine.\n\n11\n\n66. If you are unable to connect to your EC2 instance in the future, it could be because\n\nyour IP address changed . In order to update your security settings, go to the\n\nInstances page and click on the instance id:\n\n12\n\nYou will find a Security tab near the bottom of the screen. Click on Security. Click\n\non the Security Group that is listed. It should be something like sg-12345… After\n\nthat, click on Edit Inbound Rules. You should add 5 new rules for each of the ports,\n\n22, 2222, 4200, 8080 and 9995. For each of the dropdown boxes for these new\n\nrules that says Custom, change Custom to be My IP. Click the Save Rules button\n\nat the bottom when you are done.\n\n67. At the end of the semester, your AWS account will be terminated. Anything that\n\nyou’ve created and used on AWS will be gone and you cannot retrieve it. Therefore,\n\nyou are encouraged to save any code/work to your local machine before the\n\nsemester ends if you want to keep it.\n\n11 The current t2.xlarge provides 4 CPUs and 16GB of RAM. It is more than sufficient for this course.\n\n12 If you do not know your IP address, you can visit https://www.whatismyip.com/ to find it.\n\nDS 730: Activity 1 / Page 21 of 51\n\n\n\n\n\n\n", "Important:\n\nDuring the semester, I recommended stopping as you will be able to restart your\n\ninstance as it were without having to reconfigure anything. If you terminate it each\n\ntime throughout the semester, you will lose everything and have to redo everything.\n\nIn order to restart your instance:\n\n1. Come back to this page.\n\n2. Click your instance.\n\n3. Click Instance state and then Start instance.\n\nTask 3: Create a Hortonworks Sandbox in the Cloud with AWS\n\nThis task creates a sandbox in the cloud with most of the required software installed on\n\nit. The main advantage of this is that you do not have to use 1 single machine for this\n\ncourse and can connect to your sandbox from anywhere. If there is a problem, your\n\ninstructor can connect to your machine and troubleshoot any issues. As a side note, the\n\nvirtual machine you are creating has nothing to do with the virtual lab that is provided by\n\nthe program. Do not mistake what you are doing here with the virtual lab that you may\n\nhave used in previous courses. You should not use the virtual lab at all in this course .13\n\n1. Go back to your EC2 instances page and you should have a stopped t2.xlarge\n\ninstance (the instance you created in the last task). Click on that instance, go up\n\nto Instance State and click on Start instance. Your instance will start up. Your IP\n\naddress should be the same as it was in Task 2. As a reminder, my Elastic IP\n\nwas 3.216.220.128. It should be the same. Connect to your EC2 instance using\n\nPuTTy and shown in the previous task. As a reminder, when you are connecting\n\nwith PuTTy to your IP address using port 22, you are connecting to your Linux\n\nfilesystem.\n\n2. Once you are connected with PuTTy, enter the following commands. Note that\n\nstep (c) will take some time as you are downloading and extracting many\n\ngigabytes of data:\n\na. wget http://faculty.cs.uwosh.edu/faculty/krohn/ds730/installAll.sh\n\nb. chmod +x installAll.sh\n\nc. sudo ./installAll.sh\n\n13 The only exception to this is if you are unable to connect to AWS using your local computer. If you are\n\nusing a work machine that is heavily locked down, it’s possible some of this software won’t work.\n\nTherefore, a backup option is to connect to the Virtual Lab first and then use the Virtual Lab to connect to\n\nAWS.\n\nDS 730: Activity 1 / Page 22 of 51\n\n\n\n\n\n\n\n\n", "The wget command is a nice command that you can use to download files from\n\nthe Internet directly to your filesystem. Keep this command in mind if you ever\n\nneed to download something from some website into one of your Linux\n\nfilesystems.\n\n3. The previous script will take roughly 6 minutes to run. At the end of running the\n\ninstall script, it should say: Hortonworks is running. If you see Hortonworks is\n\nnot running, then something went horribly wrong and you need to redo tasks 2\n\nand 3 . In the future, you can come back and run this script to see if\n\nHortonworks is running:\n\n14\n\n./checkHortonworks.sh\n\nYou shouldn’t have to do this again as the Hortonworks ecosystem will restart\n\nitself every time you restart the virtual machine.\n\nAs a side note, it is quite common to start working on your work, walk away and\n\nforget to shutdown your virtual machine. As of this writing, for every hour you\n\nleave your machine on, it will cost you roughly 18 cents in credits. If you discover\n\nthis the next day, it’s generally not a problem. But if you walk away on a Sunday\n\nnight and don’t come back until Friday, you can easily burn through $15-$20 in\n\ncredits. With that said, there is a script that automatically shuts your virtual\n\nmachine down after 5 hours.\n\nIf you wish to change the 5 hour default to a different amount, log into port 22 of\n\nyour virtual machine and edit the following file using vim or your favorite editor:\n\nend.sh. You will notice a line that says: sleep 298m which means, wait for 298\n\nminutes before doing anything. Once 298 minutes is over, the machine will start\n\nits shutdown sequence, which as written, will shutdown in an additional 2\n\nminutes. You can edit the file using vim and change 298m to be whatever you\n\nwant it to be, ~2 hours (120m), ~8 hours (480m), etc.\n\nYou should not rely on the automatic shutdown to stop your virtual machine\n\nthough. If you are done working after a couple of hours, you should manually go\n\nback into AWS and stop your instance. There is no need to waste credits if you\n\nknow you are done using your machine.\n\n14 A script failure is very unlikely to happen. We have tested this script dozens of times and it has\n\nsucceeded on each instance. Network issues, virtual machine issues are extremely rare (especially on\n\nAWS) but not impossible. Therefore, if one of these extremely rare cases occurs, it’s best to just start over\n\nwith task 2 and create a new virtual machine.\n\nDS 730: Activity 1 / Page 23 of 51\n\n\n\n", "4. Use your browser to navigate to your IP using port 8080. You will go to\n\nhttp://IP:8080 (be sure to replace IP with your IP address) and enter in\n\nmaria_dev as the username and password . Note that it is http and not https.\n\nHortonworks does not have https installed and therefore, we can’t use it. You\n\nshould see something like this:\n\n15\n\n5. This step is important for all future activities and projects whenever you\n\nstart up your virtual machine. You must check this page before beginning any\n\nactivity/project to ensure the system is ready to use. As long as there are 0 red\n\nalerts at the top, you are ready to start working. If you see any red alerts, you\n\nmust wait for the alerts to clear before starting. Even though you may be able\n\nto load up the 8080 port and the 4200 port, it does not mean the machine is\n\ncompletely started and ready to use. You must wait for all red alerts to clear\n\nbefore beginning. Note that an orange alert is fine as this is just a warning. If\n\n15 If you go to the webpage too quickly, it may give you a 502 error or say the page cannot be found. If this\n\nhappens, simply reload the page in a minute. It will load up eventually. When you enter in the username\n\nand password, there may be a dialog box that pops up and says your password is exposed and you\n\nshould change your password. Since your IP address is the only one that can access this webpage, you\n\nare safe leaving the username and password at the default.\n\nDS 730: Activity 1 / Page 24 of 51\n\n\n\n\n\n", "you only have orange alerts, you can continue. You will likely have 1 orange alert\n\nthat talks about disk usage. We are only allocating 50GB of space and we will\n\nend up using 60%+ of it. If you have red alerts, you must wait. The virtual\n\nmachine does take a few minutes to load up all of the software. Feel free to\n\nexplore the dashboard to see all of the tools available for you to use. Once you\n\nare done exploring, you can close out of this window. As stated above, be sure to\n\nwait until all red alerts are cleared before moving to the next step.\n\n6. Do not use PuTTy for this step! Use your browser and navigate to your IP\n\nusing port 4200. You will go to http://IP:4200. This step is connecting to your\n\nHortonworks filesystem. Log in using maria_dev as the username and password\n\nand enter the following commands:\n\na. wget http://faculty.cs.uwosh.edu/faculty/krohn/ds730/installR.sh\n\nb. chmod +x installR.sh\n\nc. sudo ./installR.sh\n\n7. Wait for R to install. It will take several minutes. To ensure it installed correctly,\n\nenter in R. You should see something like this:\n\n8. Enter in the following R commands just to make sure everything is working:\n\na. my_list = c(1:10)\n\nb. mean(my_list)\n\nIt should display 5.5. If it does, enter in q() to quit. Enter in n since we don’t want\n\nto save this workspace.\n\nDS 730: Activity 1 / Page 25 of 51\n\n\n\n\n\n", "9. Once R is installed and tested, type in exit to leave the Hortonworks filesystem.\n\nYou can close out of this browser.\n\nConnect to the Hortonworks Filesystem with Cyberduck\n\nIn order to connect to your Hortonworks filesystem using Cyberduck to transfer files\n\nback and forth from your local machine to your Hortonworks filesystem, follow these\n\nsteps :16\n\n1. Download Cyberduck from\n\nhttp://faculty.cs.uwosh.edu/faculty/krohn/ds730/cyberduck.html and install it. The\n\ndownload and install should be self-explanatory.\n\n2. Open Cyberduck.\n\n3. Go up to File > Open Connection.\n\n4. Change the dropdown box to be SFTP (SSH File Transfer Protocol)\n\n5. Enter in your IP address in the Server.\n\n6. For the Port, leave it at 2222\n\n7. Change the Username to maria_dev.\n\n8. Enter in maria_dev as the Password.\n\n9. Confirm that your screen looks something like this:\n\n10. Click Connect.\n\n16 If you are using a machine that will not let you install software (e.g. a locked down work machine), you\n\ncan connect to the Virtual Lab and use CyberDuck on the Virtual Lab. CyberDuck will allow you to transfer\n\nfiles from your cloud machine to the Virtual Lab. You’ll then be able to transfer those files from the Virtual\n\nLab to your local machine (or just upload them to the dropbox from the Virtual Lab).\n\nDS 730: Activity 1 / Page 26 of 51\n\n\n\n\n\n", "11. If the system asks you to trust the key, simply indicate OK. You can choose to\n\nalways accept the key. You should be connected and see something like this:\n\n12. In order to upload a file, simply click on Upload. Find the file you want to upload\n\n13. If you want to download a file from the server, double click on the file name and it\n\nand click Choose.\n\nwill go to your Downloads folder.\n\n14. If you are continuing with the next tasks, you can skip steps 14 and 15. Be sure\n\nto stop your virtual machine whenever you are done using it. If you are not\n\ncontinuing to the next task right now, go to your instances page on AWS and click\n\non the instance you just created:\n\n15. Click on Instance State → Stop Instance. We do not want to terminate this\n\ninstance because we will be reusing this software throughout the semester.\n\nTask 4: Testing Java\n\n1. Connect to your Hortonworks filesystem using port 4200 in the browser. You can\n\nalso connect to your Hortonworks filesystem by using port 2222 and PuTTy. If\n\nconnecting with PuTTy, you will use maria_dev@yourIP as the hostname and 2222\n\nas the port. You do not need to provide a key. When prompted, enter in maria_dev\n\nas the password.\n\n2. Create a folder called JavaExamples. To do this, type in mkdir JavaExamples. To\n\nenter that directory, type in cd JavaExamples.\n\nDS 730: Activity 1 / Page 27 of 51\n\n\n\n\n", "3. Create a file called Test.java in that folder (see VIM from earlier).\n\n4. Enter the following code into that file:\n\npublic class Test{\n\npublic static void main(String args[]){\n\nSystem.out.println(\"It works!\");\n\n}\n\n}\n\n5. When you are back at the terminal window, compile your program by entering\n\n6.\n\njavac *.java\n\nIn order to run the file, enter java Test\n\n● You should see It works! printed to the screen.\n\nTask 5: Test Python\n\nIt is assumed that you have some general problem solving skills with programming\n\nbefore starting this course. You should be familiar with the following topics before\n\nstarting this course:\n\nSelection Statements (e.g. if)\n\nRepetition Statements (e.g. while, for)\n\nVariables\n\nLists/Arrays\n\nCalling Functions/Methods\n\nCreating Functions/Methods\n\nIf you do not know what one of those topics is or do not have a good grasp on how to\n\nuse each one, you might want to reconsider whether you are prepared for this course.\n\nWe will write a few short Python programs to test out our Python installation and also to\n\nassess your programming ability. You must test your code on your Hortonworks\n\nfilesystem using the command line. If you find yourself taking many hours to solve these\n\ntasks, then it might be best to gain some more experience solving problems with code\n\nbefore taking this course. A good website to work on your programming is\n\nhttps://projecteuler.net/archives. The first 20 or so problems are, relative to most\n\nproblems in this course, easy to solve. Many of them require the skills above to solve\n\nand most of them are very short programs. For example, a solution for problem 1 is\n\nlocated at the end of this document.\n\nDS 730: Activity 1 / Page 28 of 51\n\n\n\n\n\n\n\n", "Important:\n\n● You are not allowed to use any external libraries (e.g. numpy, sympy, itertools\n\netc.) for these problems as they make the problems trivial. If you are importing\n\nsomething other than sys, then you are not doing this correctly.\n\n● Make sure to follow the directions exactly as my tester code will not work if you\n\ndon't. For example, in the first problem, if you do not call your file first.py or\n\ncreate a function called fact instead of factorial, the automated tests will fail.\n\nAlso, be sure to output exactly what the problem asks for and nothing else. For\n\nexample, for the factorial problem, only output the actual factorial number. If the\n\nanswer is 720, only output 720. Do not output something along the lines of: “The\n\nfactorial is 720.” The final Python problem addresses the importance of creating\n\nappropriate output.\n\nRun Python program from terminal\n\nMany of you may be unfamiliar with the terminal (command line) and we will be using it\n\nfor a portion of this course. The following short instructions describe how to create a\n\nPython file using only the terminal and how to run your Python code from the terminal.\n\n1. Connect to your Hortonworks filesystem using the browser and port 4200 or PuTTy\n\n2. Create your Python programs using vim as described before .17\n\n3. Once you have created your pythonFile.py program, go back to the terminal so that\n\nwith port 2222.\n\nyou can test your code.\n\n4. Many of you have probably written Python code using some editor like Jupyter.\n\nWhile this is a good start, it is helpful to know what is actually going on behind the\n\nscenes. In other words, when you hit the “Run” button, what is actually happening?\n\nKnowing this is going to be important when we start writing MapReduce code. In\n\norder for your Python code to run on the command line, you need to be sure that the\n\nfirst line of your Python file looks like this:\n\n#!/usr/bin/python3.6\n\nThe above line tells the operating system that you want to run the following code\n\nusing the python3.6 interpreter.\n\n17 You are welcome to create your Python files locally. However, you must test them on the Hortonworks\n\nfilesystem to ensure they work in that environment since all programs will be tested in that environment.\n\nDS 730: Activity 1 / Page 29 of 51\n\n\n\n\n\n\n\n", "5.\n\nIn order to run your program, enter in the following 2 commands:\n\nchmod +x pythonFile.py\n\n./pythonFile.py\n\nThe first command tells the operating system that this file is one that we want to\n\nexecute. The second command executes the program.\n\nReading input from redirected file input\n\nIn this course, it will be important that we know how to read in from the terminal window.\n\nInstead of opening up specific files, it is often handy to simply supply the filenames we\n\nwant to process at runtime. In order to do this, the following command is used to “send”\n\ninformation from a file to a program:\n\n./pythonFile.py < someInputFile\n\nThe < operator is called the redirect operator. It will take the text in someInputFile and\n\nsend it to the first.py program. The following is a very simple Python program on how to\n\nread in that text and simply print it back out:\n\n#!/usr/bin/python3.6\n\nimport sys\n\nline = sys.stdin.readline()\n\nwhile line:\n\nprint(line.strip())\n\nline = sys.stdin.readline()\n\nAs you might have guessed, the above program reads in 1 line at a time and simply\n\nprints it out. It isn’t doing anything that interesting. The sys.stdin.readline() method\n\nappends a newline character to the end of the string. Therefore, when printing, the code\n\nstrips off the newline character. You can use all of your standard string manipulations on\n\nthe line variable, e.g. split, strip, find, etc. Note that the first line of the file is telling the\n\noperating system that this is a Python3 file and should be run as such. It is important\n\nthat the first line of the code is exactly what is written above for the Python files in this\n\nactivity.\n\nCreate a Factorial Calculation Program\n\nYou will be creating a program that calculates the factorial of a number.\n\nDS 730: Activity 1 / Page 30 of 51\n\n\n", "1. Create a file called first.py.\n\n2.\n\nInside that Python file, create a function called factorial that accepts an integer as\n\nan argument and returns the factorial of that integer. Do not print out anything in the\n\nfactorial function. If the factorial function is called with a negative integer, a -1 is\n\nreturned from the function. Do not prompt the user to enter a value. This will be\n\ntaken care of by the tester program. Do not put anything else in this file except the\n\nfactorial function (i.e. don’t put tester code in this file).\n\n3. Make sure your function definition looks like this:\n\ndef factorial(val):\n\nOutput Average of Integers\n\nCreate a program that reads in integers and outputs (i.e. print) the average of all of the\n\nnumbers. Your program will be called using the following command:\n\n./second.py < someInputFile\n\nSome things to note:\n\n● Only integers will appear in the input file.\n\n● All of the integers in the input file will be separated by a space.\n\n● All integers will be on a single line.\n\n● Create a separate file called second.py to solve this problem.\n\n● Do not read in from a specific file. If you are using the open function, you are\n\ndoing this problem wrong.\n\nOutput Prime Numbers\n\nThe goal of this task is to create a Python program that reads in 2 integers that are\n\nseparated by a space. There will only be 2 integers in the input file. Your Python code\n\nmust be stored in a file called third.py. Your program then prints out all of the prime\n\nnumbers strictly between those 2 numbers in increasing order. If there are no prime\n\nnumbers strictly between those 2 numbers, then a No Primes message is printed out.\n\nThe order that the numbers were input does not matter. Similar to the previous problem,\n\nthe program will be called using the following command:\n\n./third.py < someInputFile\n\nOutput: If there are no primes between the two numbers, your code outputs exactly:\n\nNo Primes\n\nDS 730: Activity 1 / Page 31 of 51\n\n\n", "If there is one prime number, only that 1 number prints out.\n\nIf there are multiple prime numbers, the following pattern is used:\n\nfirstNum:secondNum!thirdNum&fourthNum:fifthNum!sixthNum&seventhNum\n\nIn other words, you separate the first number from the second number with a colon. You\n\nseparate the second number from the third number with an exclamation point. You\n\nseparate the third number from the fourth number with an ampersand. This delimiter\n\npattern is repeated in subsequent numbers (colon, exclamation point, ampersand) until\n\nthere are no more numbers left to be printed. There is no delimiter before the first\n\nelement nor is there a delimiter after the last element. The numbers in the output should\n\nbe in numerical order.\n\nExample:\n\nFor example, if 5 and 24 were entered in that order, then 7:11!13&17:19!23 would\n\nprint out.\n\nIf 24 and 5 were entered in that order, then 7:11!13&17:19!23 would print out.\n\nInterpret Dirty Data\n\nThroughout this course, your output will be tested using automated testers. Being able\n\nto test your code quickly allows us more time to dig into your code and provide better\n\nfeedback. If we have to spend a lot of time trying to figure out what your output is or how\n\nto run your program, then this cuts into the time we have to look at your code.\n\nTherefore, it is important that your code is contained in the files we specify and\n\nproduces the exact output we expect. Having incorrect delimiters, incorrect spacing, etc\n\nwill cause our automated tests to fail. Be sure you read the output specifications closely\n\nso that you are producing the correct output. The output of the previous problem is quite\n\narbitrary (although it assesses good problem solving skills) but this is done to ensure\n\nyou are prepared for future output requirements. Future output requirements will be\n\nmuch more natural and obvious.\n\nTo show you why it is helpful to have all output be the same, consider the following\n\nproblem. You are a manager for a global weather company and you have asked your\n\nregional offices to send you information in the following comma separated format:\n\nYear,Month,Day,TimeCST,TemperatureF,WindMPH\n\nAll offices put a header row in each file to explain the data. A key thing is that the\n\ntemperature should have been in Fahrenheit and located in the second to last column.\n\nAnother key thing is that the wind should have been located in the last column and\n\nDS 730: Activity 1 / Page 32 of 51\n\n\n\n\n\n\n", "should have been in miles per hour. None of the offices sent the data exactly how you\n\nasked for it. Your job is to figure out what each of them did and then answer the\n\nfollowing 2 questions:\n\n1. Take the average of all of the wind speeds for March, 2006. Which city had the\n\n2. Take the average of all of the temperatures for 2006. Which city had the closest\n\nclosest wind speed to 8.30 mph?\n\ntemperature, in fahrenheit, to 49.65?\n\nYou can download the 4 files from\n\nhttp://faculty.cs.uwosh.edu/faculty/krohn/ds730/a1Weather.zip\n\nThe 4 cities are ABC, KLM, PQR and XYZ (see filenames). The way you clean the data\n\nand obtain the answers is entirely up to you with a couple of caveats. Any wind speed\n\nthat is negative should be filtered out as this is not possible. Any temperature below\n\n-200 should be filtered out. Any wind speed that is listed as calm should be interpreted\n\nas 0mph or 0kph depending on the column header. Calm winds should not be filtered\n\nout. When filtering out data, only filter out the cell that is invalid, not the entire row. For\n\nexample, if a row has a temperature of -9999 and a wind of 7.8, filter out the\n\ntemperature but keep the wind value.\n\nFor this dirty data task, you only need to create a text file called answers.txt that\n\ncontains the answer to each of the above questions. Any reasonable formatting of\n\nanswers.txt is acceptable. You do not need to upload any code or explain how you\n\nobtained your answers for this final question.\n\nTask 6: Submitting your Work\n\nYou must test and ensure your Python code works on the Hortonworks filesystem. Once\n\nyou are done testing, we want to transfer the files to your local machine and zip them up\n\nfor submitting. This can be done with CyberDuck. In order to transfer files:\n\n1. Open Cyberduck.\n\n2. Go up to File > Open Connection.\n\n3. Change the dropdown box to be SFTP (SSH File Transfer Protocol)\n\n4. Enter in your IP address in the Server.\n\n5. Since we created our code on our Hortonworks filesystem, as a reminder, there\n\nis another way to access the Hortonworks filesystem. We can do it using\n\nPuTTy/Cyberduck using port 2222. For the Port, enter 2222\n\n6. Change the Username to maria_dev.\n\n7. Enter in maria_dev as the Password.\n\nDS 730: Activity 1 / Page 33 of 51\n\n\n\n", "8. Click the Connect button.\n\n9.\n\nIn order to upload a file, simply click on Upload. Find the file you want to upload\n\nand click Choose.\n\n10. If you want to download a file from the server, double click on the file name and it\n\nwill go to your Downloads folder.\n\n11. Download your Python files from your Hortonworks filesystem.\n\nYou should have a total of 3 Python files: first.py, second.py, and third.py. You should\n\nalso have an answers.txt file. Zip up these 4 files into 1 zip file called a1.zip and submit\n\na1.zip to the dropbox.\n\nOptional Tasks\n\nThe following are optional tasks that you can do if you wish to set up a Hortonworks\n\nmachine using Microsoft Azure or locally on your own machine. Since our goal is to\n\nhave a uniform setup that everyone uses, we will not be providing any support for the\n\nfollowing tasks. These steps are simply optional if you want a different cloud experience\n\nor if you want to set up your own Hortonworks machine locally.\n\nOptional Task 1: Create a Hortonworks Sandbox on Azure\n\nThis task creates a sandbox in the cloud with most of the required software installed on\n\nit. The main advantage of this is that you do not have to use 1 single machine for this\n\ncourse and can connect to your sandbox from anywhere. If there is a problem, your\n\ninstructor can connect to your machine and troubleshoot any issues. As a side note, the\n\nvirtual machine you are creating has nothing to do with the virtual lab that is provided by\n\nthe program. Do not mistake what you are doing here with the virtual lab that you may\n\nhave used in previous courses. You should not use the virtual lab at all in this course.\n\n1. Go to https://azure.microsoft.com/en-us/free/students/ and click on the Activate\n\nnow button. If you already have an account with your school email address, then\n\nsign in. Otherwise, click on Create one! and sign up for an account. Since\n\nMicrosoft’s verification is not universally the same, your prompts and pages may\n\nbe different from mine. Essentially, do what you need to do to create an account.\n\nThis account is how your instructor created an account:\n\nI was not able to create a new account with my .edu address so I simply used an\n\nold gmail account. I was eventually able to get signed in. When signed in, it\n\nasked me to verify my student status with a school email address. I typed in my\n\n.edu email account and clicked on Verify academic status. After a few seconds,\n\nDS 730: Activity 1 / Page 34 of 51\n\n\n\n", "an email showed up in my inbox and I clicked on the verification link. It asked me\n\nto verify again by phone. I was not able to verify a phone number from google\n\nvoice so my guess is other VOIP numbers will also fail. I was able to get a text to\n\nmy regular cell number so that worked fine. A call to your office number would\n\nprobably work as well. In either case, my identity was verified and I accepted the\n\nagreement and hit the Sign up button. You may see a popup about starting a\n\ntour. Take the tour if you want or simply hit Maybe later. Once I was all signed\n\nup, I ended up on this page. Your page may look different from mine. As long as\n\nstep 2 is possible for you, you are in the right place.\n\n2. On the left hand side, click on Create a resource.\n\n3. Do a search for Hortonworks and click on the option for Hortonworks Data\n\nPlatform (HDP) Sandbox. You should see something that looks like this:\n\nDS 730: Activity 1 / Page 35 of 51\n\n\n\n", "4. Click on the Create button. From here we just need to customize our sandbox.\n\n5. On the Basics tab, click on Create new for the resource group. You can call your\n\nresource anything you want, e.g. DS730. The Virtual machine name can be\n\ncalled Hortonworks. Under Instance Details, look at the Size option. We are only\n\nallowed 4 cores with our student account. Therefore, click on the Change size\n\nlink and choose the Standard B4ms option. Change the Administrator\n\nAccount Authentication type to be a password. It is less secure than an SSH\n\npublic key but it is sufficient for this course. Create any username/password you\n\nwant. You will never need to use this username/password in this course. Feel\n\nfree to click on the Next buttons if you would like to configure your virtual\n\nDS 730: Activity 1 / Page 36 of 51\n\n\n\n", "machine more. However, the default options are sufficient for this course. Click\n\non the Review + create button.\n\n6. Review the options you chose and click on the Create button at the bottom.\n\nDeployment will take a while so simply wait for it to finish. You should see\n\nsomething like this (click on the bell icon to get the notifications on the right hand\n\nside):\n\n7. After about 5 minutes, the deployment should succeed. Yours will likely take\n\nlonger as you will be doing it at the beginning of the semester when everyone\n\nelse is trying to do it. Once the deployment has succeeded, click on the Go to\n\nresource button in the middle of the screen. You will see something like this:\n\n8. Under the Settings option, click on Networking. By default, essentially all ports\n\nare closed for security. However, we need to connect to them so we need to\n\nopen up some of the ports. Click on the Add inbound port rule button. Add the\n\nfollowing port using these rule:\n\na. Destination port ranges - Set it to be 8080. It might default at that port\n\nalready. If that is the case, then leave it.\n\nb. Name - Not critical but change it to Ambari so you know what the port is.\n\nc. The rest of the options can be left at their defaults. Feel free to change the\n\nSource if you are super concerned about security.\n\nIt will take a minute to create the rule. Add the following Port/Name inbound port\n\nrules as well: (4200/ssh, 9995/zeppelin, 2222/hortonworksfs). You may have to\n\nDS 730: Activity 1 / Page 37 of 51\n\n\n\n\n", "change the priority to be a different number for each rule. Change it to any\n\nnumber that is unique.\n\n9. Your Hortonworks Sandbox is setup and is now ready to use. A great guide to\n\nlearning about your Sandbox is here:\n\nhttps://hortonworks.com/tutorial/learning-the-ropes-of-the-hortonworks-sandbox\n\nHowever, everything you need to know for this course will be shown in this and\n\nfuture activities.\n\n10. Click on the All resources option on the left. Click on the Hortonworks link\n\nwhere the type is Virtual machine:\n\n11. You will find your Public IP address on the right hand side. Mine looks like this:\n\n12. Note that my IP address is 40.117.228.13 and I will use that IP in the steps\n\nbelow. Replace my IP with yours when you do the following steps.\n\n13. Go to http://40.117.228.13:8080 and enter in maria_dev as the username and\n\npassword. You should see something like this:\n\nDS 730: Activity 1 / Page 38 of 51\n\n\n\n\n\n\n", "14. This step is important for all future activities and projects whenever you\n\nstart up your virtual machine. You must check this page before beginning any\n\nactivity/project to ensure the system is ready to use. As long as there are 0 alerts\n\nat the top, you are ready to start working. If you see any alerts, you must wait\n\nfor the alerts to clear before starting. Even though you may be able to load up\n\nthe 8080 port and the 4200 port, it does not mean the machine is completely\n\nstarted and ready to use. You must wait for all alerts to clear before\n\nbeginning. Note that an orange alert is fine as this is just a warning. If you only\n\nhave orange alerts, you can continue. If you have red alerts, you must wait. The\n\nvirtual machine does take a few minutes to load up all of the software. Feel free\n\nto explore the dashboard to see all of the tools available for you to use. Once you\n\nare done exploring, you can close out of this window.\n\n15. The next 2 steps are very important to do each time you are done using your\n\nvirtual machine. If you forget to do these steps, your virtual machine will still be\n\nrunning and you will run out of credits before the semester ends. Go back to your\n\nAzure browser window and click on All resources. Click on the Hortonworks\n\nVirtual Machine and you should see something like this:\n\nDS 730: Activity 1 / Page 39 of 51\n\n\n\n", "16. Click on the Stop button in the top-middle of the page. Your virtual machine will\n\nshut down and you will no longer be charged for the machine. You are only using\n\nyour credits when the virtual machine is running. Wait a few minutes and ensure\n\nthat the virtual machine shuts down. You can click on the bell icon in the upper\n\nright hand corner to see what is happening. You should see something like this\n\nshowing that the virtual machine is being stopped:\n\n17. Troubleshooting Hortonworks. If you find your machine in a bad state and your\n\nprograms are taking a long time to run or simply aren’t running at all, these are\n\nthe best tips to getting back into a good state. You do not have to do these at the\n\nbeginning but just remember these tips are here for future reference.\n\na.\n\nIf you find there are several alerts or something didn’t load up correctly,\n\nyou are encouraged to debug this by simply restarting the services on the\n\nvirtual machine. If you find that Hadoop, Hive, Pig, etc. are not functioning\n\nproperly, restarting all of the services will often help. To do this, click on the\n\nDS 730: Activity 1 / Page 40 of 51\n\n\n\n\n", "Actions button in the lower left hand corner. Click on Stop All. Click on\n\nthe Confirm Stop button that pops up. It will likely take a couple of\n\nminutes for all services to stop:\n\nb.\n\nc.\n\nOnce all services have stopped, click the OK button. To start up the\n\nservices, click on the Actions button, click on Start All then click on\n\nConfirm Start. Your services are now restarting.\n\nIf the previous step does not solve your issue, then stopping the entire\n\nmachine and restarting it is the next thing to try. Stop your virtual machine\n\nby using steps 17 and 18 above. In order to start your virtual machine up\n\nagain, click on the Start button in the top-middle of the screen. After a few\n\nminutes, your machine will start up again.\n\nIf restarting your machine does not solve the problem, then the fastest\n\nsolution is to delete your virtual machine and create a new one. If some\n\nstep was skipped or your virtual machine got itself into a bad state, then\n\ntroubleshooting the issue could take hours. However, deleting your current\n\nvirtual machine and creating a new one takes minutes. Just be aware that\n\nif you delete your virtual machine, any files on that virtual machine will be\n\ndeleted as well. Be sure to download/copy over anything you want before\n\ndeleting.\n\nTo delete your virtual machine, click on All resources on the left hand\n\nside. Select all of the resources listed and click on the Delete button at the\n\ntop of the screen. Type yes to confirm deletion and click the Delete\n\nbutton. Go back to step 2 and create a new virtual machine .18\n\n18 Since we are using a student account, we are limited in the number of resources we can use.\n\nTherefore, you need to wait until your resources have been deleted before you can create a new virtual\n\nmachine. Even if you wait, it is possible the system is still updating itself and may not let you create a new\n\nmachine right away. If you encounter errors creating a new virtual machine (it may say validation failed\n\nwhen you click on review + create), simply wait a few minutes and try again. Eventually, you will be able\n\nto create a new virtual machine.\n\nDS 730: Activity 1 / Page 41 of 51\n\n\n\n\n", "Optional Task 2: Create a Local Hortonworks Sandbox\n\nThe newest version of Hortonworks is different from the one on Azure and there are\n\nconsiderable differences in the UI. There are also considerable differences in how you\n\nrun each of the programs. All instructions for the remainder of the course will assume\n\nthat you are running Hortonworks 2.6.5. Therefore, you are encouraged to download the\n\nolder version of Hortonworks to be consistent with the one on Azure.\n\nThis task creates a Hortonworks virtual machine and installs all of the software we are\n\nusing in this course. This task takes a couple of hours depending on your Internet\n\nconnection speed. However, the majority of it is waiting for everything to download. You\n\nwill need about 100GB of hard drive space available and a machine that has at least\n\n8GB of memory, preferably more . These instructions have worked on a machine\n\nrunning a 64-bit version of Windows 10 and a 64-bit version of Windows 7 .20\n\n19\n\n1. Download VirtualBox. The current version is 6.0.8. You can find the\n\n2.\n\ndownload here:\n\nhttp://faculty.cs.uwosh.edu/faculty/krohn/ds730/virtualbox.html\n\nInstall VirtualBox on your machine. It is a simple install that requires you to\n\nclick Next and Install a few times.\n\n● If it asks about installing network devices, this is no problem; you can\n\nsimply click Install on those prompts.\n\n3. When the install is complete, click Finish.\n\n4. Open VirtualBox up once it is installed. You should see something like this:\n\n19 You should have a minimum of 8GB of free RAM in order for Hortonworks to work. The more memory\n\nthe better.\n\n20 You need to ensure your machine has VT-x (virtualization) enabled. This often needs to be done in the\n\nBIOS. If VirtualBox is failing to work, ensure virtualization is enabled. See\n\nhttps://www.howtogeek.com/213795/how-to-enable-intel-vt-x-in-your-computers-bios-or-uefi-firmware/ for\n\nmore details. If your machine is not working and you are not comfortable modifying BIOS settings or you\n\njust don’t want to do it, there are several other options to run Hortonworks in the cloud.\n\nDS 730: Activity 1 / Page 42 of 51\n\n\n\n\n\n", "5. Download Hortonworks Sandbox. The version of Hortonworks Data\n\nPlatform (HDP) that is similar to the one on Azure is 2.6.5. You can download\n\nit from here: http://faculty.cs.uwosh.edu/faculty/krohn/ds730/hortonworks.html\n\n● Click on the dropdown box where it says Choose Installation Type and\n\nselect VirtualBox:\n\n● It will likely ask you to fill out your name, email address, etc before\n\ndownloading. Fill out the form, read and accept the terms and conditions\n\nand click Submit. On this part of the page, be sure to click on 2.6.5:\n\nDS 730: Activity 1 / Page 43 of 51\n\n\n\n\n\n", "● Note that this is a 10GB download so it may take a while if you have a\n\n6. Once the file has downloaded, go to VirtualBox and click on File → Import\n\nslower home connection.\n\nAppliance.\n\n7. Click on the folder icon and find the *.ova file you just downloaded and click\n\nOpen. For reference, mine was called HDP_2.6.5_virtualbox_180626.ova.\n\nOnce you have selected it, click Next.\n\n8. All of the settings can remain unchanged. If you wish to store your sandbox\n\non an external drive, scroll down and click on the Virtual Disk Image location\n\nsetting at the bottom and change the location. You may also change the RAM\n\nsetting if you would like to use more/less memory. Do not use less than 4GB\n\nof memory or your sandbox may not run well. The recommended amount of\n\n8GB is preferred. Click on Import.\n\n9. As a reference, my initial Importing virtual disk image import took roughly a\n\nminute. Once everything is imported, click on the name of the virtual box\n\n(probably called Hortonworks Docker Sandbox HDP) and click on Start. You\n\nshould see something like these:\n\nDS 730: Activity 1 / Page 44 of 51\n\n\n\n", "DS 730: Activity 1 / Page 45 of 51\n\n\n\n\n", "10. Eventually, you will see the following screen. If the screen goes blank, simply\n\nhit the space key. Once you see this, your virtual machine is ready to use:\n\nThe virtual machine that you setup is like any other machine that you have created.\n\nThere is an operating system, a filesystem, etc. For those of you familiar with the\n\ncommand line, you may want to use the command line to do certain things.\n\nHortonworks provides a nice web client that allows you to connect to your machine.\n\n11. Open up an Internet browser and go to http://localhost:4200. You should see\n\nsomething like this:\n\n12. Type in maria_dev for your username and password.\n\nDS 730: Activity 1 / Page 46 of 51\n\n\n\n\n\n", "13. When you are logged in, type in exit to close your connection. It will say\n\nSession closed. and you will see a Connect button in the middle of the\n\nscreen. You can close this window in your browser.\n\nThe Ambari dashboard is a nice administrative tool that comes with the sandbox. You\n\nwill find many applications running but may also find some that are stopped. This is fine\n\nas we won’t be able to explore everything in the sandbox. Our goal is to view/use some\n\nof the main ones.\n\n14. If you quickly go to http://localhost:8080, you may get a 502 Bad Gateway\n\nerror or some other error message. This is normal and it simply means your\n\nHortonworks system is not started completely yet. Wait a few minutes until it\n\nis loaded up.\n\n15. Open up an Internet browser and go to http://localhost:8080 and type in\n\nmaria_dev for the username and the password. This username will be our\n\nmain username throughout the semester. If you log in too quickly, you might\n\nsee something like these:\n\nDS 730: Activity 1 / Page 47 of 51\n\n\n\n\n", "DS 730: Activity 1 / Page 48 of 51\n\n\n\n\n", "This is normal and there is no concern. Just let the system load itself up. It may\n\ntake several minutes but eventually the yellow/red alerts will disappear and\n\neverything will be running. If you still have red alerts to the right of the software\n\ntitles after 15 minutes, then something is likely wrong. See troubleshooting at the\n\nend for possible fixes.\n\nOnce everything is loaded up, feel free to poke around the dashboard to see\n\nwhat options you have. There isn’t much to do right now but we will be using\n\nmany of these software tools in this course. Your final running page will look\n\nsomething like this:\n\nWhen you are finished exploring, feel free to shutdown the virtual machine. To do\n\nthis, go to your VirtualBox Hortonworks window, click File, then Close and finally\n\nDS 730: Activity 1 / Page 49 of 51\n\n\n\n", "choose to Send the shutdown signal. If you choose any of the other options,\n\nyour system may end up in a bad state.\n\nTroubleshooting Hortonworks. If you find your machine in a bad state and your\n\nprograms are taking a long time to run or simply aren’t running at all, these are\n\nthe best tips to getting back into a good state. These are essentially identical to\n\nwhat you saw in the last task. You do not have to do these at the beginning but\n\njust remember these tips are here for future reference.\n\nIf you find there are several alerts or something didn’t load up correctly,\n\nyou are encouraged to debug this by simply restarting the services on the\n\nvirtual machine. If you find that Hadoop, Hive, Pig, etc. are not functioning\n\nproperly, restarting all of the services will often help. To do this, click on the\n\nActions button in the lower left hand corner. Click on Stop All. Click on\n\nthe Confirm Stop button that pops up. It will likely take a couple of\n\nminutes for all services to stop:\n\nOnce all services have stopped, click the OK button. To start up the\n\nservices, click on the Actions button, click on Start All then click on\n\nConfirm Start. Your services are now restarting.\n\nIf the previous step does not solve your issue, then stopping the entire\n\nmachine and restarting it is the next thing to try. Stop your virtual machine\n\nby using the steps on the previous page.\n\nIf restarting your machine does not solve the problem, then the fastest\n\nsolution is to delete your virtual machine and create a new one. If some\n\nstep was skipped or your virtual machine got itself into a bad state, then\n\ntroubleshooting the issue could take hours. However, deleting your current\n\nvirtual machine and creating a new one takes minutes. Just be aware that\n\nif you delete your virtual machine, any files on that virtual machine will be\n\ndeleted as well. Be sure to download/copy over anything you want before\n\ndeleting.\n\nTo delete your virtual machine, go back to step 6 and create a new virtual\n\nmachine.\n\nDS 730: Activity 1 / Page 50 of 51\n\n\n\n", "Project Euler Problem 1 solution\n\n#!/usr/bin/python3.6\n\nsum = 0  #create a sum variable, start it off at 0\n\ncurrent_value = 2  #check all numbers from 2 to 999\n\nwhile current_value < 1000:\n\n#if the current number is divisible by 3 or 5, add to sum\n\nif current_value % 3 == 0 or current_value % 5 == 0:\n\nsum = sum + current_value\n\ncurrent_value = current_value + 1\n\nprint(sum)\n\nDS 730: Activity 1 / Page 51 of 51\n\n\n"]], ["C:\\Users\\mjols\\Documents\\DS UWEC courses\\730 Big Data\\lesson2 hadoop intro\\ds730_lesson2_hadoop_intro.pdf", [" \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nImportant note: Transcripts are not substitutes for textbook assignments. \n\n \n\n1 \n\n\n\n\n", " \n\nWe need simple ways of processing large amounts of data because data is exploding in \n\nsize. It's estimated that Twitter generates 500 million tweets per day. Now, if we assume \n\nthat every tweet has about 100 characters on average, just creating those 5 million \n\ntweets is roughly 400 gigabytes of data. If we assume then that every tweet this read \n\nmaybe 30 to 40 times on average, we end up with about 15 terabytes of data being \n\ntransferred every single day. \n\n \n\nFacebook allows much more data to be transferred every day with pictures and videos. \n\nAnd it's been estimated that Facebook generates about 500 terabytes of data every \n\nsingle day. The internet as a whole is estimated at one zettabyte of data every day. \n\n \n\nAnd that may not mean much. It may just be some random word. So what exactly is \n\none zettabyte? \n\n \n\nWell, digital cameras produce pictures. And these pictures are about say one to five \n\nmegabytes, depending on the quality. Well, there are 1,000 megabytes in a gigabyte. \n\nThere are 1,000 gigabytes in at terabyte. \n\n \n\nThere are 1,000 terabytes in a petabyte, 1,000 petabytes in an exabyte. And finally, there \n\nare 1,000 exabytes in a zettabyte. That's a lot of data that's being generated every single \n\nday. So the big question then becomes, how do we store even a fraction of this data and \n\nanalyze it quickly? And this is where Hadoop comes into play. \n\n \n\n \n\n \n\n \n\n2 \n\n\n", " \n\nHadoop is software that's written in Java by Apache. And Hadoop has two main \n\ncores. The first is with respect to how the data is actually stored. \n\n \n\nHadoop stores its data in something that we call the Hadoop Distributed File System, or \n\nHDFS for short. The second has to do with the processing of that data. And Hadoop uses \n\nthe MapReduce framework to analyze the data.  \n\n \n\n \n\n \n\n \n\n3 \n\n\n", " \n\nThe motivation behind using Hadoop is that it makes distributed storage of our data and \n\ndistributed computing almost automatic. Once you've gone through the process of \n\nsetting up the Hadoop environment, storing data, running MapReduce programs, both of \n\nthese things are rather easy to do. There's no need to manually copy files from one \n\nmachine to another so that you have backups of data. \n\n \n\nThere's no need to create somewhat complicated, multi-threaded jobs or multi-\n\nthreaded Python programs to analyze that data quickly. Another advantage of \n\nHadoop is that if you need more storage or if you need more computation, all you \n\nhave to do is simply add more machines to your environment. And now you have \n\nmore power. \n\n \n\n \n\n \n\n \n\n4 \n\n\n", " \n\nThe setup of Hadoop is nontrivial. It is not just a simple click on an icon, hit Next a bunch \n\nof times, and then install at the end and then the software will install. It's not that type \n\nof software. There are many different ways that you can configure Hadoop. \n\n \n\nAnd so what Apache did is they left many of the details up to you. Since the setup is not \n\nobvious, what you're going to do is go through the setup of Hadoop during this week's \n\nactivity. And I think going through the process of actually setting up Hadoop and seeing \n\nhow the Distributed File System is set up, how MapReduce is working within Hadoop is \n\nreally a great way to learn about all the parts instead of simply listening to me talk about \n\neach of the components in some presentation. \n\n \n\n \n\n \n\n \n\n5 \n\n\n", " \n\nSince we have all of this data, how exactly does Hadoop store all of it? Hadoop doesn't \n\nuse the regular file system that you and I are familiar with. There is no C drive. There's no \n\ntree hierarchy. \n\n \n\nHadoop doesn't use the normal file system that, again, that we're familiar with. Hadoop \n\nuses something called the distributed storage. And what this means is that chunks of a \n\nparticular file could be spread across multiple machines. \n\n \n\nHDFS has two main types of nodes. The first node is what we call the NameNode. And \n\nthe NameNode is sort of the master node that controls the administrative portion of \n\nthe file storage. And what I mean by this is that it knows the location of every file. \n\n \n\nIt knows the location of every node in the system. Really what the NameNode does is it \n\nkeeps track of all of the metadata of your files. The NameNode is responsible for making \n\nsure that data is replicated correctly so that your data is backed up. So if you want three \n\nor four copies, the NameNode will take care of this for you. \n\n \n\nOne of the problems with the original HDFS is that there was only one NameNode. And \n\nthis was a single point of failure. And so if your NameNode crashed, things obviously \n\ndidn't go well. Because of this, there is now a secondary NameNode that can come to \n\nthe rescue if the original NameNode crashes. \n\n \n\nThe other main type of node that we have is something called the DataNode. And this is \n\nwhere the data is actually stored. DataNodes generally store their data in 64 megabyte \n\nblocks. And this is huge compared to the typical blocks that you find in a regular file \n\nsystem. \n\n \n\n \n\n \n\n6 \n\n\n", "If you go to Windows or Linux or whatever, usually the files are stored in four to eight \n\nkilobyte blocks. And so storing files in 64 megabyte blocks is much, much larger. And \n\nthe reason that HDFS does this is for speed. \n\n \n\nIf you have a 50 megabyte file on a regular file system, it could be fragmented all over the \n\nhard  drive.  You  know,  you  could  find  two  of  the  megabytes  in  one  location  and  five \n\nmegabytes somewhere else. \n\n \n\nThis doesn't happen with HDFS. With HDFS, that 50 megabytes is stored in one \n\ncontiguous location. There's no need to search through your hard drive finding this \n\nfragmented file. The file that you need is going to be in simply one block of data. The \n\nHDFS tutorial will go into the details of how all of this works. And again, this week's \n\nactivity will show you how everything is set up. \n\n \n\n \n\n \n\n7 \n\n", " \n\nThe  second  component  of  Hadoop  is  the  actual  computing,  the  analysis  of  the  data. \n\nHadoop uses the MapReduce framework, which you're familiar with now. Right? You take \n\nyour data, and you map it into some intermediate form. \n\n \n\nAnd then, you take that intermediate form. And you reduce it to something else. And \n\nthis is what Hadoop uses. Hadoop uses the MapReduce framework. Instead of copying \n\ndata back and forth from one machine back to the master machine, what Hadoop does \n\nis it sends the code from the master machine to each of the individual machines that's \n\nholding the data. \n\n \n\nAnd this is very important when you think about the latency of the network is generally \n\ngoing to be much higher than the latency in one single machine. So instead of having to \n\nsend that data from one machine to another-- this megabyte or gigabyte or terabytes of \n\ndata back and forth-- what Hadoop does is it simply sends the code to where the data is. \n\nAnd this speeds up the execution of your program or whatever you're trying to do \n\nconsiderably. \n\n \n\n \n\n \n\n \n\n8 \n\n\n", " \n\nWhat we're going to do is go through a quick overview of how Hadoop runs a job. And it \n\nstarts off with something called the JobClient. And the JobClient is really us. We are the \n\nones that are going to be submitting the jobs. \n\n \n\nThe JobTracker is similar to what the NameNode is in HDFS. The JobTracker coordinates \n\nall of the jobs. The TaskTracker is similar to what the DataNode is in HDFS, in that it's \n\nthe thing that does all of the actual work. The TaskTracker is the thing that executes the \n\njobs. \n\n \n\nSo what is the process of Hadoop when it's actually doing the computing? The first step is \n\nthat the job is submitted to the JobTracker. Once the JobTracker knows about the job, \n\nthen what it can do is copy the code to where it needs to be. And then, it's run locally \n\nwherever the data is. \n\n \n\nBut in order to find out where that data is, the JobTracker has to first query the \n\nNameNode to figure out where is the data that I'm analyzing. After the JobTracker \n\ncreates a plan of which node is going to run-- because usually data is replicated across \n\nmultiple machines, and so the JobTracker will find the node that is being least utilized \n\nand assign a job to that particular node. After that's done, what's going to happen is the \n\nJobTracker is going to submit the work to the TaskTrackers. \n\n \n\nOnce the TaskTracker has the job that's going to run, then the TaskTracker is actually \n\ngoing to run the MapReduce code on the data and report back to the JobTracker with \n\nthings called heartbeats. And so if one of these TaskTracker fails, then the JobTracker will \n\nknow this. And it will reschedule the job. Once all the tasks are finished, then the \n\nJobTracker reports updates on the status so that we know whether or not a job is \n\n \n\ncomplete.  \n\n \n\n \n\n9 \n\n\n", " \n\nThe one thing that I want to highlight on this slide is the fact that Hadoop can process \n\nterabytes, petabytes, zettabytes of data without having to worry about how much \n\nmemory is available. If you wanted to read in all of the data that we're processing into a \n\nJava or Python program, we would quickly run out of memory, especially if we were \n\ndoing more than, say, 20 gigabytes of data. \n\nMost machines don't have 20 gigabytes of data. \n\n \n\nAnd so we can't simply just read it all into memory and then execute some program on \n\nthat data. So what we need to do is be able to stream the data, this terabytes or \n\npetabytes of data. And the streaming ability of Hadoop allows us to process this data \n\nvery quickly without needing a lot of memory. \n\n \n\n \n\n \n\n \n\n10 \n\n\n", "The thing that you should take away from Hadoop is that you can easily distribute data \n\nacross multiple machines. And you can also easily process that data across multiple \n\nmachines. And we'll see how this is done in this week's activity. \n\n \n\n \n\n \n\n \n\n \n\n11 \n\n\n", " \n\n \n\n12 \n\n\n"]], ["C:\\Users\\mjols\\Documents\\DS UWEC courses\\730 Big Data\\lesson2 hadoop intro\\VNI_Hyperconnectivity_WP.pdf", [" \n\n \n\nThe Zettabyte Era: Trends and Analysis \n\nWhite Paper \n\n \n\nMay 2015 \n\nThis document is part of the Cisco® Visual Networking Index (VNI), an ongoing \n\ninitiative to track and forecast the impact of visual networking applications. \n\nThe document presents some of the main findings of Cisco’s global IP traffic \n\nforecast and explores the implications of IP traffic growth for service providers. \n\nFor a more detailed look at the forecast and the methodology behind it, visit \n\nCisco VNI: Forecast and Methodology, 2014–2019. \n\nExecutive Summary \n\nAnnual global IP traffic will pass the zettabyte (1000 exabytes) threshold by the end of 2016, and will \n\nreach 2 zettabytes per year by 2019. By 2016, global IP traffic will reach 1.1 zettabytes per year, or 88.4 \n\nexabytes (nearly one billion gigabytes) per month, and by 2019, global IP traffic will reach 2.0 zettabytes per year, \n\nor 168 exabytes per month. \n\nGlobal IP traffic has increased fivefold over the past five years, and will increase threefold over the next \n\nfive years. Overall, IP traffic will grow at a compound annual growth rate (CAGR) of 23 percent from 2014 to 2019. \n\nBusy-hour Internet traffic is growing more rapidly than average Internet traffic. Busy-hour (or the busiest \n\n60-minute period in a day) Internet traffic increased 37 percent in 2014, compared with 29 percent growth in \n\naverage traffic. Busy-hour Internet traffic will increase by a factor of 3.9 between 2014 and 2019, and average \n\nInternet traffic will increase by a factor of 3.2. Busy-hour Internet traffic will reach 1.4 petabits per second (Pbps) \n\nin 2019, and average Internet traffic will reach 414 terabits per second (Tbps). \n\nMetro traffic surpassed long-haul traffic in 2014, and will account for 66 percent of total IP traffic by \n\n2019. Metro traffic will grow more than twice as fast as long-haul traffic from 2014 to 2019. The higher growth in \n\nmetro networks is due in part to the increasingly significant role of content delivery networks (CDNs), which bypass \n\nlong-haul links and deliver traffic to metro and regional backbones. \n\n© 2015 Cisco and/or its affiliates. All rights reserved. This document is Cisco Public. \n\nPage 1 of 29 \n\n\n\n\n\n\n", " \n\n \n\nContent delivery networks (CDNs) will carry nearly two-thirds of Internet traffic by 2019. Sixty-two percent of \n\nall Internet traffic will cross CDNs by 2019 globally, up from 39 percent in 2014. \n\nTwo-thirds of all IP traffic will originate with non-PC devices by 2019. In 2014, only 40 percent of total IP traffic \n\noriginated with non-PC devices, but by 2019 the non-PC share of total IP traffic will grow to 67 percent. PC-\n\noriginated traffic will grow at a CAGR of 9 percent, and TVs, tablets, smartphones, and machine-to-machine (M2M) \n\nmodules will have traffic growth rates of 17 percent, 65 percent, 62 percent, and 71 percent respectively. \n\nTraffic from wireless and mobile devices will exceed traffic from wired devices by 2016. By 2016, wired \n\ndevices will account for 47 percent of IP traffic, and Wi-Fi and mobile devices will account for 53 percent of \n\nIP traffic. In 2014, wired devices accounted for the majority of IP traffic, at 54 percent. \n\nGlobal Internet traffic in 2019 will be equivalent to 66 times the volume of the entire global Internet in 2005. \n\nGlobally, Internet traffic will reach 37 gigabytes (GB) per capita by 2019, up from 15.5 GB per capita in 2014. \n\nThe number of devices connected to IP networks will be more than three times the global population \n\nby 2019. There will be more than three networked devices per capita by 2019, up from nearly two networked \n\ndevices per capita in 2014. Accelerated in part by the increase in devices and the capabilities of those devices, \n\nIP traffic per capita will reach 22 GB per capita by 2019, up from 8 GB per capita in 2014. \n\nBroadband speeds will more than double by 2019. By 2019, global fixed broadband speeds will reach \n\n42.5 Mbps, up from 20.3 Mbps in 2014. \n\nGlobal Internet Video Highlights \n\nIt would take an individual more than 5 million years to watch the amount of video that will cross global \n\nIP networks each month in 2019. Every second, nearly a million minutes of video content will cross the network \n\nby 2019. \n\nGlobally, IP video traffic will be 80 percent of all IP traffic (both business and consumer) by 2019, up from \n\n67 percent in 2014. This percentage does not include the amount of video exchanged through peer-to-peer (P2P) \n\nfile sharing. The sum of all forms of video (TV, video on demand [VoD], Internet, and P2P) will continue to be in the \n\nrange of 80 to 90 percent of global consumer traffic by 2019. \n\nInternet video to TV grew 47 percent in 2014. This traffic will continue to grow at a rapid pace, increasing \n\nfourfold by 2019. Internet video to TV will be 17 percent of consumer Internet video traffic in 2019, up from 16 \n\nConsumer VoD traffic will nearly double by 2019. The amount of VoD traffic in 2019 will be equivalent to \n\npercent in 2014. \n\n7 billion DVDs per month. \n\nGlobal Mobile Highlights \n\nGlobally, mobile data traffic will increase 10-fold between 2014 and 2019. Mobile data traffic will grow at \n\na CAGR of 57 percent between 2014 and 2019, reaching 24.3 exabytes per month by 2019. \n\nGlobal mobile data traffic will grow three times faster than fixed IP traffic from 2014 to 2019. Global mobile \n\ndata traffic was 4 percent of total IP traffic in 2014, and will be 14 percent of total IP traffic by 2019. \n\n© 2015 Cisco and/or its affiliates. All rights reserved. This document is Cisco Public. \n\nPage 2 of 29 \n\n\n\n", "Regional Highlights \n\nIP traffic is growing fastest in the Middle East and Africa, followed by Asia Pacific. Traffic in the Middle East \n\nand Africa will grow at a CAGR of 44 percent between 2014 and 2019. \n\nSummary of regional growth rates: \n\n●  IP traffic in North America will reach 49.7 exabytes per month by 2019; growing at a CAGR of 20 percent. \n\n●  IP traffic in Western Europe will reach 24.7 exabytes per month by 2019; growing at a CAGR of 21 percent. \n\n●  IP traffic in Asia Pacific will reach 54.4 exabytes per month by 2019; growing at a CAGR of 21 percent. \n\n●  IP traffic in Latin America will reach 12.9 exabytes per month by 2019; growing at a CAGR of 25 percent. \n\n●  IP traffic in Central and Eastern Europe will reach 16.9 exabytes per month by 2019; growing at a CAGR \n\n●  IP traffic in the Middle East and Africa will reach 9.4 exabytes per month by 2019; growing at a CAGR of \n\nof 33 percent. \n\n44 percent. \n\nNote:    Several interactive tools are available to allow you to create custom highlights and forecast charts by \n\nregion, by country, by application, and by end-user segment (refer to the Cisco VNI Forecast Highlights tool \n\nand the Cisco VNI Forecast Widget tool). \n\nGlobal Business Highlights \n\nBusiness IP traffic will grow at a CAGR of 20 percent from 2014 to 2019. Increased adoption of advanced \n\nvideo communications in the enterprise segment will cause business IP traffic to grow by a factor of 2 between \n\n2014 and 2019. \n\nBusiness Internet traffic will grow at a faster pace than IP WAN. IP WAN will grow at a CAGR of 9 percent, \n\ncompared with a CAGR of 20 percent for fixed business Internet and 51 percent for mobile business Internet. \n\nBusiness IP traffic will grow fastest in the Middle East and Africa. Business IP traffic in the Middle East and \n\nAfrica will grow at a CAGR of 26 percent, a faster pace than the global average of 20 percent. In volume, Asia \n\nPacific will have the largest amount of business IP traffic in 2018, at 9.5 exabytes per month. North America \n\nwill be second, at 8.0 exabytes per month. \n\n \n\n \n\n© 2015 Cisco and/or its affiliates. All rights reserved. This document is Cisco Public. \n\nPage 3 of 29 \n\n\n\n\n\n", "Forecast Overview \n\nThe current Cisco Visual Networking Index (VNI) forecast projects global IP traffic to nearly triple from 2014 to \n\n2019. See Appendix A for a detailed summary. Overall IP traffic is expected to grow to 168 exabytes per month \n\nby 2019, up from 59.9 exabytes per month in 2014, a CAGR of 23 percent (Figure 1). \n\nFigure 1.    Cisco VNI Forecasts 168 Exabytes per Month of IP Traffic by 2019 \n\n \n\nSource: Cisco VNI Global IP Traffic Forecast, 2014–2019 \n\n \n\n2014–2019.” \n\nFor more details about Cisco’s forecasting methodology, refer to the paper “Cisco VNI: Forecast and Methodology, \n\nTo understand the magnitude of IP traffic volumes, it helps to look at the numbers in more familiar terms: \n\n●  By 2019, the gigabyte equivalent of all movies ever made will cross the global Internet every 2 minutes. \n\n●  Globally, IP traffic will reach 511 Tbps in 2019, the equivalent of 142 million people streaming Internet \n\nHD video simultaneously, all day, every day. \n\n●  Global IP traffic in 2019 will be equivalent to 504 billion DVDs per year, 42 billion DVDs per month, \n\nor 58 million DVDs per hour. \n\nTotal Internet traffic has experienced dramatic growth in the past two decades. More than 20 years ago, in 1992, \n\nglobal Internet networks carried approximately 100 GB of traffic per day. Ten years later, in 2002, global Internet \n\ntraffic amounted to 100 gigabytes per second (GBps). In 2014, global Internet traffic reached 16,144 GBps. \n\nTable 1 provides a view of the historical benchmarks for total Internet traffic. \n\n \n\n \n\n© 2015 Cisco and/or its affiliates. All rights reserved. This document is Cisco Public. \n\nPage 4 of 29 \n\n\n\n\n", " \n\nYear \n\n1992  \n\n1997 \n\n2002 \n\n2007 \n\n2014 \n\n2019 \n\n \n\n \n\n \n\nTable 1. \n\nThe Cisco VNI Forecast—Historical Internet Context \n\nGlobal Internet Traffic \n\n100 GB per day \n\n100 GB per hour \n\n100 GBps \n\n2000 GBps  \n\n16,144 GBps \n\n51,794GBps  \n\nSource: Cisco VNI, 2015 \n\nPer capita IP and Internet traffic growth has followed a similarly steep growth curve over the past decade. Globally, \n\nIP traffic will reach 22 GB per capita by 2019, up from 8 GB per capita in 2014, and Internet traffic will reach 18 GB \n\nper capita by 2019, up from 6 GB per capita in 2014. Not long ago, in 2008, per capita Internet traffic was 1 GB per \n\nmonth. In 2000, per capita Internet traffic was 10 megabytes (MB) per month. \n\nThe sections that follow explore the trends contributing to the continued growth of global IP traffic. \n\nTrend 1: Continued Shifts in Mix of Devices and Connections \n\nGlobally, devices and connections (11.5 percent CAGR) are growing faster than both the population (1.1 percent \n\nCAGR) and Internet users (6.9 percent CAGR). See Figure 2. This trend is accelerating the increase in the \n\naverage number of devices and connections per household and per Internet user. Each year, various new devices \n\nin different form factors with increased capabilities and intelligence are introduced and adopted in the market. \n\nA growing number of M2M applications, such as smart meters, video surveillance, healthcare monitoring, \n\ntransportation, and package or asset tracking, also are causing connection growth. By 2019, M2M connections \n\nwill be 43 percent of the total devices and connections. \n\nFigure 2.    Global Devices and Connections Growth \n\nSource: Cisco VNI Global IP Traffic Forecast, 2014–2019 \n\n \n\n© 2015 Cisco and/or its affiliates. All rights reserved. This document is Cisco Public. \n\nPage 5 of 29 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "M2M connections will be the fastest-growing category, growing more than threefold during the forecast period, at \n\n26 percent CAGR, to 10.5 billion connections by 2019. Tablets and smartphones will grow the second fastest, at \n\n17 percent CAGR (increasing by a factor of 2.2). Connected TVs (which include flat-panel TVs, set-top boxes, \n\ndigital media adapters (DMAs), Blu-ray disc players, and gaming consoles) will nearly double, to 2.9 billion by \n\n2019. PCs will continue to stabilize (less than a quarter of 1 percent decline) over the forecast period. There will be \n\nnearly as many tablets as laptops by the end of 2019 (919 million laptops and 922 million tablets). \n\nBy 2019, the consumer share of the total devices, including both fixed and mobile devices, will be 79 percent, \n\nwith business claiming the remaining 21 percent. Consumer share will grow slightly more slowly, at \n\n11 percent CAGR, relative to the business segment, which will grow at 13.7 percent CAGR. For more details \n\nabout the growth in devices and connections in residential, consumer mobile, and business segments, refer \n\nto the Cisco VNI Service Adoption Forecast, 2014–2019. \n\nGlobally, the average number of devices and connections per capita will grow from 2 in 2014 to 3.2 by 2019 \n\nTable 2. \n\nAverage Number of Devices and Connections per Capita \n\n2014 \n\n 1.60  \n\n2.40 \n\n1.97 \n\n0.99 \n\n6.14 \n\n4.43 \n\n1.95 \n\n2019 \n\n2.55  \n\n4.29 \n\n2.92 \n\n1.36 \n\n11.6 \n\n8.18 \n\n3.20 \n\nCAGR \n\n9.8 % \n\n12.3% \n\n8.2% \n\n6.4% \n\n13.5% \n\n13.1% \n\n10.4% \n\n(Table 2). \n\n              \n\nAsia Pacific \n\nCentral and Eastern Europe \n\nLatin America \n\nMiddle East and Africa \n\nNorth America \n\nWestern Europe \n\nGlobal \n\n \n\nSource: Cisco VNI, 2015 \n\nThe changing mix of devices and connections and growth in multi-device ownership affects traffic and can be seen \n\nin the changing device contribution to total IP traffic. At the end of 2014, 40 percent of IP traffic and 22.5 percent \n\nof consumer Internet traffic originated from non-PC devices. By 2019, 67 percent of IP traffic and 64 percent of \n\nconsumer Internet traffic will originate from non-PC devices (Figure 3). \n\n \n\n \n\n© 2015 Cisco and/or its affiliates. All rights reserved. This document is Cisco Public. \n\nPage 6 of 29 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", " \n\n \n\n \n\n \n\nFigure 3.    Global IP Traffic by Devices \n\nSource: Cisco VNI Global IP Traffic Forecast, 2014–2019 \n\nAs in the case of mobile networks, video devices can have a multiplier effect on traffic. An Internet-enabled HD \n\ntelevision that draws 45 minutes of content per day from the Internet would generate as much Internet traffic as an \n\nentire household today. With the growth of video viewing on tablets, traffic from tablets is growing as a percentage \n\nof total Internet traffic. Tablets will account for 24 percent of total global Internet traffic by 2019, up from 6 percent \n\nin 2014 (Figure 4). \n\nFigure 4.    Global Internet Traffic by Device Type \n\n \n\n \n\nSource: Cisco VNI Global IP Traffic Forecast, 2014–2019 \n\n© 2015 Cisco and/or its affiliates. All rights reserved. This document is Cisco Public. \n\nPage 7 of 29 \n\n\n\n\n\n", " \n\n \n\n \n\nThe video impact of the devices on the traffic is more pronounced due to the introduction of ultra-high-definition \n\n(UHD), or 4K, video streaming. This technology has such an impact because the bit rate for 4K video at about \n\n18 Mbps is more than double the HD video bit rate and nine times more than standard-definition (SD) video bit \n\nrate. We estimate that by 2019, 31 percent of the installed flat-panel TV sets will be UHD, up from 2.7 percent in \n\n2014 (Figure 5). \n\nFigure 5.   \n\nIncreasing Video Definition: By 2019, More Than 30 Percent of Connected Flat-Panel TV Sets Will Be 4K \n\nSource: Cisco VNI Global IP Traffic Forecast, 2014–2019 \n\nUltra-HD (or 4K) IP VoD will account for 21 percent of global VoD traffic in 2019 (Figure 6). \n\nFigure 6.    Global 4K Video Traffic \n\n \n\n \n\nSource: Cisco VNI Global IP Traffic Forecast, 2014–2019 \n\n© 2015 Cisco and/or its affiliates. All rights reserved. This document is Cisco Public. \n\nPage 8 of 29 \n\n\n\n\n\n", " \n\n \n\n \n\n \n\nTrend 2: IPv6 Adoption Enables Internet of Everything Connectivity \n\nThe transition from an IPv4 environment to an IPv6 environment is making excellent progress, with increases \n\nin IPv6 device capabilities, content enablement, and operators implementing IPv6 in their networks. These \n\ndevelopments are particularly important because Asia, Europe and Latin America have already exhausted their \n\nIPv4 allotments, and North America is expected to exhaust its allotment this summer and Africa by 2019. \n\nTable 3 shows the projected exhaustion dates as of May 2015, according to the Regional Internet Registries (RIR). \n\nTable 3. \n\nIPv4 Address Exhaustion Dates \n\nRegional Internet Registries \n\nAsia-Pacific Network Information Centre (APNIC) \n\nExhaustion Date \n\nApril 19, 2011 (actual) \n\nRéseaux IP Européens Network Coordination Centre (RIPE NCC) \n\nSeptember 14, 2012 (actual) \n\nLatin America and Caribbean Network Information Centre (LACNIC) \n\nJune 10, 2014 (actual) \n\nAmerican Registry for Internet Numbers (ARIN) \n\nJuly 10, 2015 (projected) \n\nAfrican Network Information Center (AFRINIC) \n\nFebruary 19, 2019 (projected) \n\nBuilding on the VNI IPv6-capable devices analysis, the forecast estimates that globally there will be 10 billion \n\nIPv6-capable fixed and mobile devices by 2019, up from 3 billion in 2014, a CAGR of 26 percent. In terms of \n\npercentages, 41 percent of all fixed and mobile networked devices will be IPv6-capable by 2019, up from \n\n22 percent in 2014 (Figure 7). \n\nThis estimate is based on the capability of the device and the network connection to support IPv6, and is not a \n\nprojection of active IPv6 connections. Mobile-device IPv6 capability is assessed based on OS support of IPv6 \n\nas well as by estimating the type of mobile network infrastructure to which the device is capable of connecting \n\n(3.5G or higher.) Fixed-device IPv6 capability is assessed based on device support of IPv6 as well as an \n\nestimation of the capability of the residential customer premises equipment (CPE) or business routers to support \n\nIPv6, depending on the device end-user segment. \n\nFigure 7.    Global IPv6-Capable Devices and Connections Forecast 2014–2019 \n\nSource: Cisco VNI Global IP Traffic Forecast, 2014–2019 \n\n \n\n© 2015 Cisco and/or its affiliates. All rights reserved. This document is Cisco Public. \n\nPage 9 of 29 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", " \n\n \n\n \n\nGlobally, 81 percent of smartphones and tablets will be IPv6-capable by 2019, up from 56 percent in 2014. \n\nGlobally, there will be 4.5 billion IPv6-capable smartphones and tablets by 2019, up from 1.4 billion in 2014. \n\nBy 2019, 21 percent of M2M connections will be IPv6-capable, reaching 2.2 billion, a 64 percent compound annual \n\ngrowth rate. \n\nAccording to the World IPv6 Launch Organization in May 2015, fixed and mobile network operators worldwide are \n\ndeploying IPv6 and starting to report notable IPv6 traffic generation. KDDI reported nearly18 percent, Romania’s \n\nRCS & RDS reported nearly 20 percent, France’s Free Telecom reported 29 percent, Comcast reported \n\n37 percent, AT&T reported 52 percent, and Verizon Wireless reported 69 percent. According to Google, in May \n\n2015 the percentage of users who access Google through IPv6 is about 6 percent. Akamai reported threefold \n\ngrowth in its IPv6 traffic volume worldwide since last year, with about 900,000 IPv6 hits per second in May 2015, \n\ncompared to nearly 270,000 IPv6 hits per second in May 2014. \n\nAmid these industry developments, this year’s VNI forecast is undertaking an effort to estimate the potential IPv6 \n\nnetwork traffic that could be generated if a percentage of IPv6-capable devices become actively connected to an \n\nIPv6 network, given the estimated global average for monthly traffic per device type. \n\nLooking to 2019, if 60 percent of IPv6-capable devices are actively connected to an IPv6 network, the forecast \n\nestimates that globally IPv6 traffic would amount to 45.7 exabytes per month, or 34 percent of total Internet traffic. \n\n(Figure 8). \n\nFigure 8.    Projected Global Fixed and Mobile IPv6 Traffic Forecast 2014–2019 \n\nSource: Cisco VNI Global IP Traffic Forecast, 2014–2019 \n\n \n\nThis initial estimation of potential IPv6 traffic is based on the assumptions that IPv6 device capability, IPv6 content \n\nenablement, and IPv6 network deployment will keep pace with current trends and may even accelerate during the \n\nforecast period. Considering the interdependence of these variables, forecast assumptions could be subject to \n\nrefinement as our analysis continues. \n\n© 2015 Cisco and/or its affiliates. All rights reserved. This document is Cisco Public. \n\nPage 10 of 29 \n\n\n\n\n", " \n\n \n\n \n\nContent providers are also moving to increase the IPv6 enablement of their sites and services. Eight percent of \n\nFacebook’s members worldwide use IPv6 to reach its content today, and in the United States this number is \n\n17.5 percent and doubling each year. Google logs just over 6 percent of its visitors coming over IPv6. According to \n\nCisco’s IPv6 labs, by 2019 the content available over IPv6 will be about 25 percent. There can be, however, \n\nvariation depending on the popularity of websites across regions and countries. In addition, specific country \n\ninitiatives and content provider deployments have had a positive impact on local IPv6 content reachability. \n\nOverall, the likelihood that a significant portion of Internet traffic will be generated over IPv6 networks holds \n\nconsiderable opportunity for network operators, content providers, and end users seeking to gain the scalability \n\nand performance benefits of IPv6 and enable the Internet of Everything (IoE). \n\nTrend 3: M2M Applications Across Many Industry Verticals Drive IoE Growth \n\nThe Internet of Everything phenomenon, in which people, processes, data, and things connect to the Internet and \n\neach other, is showing tangible growth. Globally, M2M connections will grow more than threefold, from 3.3 billion in \n\n2014 to 10.5 billion by 2019 (Figure 9). There will be nearly one-and-a-half M2M connections for each member of \n\nthe global population by 2019. \n\nFigure 9.    Global M2M Connection Growth \n\nSource: Cisco VNI Global IP Traffic Forecast, 2014–2019 \n\n \n\nConnected home applications, such as home automation, home security and video surveillance, connected white \n\ngoods, and tracking applications, will represent 48 percent, or nearly half, of the total M2M connections by 2019, \n\nshowing the pervasiveness of M2M in our lives (Figure 10). Connected healthcare, with applications such as health \n\nmonitors, medicine dispensers, first-responder connectivity, and telemedicine, will be the fastest growing industry \n\nsegment, at 49 percent CAGR. Connected car applications will have the second fastest growth, at 37 percent \n\nCAGR. Chips for pets and livestock, digital health monitors, and numerous other next-generation M2M services \n\nare promoting this growth. \n\n© 2015 Cisco and/or its affiliates. All rights reserved. This document is Cisco Public. \n\nPage 11 of 29 \n\n\n\n\n\n", " \n\n \n\n \n\n \n\nFigure 10.    Global M2M Connection Growth by Industry Verticals \n\nSource: Cisco VNI Global IP Traffic Forecast, 2014–2019 \n\nAlthough the number of connections is growing threefold, global M2M IP traffic will grow 15-fold over this same \n\nperiod, from 308 petabytes in 2014 (0.5 percent of global IP traffic) to 4.6 exabytes by 2019 (2.7 percent of global \n\nIP traffic). See Figure 11. The amount of traffic is growing faster than the number of connections because more \n\nvideo applications are being deployed on M2M connections and because of the increased use of applications, \n\nsuch as telemedicine and smart car navigation systems, that require greater bandwidth and lower latency. \n\nFigure 11.    Global M2M Traffic Growth: Petabytes \n\n \n\n \n\nSource: Cisco VNI Global IP Traffic Forecast, 2014–2019 \n\n© 2015 Cisco and/or its affiliates. All rights reserved. This document is Cisco Public. \n\nPage 12 of 29 \n\n\n\n\n\n", " \n\n \n\n \n\nTrend 4: Service Adoption Trends: Residential, Consumer Mobile, and Business Services \n\nGlobal Residential Services: Video Continues to Grow \n\nBetween 2013 and 2014, the highest growth happened on the Internet side in online video, with 18 percent \n\nyear-over-year (YoY) growth. Social networking was the most widely adopted residential Internet service, with \n\nYoY growth of 4 percent, growing from 1.2 billion users in 2013 to 1.3 billion users in 2014. See Figure 12. \n\nFigure 12.    Global Residential Services Adoption and Growth \n\nSource: Cisco VNI Service Adoption Forecast, 2014–2019 \n\n \n\nBy 2019, digital TV and online video will be the two services with the highest penetration rates, with 79 percent \n\nand 75 percent respectively. The fastest growth will come from time-delayed TV services such as personal video \n\nrecorder (PVR) and digital video recorder (DVR) services, at 8 percent CAGR. Online music (8 percent CAGR) will \n\nbe the fastest growing residential Internet service. Online music and video are both driven by cloud-based personal \n\nstorage and sharing sites, in addition to both copyrighted and user-generated content use. Among the digital-TV \n\nservices, time-delayed or DVR/PVR service will grow the fastest, at 14 percent CAGR. \n\nGlobal Consumer Mobile Services \n\nBetween 2013 and 2014, all consumer mobile services grew more than 10 percent YoY. The highest growth was in \n\nconsumer location-based services (LBS), with YoY growth of 47 percent, from a base of 406 million users in 2013 \n\nto 597 million in 2014. Other significant YoY growth was in mobile banking and commerce (46 percent), followed by \n\nmobile video (39 percent). Regions such as Latin America (71.4 percent YoY growth) and the Middle East and \n\nAfrica (70.7 percent YoY growth) had the fastest growth in consumer mobile LBS. Mobile banking and commerce \n\nalso grew the fastest in Latin America, at 76 percent YoY growth, and mobile video growth was led by Central and \n\nEastern Europe, at 68 percent YoY growth (Figure 13). \n\n© 2015 Cisco and/or its affiliates. All rights reserved. This document is Cisco Public. \n\nPage 13 of 29 \n\n\n\n\n", "Figure 13.    Global Consumer Mobile Services Adoption and Growth \n\n \n\n \n\nSource: Cisco VNI Service Adoption Forecast, 2014–2019 \n\nFrom 2014 to 2019, seven out of eight consumer mobile services will grow at more than 15 percent CAGR, and \n\ntwo will grow at more than 25 percent CAGR. The fastest growth will be in consumer LBS (27.5 percent), followed \n\nby mobile commerce (25 percent). Regions with especially high rates of growth in mobile commerce services are \n\nthe Middle East and Africa, Latin America, and Asia Pacific, which have historically been underserved (or not \n\nreached) by traditional brick-and-mortar financial institutions. \n\nGlobal Business Services \n\nBetween 2013 and 2014, the highest YoY growth was in business LBS, with a 41 percent increase: from 68 million \n\nusers in 2013 to 95 million in 2014. Other significant YoY growth was in desktop videoconferencing (30 percent). \n\nSee Figure 14. \n\nFigure 14.    Global Business Services Adoption and Growth \n\n \n\n \n\nSource: Cisco VNI Service Adoption Forecast, 2014–2019 \n\n \n\n© 2015 Cisco and/or its affiliates. All rights reserved. This document is Cisco Public. \n\nPage 14 of 29 \n\n\n\n\n\n", " \n\n \n\n \n\n \n\nBusiness LBS includes services used by corporate subscribers in which the subscription is generally paid by the \n\nemployer. These services include salesforce and field-force automation, fleet management, etc., among others \n\nThis year’s study suggests further slowdown in the growth of room-based videoconferencing. Single-codec \n\nvideoconferencing systems grew except in Central and Eastern Europe and Middle East and Africa. All regions, \n\nwith the exception of Asia Pacific and Latin America, experienced a decline in executive conferencing systems, \n\nand all regions except Latin America experienced a decline in multicodec systems. Multicodec systems are \n\ntypically fully managed and so are expensive to maintain and operate. As unit sales drop, so does the network of \n\nunits to connect to, and therefore use may be limited. Low-use systems are decommissioned over time due to the \n\nhigh fixed cost of managing these systems. We see that personal or desktop videoconferencing is increasingly \n\nreplacing room-based conferencing as video becomes simpler and more integrated into unified communications \n\nbusiness service offers. \n\nFrom 2014 to 2019, the fastest-growing business service is expected to be desktop or personal videoconferencing. \n\nThe growth in personal videoconferencing, specifically unified communications–based videoconferencing, has \n\nrecently accelerated due to the higher quality and lower price of new services and products, and also due to the \n\navailability of desktop videoconferencing offers, which can be standalone or integrated. Also, the growth in mobile \n\nclients will support videoconferencing growth. Conversely, the use of web conferencing without video will show a \n\ndecline of 3 percent CAGR over the forecast period. \n\nFor details about all aspects of the service adoption study, use the Cisco VNI Service Adoption Highlights tool. \n\nTrend 5: Applications Traffic Growth \n\nThe sum of all forms of IP video, which includes Internet video, IP VoD, video files exchanged through file sharing, \n\nvideo-streamed gaming, and videoconferencing, will continue to be in the range of 80 to 90 percent of total IP \n\ntraffic. Globally, IP video traffic will account for 80 percent of traffic by 2019 (Figure 15). \n\nFigure 15.    Global IP Traffic by Application Category \n\nSource: Cisco VNI Global IP Traffic Forecast, 2014–2019 \n\n \n\n© 2015 Cisco and/or its affiliates. All rights reserved. This document is Cisco Public. \n\nPage 15 of 29 \n\n\n\n\n\n", "The implications of video growth are difficult to overstate. With video growth, Internet traffic is evolving from a \n\nrelatively steady stream of traffic (characteristic of P2P) to a more dynamic traffic pattern. \n\nIn the past year, service providers have observed a pronounced increase in traffic associated with gaming \n\ndownloads. Newer consoles such as the Xbox One and PlayStation 4 have sufficient on-board storage to enable \n\ngamers to download new games rather than buy them on disc. These graphically intense games are large files, \n\nand gaming downloads are already 2 percent of consumer fixed Internet traffic, and will reach 5 percent of \n\nconsumer fixed Internet traffic by 2019. Furthermore, these downloads tend to occur during peak usage periods, \n\nwith gaming downloads reaching up to 10 percent of busy-hour traffic. \n\nImpact of Video on Traffic Symmetry \n\nWith the exception of short-form video and video calling, most forms of Internet video do not have a large upstream \n\ncomponent. As a result, traffic is not becoming more symmetric, which many expected when user-generated \n\ncontent first became popular. The emergence of subscribers as content producers is an extremely important social, \n\neconomic, and cultural phenomenon, but subscribers still consume far more video than they produce. Upstream \n\ntraffic has been slightly declining as a percentage for several years. \n\nIt appears likely that residential Internet traffic will remain asymmetric for the next few years. However, numerous \n\nscenarios could result in a move toward increased symmetry; for example: \n\n●  Content providers and distributors could adopt P2P as a distribution mechanism. There has been a strong \n\ncase for P2P as a low-cost CDS for many years, yet most content providers and distributors have opted for \n\ndirect distribution, with the exception of applications such as PPStream and PPLive in China, which offer \n\nlive video streaming through P2P and have had great success. If content providers in other regions follow \n\nsuit, traffic could rapidly become highly symmetric. \n\n●  High-end video communications could accelerate, requiring symmetric bandwidth. PC-to-PC video calling is \n\ngaining momentum, and the nascent mobile video calling market appears to have promise. If high-end video \n\ncalling becomes popular, traffic could move toward greater symmetry. \n\nGenerally, if service providers provide ample upstream bandwidth, applications that use upstream capacity will \n\nbegin to appear. \n\nTrend 6: “Cord-Cutting” Analysis \n\nIn the context of the VNI Forecast, “Cord-cutting” refers to the trend in which traditional and subscription television \n\nviewing is increasingly being supplanted by other means of video viewing, such as online and mobile video, which \n\nare available to viewers through fixed and mobile Internet connections. \n\nWe are seeing a trend in which the growth in digital television service that denotes television viewing across all \n\ndigital platforms (cable, IPTV, satellite, etc.) is growing much slowly relative to online video and mobile video \n\n(Figure 16). This trend is more pronounced in regions such as North America and Western Europe, where the \n\npenetration of digital TV is already high. Also, in emerging regions mobile video growth rates are even higher, as \n\nthese regions are skipping over fixed connectivity. \n\n \n\n \n\n© 2015 Cisco and/or its affiliates. All rights reserved. This document is Cisco Public. \n\nPage 16 of 29 \n\n\n\n", " \n\n \n\n \n\n \n\nFigure 16.    Global Online and Mobile Video Growing Faster Than Digital TV \n\nSource: Cisco VNI Global IP Traffic Forecast, 2014–2019 \n\nAnother argument supporting this trend is that the total addressable markets for these services—residential \n\nInternet users (for online and mobile video) and total TV households (for digital-TV households)—show significantly \n\ndifferent growth patterns (Figure 17). Residential Internet users are expected to increase at a CAGR of nearly \n\n7 percent, whereas the number of TV households is flattening, with a meager 2 percent forecasted CAGR. \n\nFigure 17.    Growth in Global Residential Internet Users Compared to Growth in Global TV Households \n\n \n\n \n\nSource: Cisco VNI Global IP Traffic Forecast, 2014–2019 \n\n© 2015 Cisco and/or its affiliates. All rights reserved. This document is Cisco Public. \n\nPage 17 of 29 \n\n\n\n\n\n", " \n\n \n\n \n\n \n\nAlso, if we look at Internet devices such as DMAs, we find that although they represent only 7 percent of all Internet \n\nconnected set-top boxes (STBs)—including, service provider STBs, gaming consoles, and directly connected \n\nInternet TV sets—by 2019, they will represent 32 percent of global Internet STB traffic. This trend again shows \n\nthat there is increasingly less reliance on STBs managed by service providers for Internet access in general and \n\nfor video specifically (Figure 18). \n\nFigure 18.    Growth in Global Digital Media Adapters \n\nSource: Cisco VNI Global IP Traffic Forecast, 2014–2019 \n\nFrom a traffic perspective, we expect that on average a household that is still on linear TV will generate much less \n\ntraffic than a household that has cut the cord and is relying on Internet video (Figure 19). A cord-cutting household \n\nwill generate 92 GB per month in 2015, compared to 43 GB per month for an average household. This difference \n\noccurs because linear television generates much less traffic (one stream of video shared across a number of \n\nlinear-TV households) than Internet video, which is unicast to each Internet video device. \n\nFigure 19.    Global Cord Cutting Generates Double the Traffic \n\n \n\n \n\nSource: Cisco VNI Global IP Traffic Forecast, 2014–2019 \n\n© 2015 Cisco and/or its affiliates. All rights reserved. This document is Cisco Public. \n\nPage 18 of 29 \n\n\n\n\n\n", "Trend 7: Impact of Accelerating Speeds on Traffic Growth \n\nFixed Speeds \n\nBroadband speed is a crucial enabler of IP traffic. Broadband speed improvements result in increased consumption \n\nand use of high-bandwidth content and applications. The global average broadband speed continues to grow and \n\nwill more than double from 2014 to 2019, from 20.3 Mbps to 42.5 Mbps. Table 4 shows the projected broadband \n\nspeeds from 2014 to 2019. Several factors influence the fixed broadband speed forecast, including the deployment \n\nand adoption of fiber to the home (FTTH), high-speed DSL, and cable broadband adoption, as well as overall \n\nbroadband penetration. Among the countries covered by this study, Japan, South Korea, and Sweden lead \n\nin terms of broadband speed largely due to their wide deployment of FTTH. \n\nTable 4. \n\nFixed Broadband Speeds (in Mbps), 2014–2019 \n\n2014 \n\n2015 \n\n2016 \n\n2017 \n\n2018 \n\n2019 \n\nCAGR  \n\n(2014-2019)  \n\n 20.3  \n\n 23.2  \n\n 7.2  \n\n 21.8  \n\n 21.8  \n\n 22.2  \n\n 6.1  \n\n 24.7  \n\n 28.1  \n\n 7.6  \n\n 25.4  \n\n 22.8  \n\n 28.3  \n\n 7.0  \n\n 29.2  \n\n 31.1  \n\n 8.5  \n\n 28.7  \n\n 26.8  \n\n 33.4  \n\n 8.5  \n\n 33.6  \n\n 36.3  \n\n 10.5  \n\n 33.7  \n\n 32.5  \n\n 37.7  \n\n 11.1  \n\n 38.1  \n\n 41.5  \n\n 13.1  \n\n 38.7  \n\n 39.7  \n\n 41.8  \n\n 13.0  \n\n 42.5  \n\n 48.9  \n\n 16.9  \n\n 43.7  \n\n 49.1  \n\n 45.3  \n\n 14.9  \n\nConsider how long it takes to download an HD movie at these speeds: at 10 Mbps, it takes 20 minutes; at \n\n25 Mbps, it takes 9 minutes; but at 100 Mbps, it takes only 2 minutes. High-bandwidth speeds will be essential to \n\nsupport consumer cloud storage, making the download of large multimedia files as fast as a transfer from a hard \n\ndrive. Table 5 shows the percentage of broadband connections that will be faster than 10 Mbps, 25 Mbps, and \n\nTable 5. \n\nBroadband Speed Greater Than 10 Mbps, 2014–2019 \n\nGreater Than 10 Mbps \n\nGreater Than 25 Mbps \n\nGreater Than 100 Mbps \n\n2014 \n\n2019 \n\n2014 \n\n2019 \n\n2014 \n\n2019 \n\n48% \n\n46% \n\n27% \n\n58% \n\n51% \n\n53% \n\n16% \n\n68% \n\n73% \n\n33% \n\n74% \n\n62% \n\n76% \n\n20% \n\n29% \n\n26% \n\n9% \n\n33% \n\n28% \n\n34% \n\n6% \n\n33% \n\n37% \n\n12% \n\n45% \n\n37% \n\n41% \n\n8% \n\n3% \n\n3% \n\n1% \n\n2% \n\n4% \n\n2% \n\n0.3% \n\nRegion  \n\nGlobal \n\nAsia Pacific \n\nLatin America \n\nNorth America \n\nWestern Europe \n\nCentral and Eastern Europe \n\nMiddle East and Africa \n\nSource: Cisco VNI, 2015 \n\n \n\n100 Mbps by region. \n\nRegion \n\n              \n\nGlobal \n\nAsia Pacific \n\nLatin America \n\nNorth America \n\nWestern Europe \n\nCentral and Eastern Europe \n\nMiddle East and Africa \n\nSource: Cisco VNI, 2015 \n\n16% \n\n16% \n\n18% \n\n15% \n\n18% \n\n15% \n\n20% \n\n7% \n\n8% \n\n3% \n\n8% \n\n10% \n\n6% \n\n1% \n\n \n\n \n\n \n\n© 2015 Cisco and/or its affiliates. All rights reserved. This document is Cisco Public. \n\nPage 19 of 29 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "There is a strong correlation between experienced speeds and number of video minutes viewed per viewer \n\n(Figure 20). As speeds increase in each country covered in the study, the number of video minutes per viewer \n\nalso increases. \n\nFigure 20.    Increase in Experienced Speeds (Kbps) Increases Internet Video Viewership (Minutes) \n\nSource: Cisco VNI Global IP Traffic Forecast, 2014–2019 \n\n \n\n \n\nMobile Speeds \n\n10.4 Mbps. \n\nGlobally, the average mobile network connection speed in 2014 was 1.7 Mbps. The average speed will double \n\nand will be nearly 4 Mbps by 2019. Smartphone speeds, generally third-generation (3G) and higher, are currently \n\nalmost three times higher than the overall average. Smartphone speeds will nearly double by 2019, reaching \n\nAnecdotal evidence supports the idea that overall use increases when speed increases, although there is often \n\na delay between the increase in speed and the increased use, which can range from a few months to several \n\nyears. The reverse can also be true with the burstiness associated with the adoption of tablets and smartphones, \n\nwhere there is a delay in experiencing the speeds that the devices are capable of supporting. The Cisco VNI \n\nForecast relates application bit rates to the average speeds in each country. Many of the trends in the resulting \n\ntraffic forecast can be seen in the speed forecast, such as the high growth rates for developing countries and \n\nregions relative to more developed areas (Table 6). \n\n \n\n \n\n© 2015 Cisco and/or its affiliates. All rights reserved. This document is Cisco Public. \n\nPage 20 of 29 \n\n\n\n\n", " \n\n \n\n \n\nTable 6. \n\nProjected Average Mobile Network Connection Speeds (in Kbps) by Region and Country \n\n2014 \n\n2015 \n\n2016 \n\n2017 \n\n2018 \n\n2019 \n\nCAGR  \n\n(2014–2019) \n\nGlobal speed: All handsets  \n\nGlobal speed: Smartphones  \n\n 1,683  \n\n 6,097  \n\n 1,747  \n\n 6,899  \n\n 2,017  \n\n 7,686  \n\n 2,629  \n\n 8,468  \n\n 3,248  \n\n 3,963  \n\n 8,829  \n\n 10,403  \n\nGlobal speed: Tablets \n\n 8,697  \n\n 10,203  \n\n 10,907  \n\n 12,119  \n\n 12,403  \n\n 13,054  \n\n 2,026  \n\n 1,378  \n\n 2,816  \n\n 2,037  \n\n 1,620  \n\n 582  \n\n 2,233  \n\n 1,556  \n\n 3,052  \n\n 2,452  \n\n 1,939  \n\n 700  \n\n 2,443  \n\n 1,781  \n\n 3,542  \n\n 2,916  \n\n 2,353  \n\n 742  \n\n 2,730  \n\n 2,077  \n\n 4,299  \n\n 3,408  \n\n 2,762  \n\n 1,095  \n\n 3,047  \n\n 2,463  \n\n 5,196  \n\n 3,910  \n\n 3,167  \n\n 1,577  \n\n 3,509  \n\n 2,949  \n\n 6,399  \n\n 4,687  \n\n 3,671  \n\n 2,097  \n\n19% \n\n11% \n\n8% \n\n12% \n\n16% \n\n18% \n\n18% \n\n18% \n\n29% \n\n            \n\nGlobal \n\nBy Region \n\nAsia Pacific \n\nLatin America \n\nNorth America \n\nWestern Europe \n\nCentral and Eastern Europe \n\nMiddle East and Africa \n\nSource: Cisco VNI Mobile, 2015 \n\nCurrent and historical speeds are based on data from Ookla’s Speedtest. Forward projections for mobile data speeds are based on third-party \n\nforecasts for the relative proportions of 2G, 3G, 3.5G, and 4G among mobile connections through 2019. \n\nA crucial factor promoting the increase in mobile speeds over the forecast period is the increasing proportion \n\nof fourth-generation (4G) mobile connections. The impact of 4G connections on traffic is significant, because \n\n4G connections, which include mobile WiMAX and Long-Term Evolution (LTE), generate a disproportionate \n\namount of mobile data traffic. \n\nWi-Fi Speeds from Mobile Devices \n\nGlobally, Wi-Fi connection speeds originated from dual-mode mobile devices will nearly double by 2019. The \n\naverage Wi-Fi network connection speed (10.6 Mbps in 2014) will exceed 18.5 Mbps in 2019. North America will \n\nexperience the highest Wi-Fi speeds, of 29 Mbps, by 2019 (Table 7). \n\nWi-Fi speeds inherently depend on the quality of the broadband connection to the premises. The speed also \n\ndepends on the Wi-Fi standard in the CPE device. The latest standard, IEEE 802.11ac, is considered a true wired \n\ncomplement and can enable higher-definition video streaming and services that require higher data rates. Also an \n\nimportant factor in the use of Wi-Fi technology is the number and availability of hotspots. \n\nTable 7. \n\nProjected Average Wi-Fi Network Connection Speeds (in Mbps) by Region and Country \n\nRegion \n\nGlobal \n\nAsia Pacific \n\nLatin America \n\nNorth America \n\nWestern Europe \n\nCentral and Eastern Europe \n\nMiddle East and Africa \n\nSource: Cisco VNI, 2015 \n\n2014 \n\n2015 \n\n2016 \n\n2017 \n\n2018 \n\n2019 \n\nCAGR \n\n(2014-2019) \n\n 10.6  \n\n 10.3  \n\n 5.8  \n\n 14.3  \n\n 13.0  \n\n 11.7  \n\n 3.9  \n\n 12.5  \n\n 11.4  \n\n 5.9  \n\n 17.4  \n\n 13.9  \n\n 13.4  \n\n 4.0  \n\n 13.9  \n\n 12.4  \n\n 6.7  \n\n 20.3  \n\n 15.9  \n\n 15.7  \n\n 4.2  \n\n 15.4  \n\n 13.5  \n\n 7.3  \n\n 23.2  \n\n 17.5  \n\n 17.8  \n\n 4.3  \n\n 17.0  \n\n 14.5  \n\n 7.9  \n\n 26.1  \n\n 19.2  \n\n 19.9  \n\n 4.5  \n\n 18.5  \n\n 15.6  \n\n 8.6  \n\n 29.0  \n\n 20.9  \n\n 22.0  \n\n 4.7  \n\n12% \n\n9% \n\n8% \n\n15% \n\n10% \n\n14% \n\n3% \n\n© 2015 Cisco and/or its affiliates. All rights reserved. This document is Cisco Public. \n\nPage 21 of 29 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", " \n\n \n\nTrend 8: Mobility (Wi-Fi) Continues to Gain Momentum \n\nGlobally, there will be nearly 341 million public Wi-Fi hotspots by 2018, up from 48 million hotspots in 2014, a \n\nsevenfold increase, according to a study conducted by iPass Inc. and Maravedis and Rethink. Europe will have \n\nthe greatest number of hotspots, with 115 million hotspots by 2018, with North America a close second, with \n\n109 million hotspots. Public Wi-Fi along with community hotspots are taken into account as well. Community \n\nhotspots or homespots are just emerging as a potentially significant element of the public Wi-Fi landscape. In this \n\nmodel, subscribers allow part of the capacity of their residential gateway to be open to casual use. The homespot \n\nmay be provided by a broadband or other provider directly or through a partner. \n\nCritical enablers of Hotspot 2.0 adoption are higher-speed Wi-Fi gateways and the adoption of the IEEE 802.11ac \n\nand 802.11n standards. Globally, the prevalence of IEEE 802.11ac, the latest Wi-Fi standard, will gain momentum \n\nfrom 2014 through 2019. Globally, shipments of home Wi-Fi routers with IEEE 802.11ac will grow 20-fold from \n\n2014 through 2019, and routers with IEEE 802.11n (CY2014 was the last year of increase in shipments of IEEE \n\n802.11n BB CPE) will continue to be replaced by IEEE 802.11ac even beyond the forecast period. \n\nIEEE 802.11n, which was ratified in 2007, provides a range of speeds that allow users to view medium-resolution \n\nvideo streaming due to higher throughput. The latest standard, IEEE 802.11ac, with very high theoretical speeds, is \n\nconsidered a true wired complement and can enable higher-definition video streaming and services with use cases \n\nthat require higher data rates (Figure 21). \n\nFigure 21.    Future of Wi-Fi as Wired Complement \n\nThe rapid growth of mobile data traffic has been widely recognized and reported. The trend toward mobility carries \n\nover into the realm of fixed networks as well, in that an increasing portion of traffic will originate from portable or \n\nmobile devices. Figure 22 shows the growth in Wi-Fi and mobile traffic in relation to traffic from wired devices. \n\nBy 2019, wired networks will account for 33 percent of IP traffic, and Wi-Fi and mobile networks will account \n\nfor 67 percent of IP traffic. In 2014, wired networks accounted for the majority of IP traffic, at 54 percent; Wi-Fi \n\naccounted for 42 percent; and mobile or cellular networks accounted for 4 percent of total global IP traffic. \n\n \n\n© 2015 Cisco and/or its affiliates. All rights reserved. This document is Cisco Public. \n\nPage 22 of 29 \n\n\n\n\n", " \n\n \n\n \n\n \n\nFigure 22.    Global IP Traffic, Wired and Wireless \n\nSource: Cisco VNI Global IP Traffic Forecast, 2014–2019 \n\nNarrowing the focus to Internet traffic and excluding managed IP traffic yields a more pronounced trend. By 2019, \n\nwired devices will account for 19 percent of Internet traffic, and Wi-Fi and mobile devices will account for \n\n81 percent of Internet traffic (Figure 23). In 2014, wired devices accounted for less than half of Internet traffic, \n\nat 39 percent. \n\nFigure 23.    Global Internet Traffic, Wired and Wireless \n\n \n\n \n\nSource: Cisco VNI Global IP Traffic Forecast, 2014–2019 \n\n© 2015 Cisco and/or its affiliates. All rights reserved. This document is Cisco Public. \n\nPage 23 of 29 \n\n\n\n\n\n", "Trend 9: Traffic Pattern Analysis (Peak Compared to Average and CDN Compared to Metro) \n\nWhile average Internet traffic has settled into a steady growth pattern, busy-hour traffic (or traffic in the busiest \n\n60-minute period of the day) continues to grow more rapidly than average traffic. Service providers plan network \n\ncapacity according to peak rates, rather than average rates. In 2014, busy-hour Internet traffic grew 37 percent, \n\nand average traffic grew at 29 percent.. Between 2014 and 2019, global busy-hour Internet use will grow at a \n\nCAGR of 31 percent, compared with 26 percent for average Internet traffic (Figure 24). \n\nVideo is the underlying reason for accelerated busy-hour traffic growth. Unlike other forms of traffic, which are \n\nspread evenly throughout the day (such as web browsing and file sharing), video tends to have a “prime time.” \n\nBecause of video consumption patterns, the Internet now has a much busier busy hour. Because video has a \n\nhigher peak-to-average ratio than data or file sharing, and because video is gaining traffic share, peak Internet \n\ntraffic will grow faster than average traffic. The growing gap between peak and average traffic is amplified further \n\nby the changing composition of Internet video. Real-time video such as live video, ambient video, and video calling \n\nhas a peak-to-average ratio that is higher than on-demand video. \n\nFigure 24.    Busy-Hour Compared with Average Internet Traffic Growth \n\nSource: Cisco VNI Global IP Traffic Forecast, 2014–2019 \n\n \n\nMetro-only traffic (traffic that traverses only the metro network and bypasses long-haul traffic links) surpassed long-\n\nhaul traffic in 2014, and will account for 66 percent of total IP traffic by 2019. Metro-only traffic will grow twice \n\nas fast as long-haul traffic from 2014 to 2019. Long-haul traffic is also deposited onto metro networks so that total \n\nmetro traffic already exceeds long-haul traffic. In 2014, total metro traffic was 2.1 times greater than long-haul \n\ntraffic, and by 2019, metro traffic will be 3.1 times greater than long-haul traffic (Figure 25). \n\n \n\n \n\n \n\n© 2015 Cisco and/or its affiliates. All rights reserved. This document is Cisco Public. \n\nPage 24 of 29 \n\n\n\n\n", " \n\n \n\n \n\n \n\nFigure 25.    Global Metro Traffic Compared to Long-Haul Traffic, 2014 and 2019 \n\nSource: Cisco VNI Global IP Traffic Forecast, 2014–2019 \n\nThe faster growth of metro traffic compared with long-haul is due in part to CDNs, which will carry 62 percent of \n\ntotal Internet traffic by 2019 (Figure 26). Although network performance is usually attributed to the speeds and \n\nlatencies offered by the service provider, the delivery algorithms used by CDNs have an equal if not more \n\nsignificant bearing on video quality. \n\nFigure 26.    Global Content Delivery Network Internet Traffic, 2014 and 2019 \n\n \n\n \n\nSource: Cisco VNI Global IP Traffic Forecast, 2014–2019 \n\n© 2015 Cisco and/or its affiliates. All rights reserved. This document is Cisco Public. \n\nPage 25 of 29 \n\n\n\n\n\n", " \n\n \n\n \n\nTrend 10: Network Performance Promotes User Behaviors (Data Plans and Caps) \n\nSpeed is a critical factor in Internet traffic. When speed increases, users stream and download greater volumes of \n\ncontent, and adaptive bit-rate streaming increases bit rates automatically according to available bandwidth. Service \n\nproviders find that users with greater bandwidth generate more traffic. In 2014, households with high-speed fiber \n\nconnectivity generated 77 percent more traffic than households connected by DSL or cable broadband, globally \n\n(Figure 27). The average FTTH household generated 61 GB per month in 2014 and will generate 120 GB per \n\nmonth in 2019. \n\nFigure 27.    Fiber-Connected Households Generate More Traffic Than Household with Other Broadband \n\nSource: Cisco VNI Global IP Traffic Forecast, 2014–2019 \n\n \n\nTo limit the volume of traffic, service providers can institute use-based tiered pricing and data caps. \n\nOn mobile networks, by looking at the use of more than 33,000 lines from Tier 1 mobile operators from 2010 \n\nto 2014, we found that monthly traffic from the top 1 percent of users is down to 18 percent of overall use \n\ncompared to 52 percent in 2010, showing the effects of tiered pricing. With mobile penetration reaching a \n\nsaturation point in many countries across all regions, the trend has been toward tiered plans as a way to monetize \n\ndata and effectively manage or throttle the top users of traffic. On the fixed networks, data caps continue to \n\nincrease to match subscribers’ growing appetite for video. In the United States, Tier 1 carriers are considering \n\n500 GB as a possible monthly limit by the 2019 time frame, from a variety of offerings today. A large provider in \n\nJapan has a 30-GB per day upload cap. In several countries, Netflix has a sizeable percentage of the Internet \n\nvideo minutes and traffic. Wildcard traffic generators such as Twitch.TV, a live streaming service in which video \n\ngamers watch each other play, has established itself on many fixed networks around the world. \n\nData caps affect a larger percentage of mobile users than fixed users. With Tier 1 carriers, approximately 8 percent \n\nof mobile users consume more than 2 GB per month (a common mobile data cap), whereas only 1 percent of fixed \n\nusers consume more than 500 GB per month (a common fixed data cap). \n\n© 2015 Cisco and/or its affiliates. All rights reserved. This document is Cisco Public. \n\nPage 26 of 29 \n\n\n\n\n", " \n\nOther Trends to Watch \n\nCisco’s approach to forecasting IP traffic is conservative, and certain emerging trends have the potential to \n\nincrease the traffic outlook significantly. The most rapid upswings in traffic occur when consumer media \n\nconsumption migrates from offline to online or from broadcast to unicast: \n\n●  Applications that might migrate from offline to online (cloud): The crucial application to watch in this \n\ncategory is gaming. Gaming on demand and streaming gaming platforms have been in development for \n\nseveral years, with many newly released in 2013 or 2014. With traditional gaming, graphical processing \n\nis performed locally on the gamer’s computer or console. With cloud gaming, game graphics are produced \n\non a remote server and transmitted over the network to the gamer. Currently, online gaming traffic \n\nrepresents only 0.04 percent of the total information content associated with online and offline game play1. \n\nIf cloud gaming takes hold, gaming could quickly become one of the largest Internet traffic categories. \n\n●  Behavior that might migrate from broadcast to unicast: Live TV is currently distributed by means of \n\na broadcast network, which is highly efficient in that it carries one stream to many viewers. Live TV over \n\nthe Internet would carry a separate stream for each viewer. AT&T in the past estimated that a shift from \n\nmulticast or broadcast to over-the-top unicast “would multiply the IP backbone traffic by more than an \n\norder of magnitude”.2 \n\n●  New consumer behavior: The adoption of UHD TV would fall into the category of new consumer behavior. \n\nUHD is already growing in terms of supporting devices, and content video providers are preparing to \n\nbroadcast and stream UHD. Higher resolution and network requirements to stream UHD will create traffic \n\nmultiplier effects. This nascent traffic type can cause surprises that have network design implications. \n\nAnother high-bandwidth application on the horizon that could consume a lot of bandwidth is spherical video. \n\nSpherical, or immersive, video integrates multiple camera angles to form a single video stream and can be \n\nwatched from the viewer’s preferred perspective. It can generate bit rates 3 to 10 times greater than \n\nnonimmersive HD bit rates. \n\nFor More Information \n\nFor more information about Cisco’s IP traffic forecast, refer to “Cisco VNI: Forecast and Methodology, 2014–2019” \n\nand visit the other resources and updates at www.cisco.com/go/vni. Several interactive tools allow you to \n\ncreate custom highlights and forecast charts by region, by country, by application, and by end-user segment. \n\nRefer to the Cisco VNI Highlights tool and the Cisco VNI Forecast Widget tool. Inquiries can be directed \n\nto traffic-inquiries@cisco.com. \n\n                                                 \n\n1 Total game play (online and offline) in the United States represents an estimated 166 exabytes per month, according \n\nto the University of California, San Diego, study, “How Much Information?” \n\n2 Alexandre Gerber and Robert Doverspike, “Traffic Types and Growth in Backbone Networks.” \n\n \n\n \n\n© 2015 Cisco and/or its affiliates. All rights reserved. This document is Cisco Public. \n\nPage 27 of 29 \n\n\n\n\n\n\n\n\n\n\n", "Appendix A: Cisco Global IP Traffic Forecast \n\nTable 8 shows a summary of Cisco’s global IP traffic forecast. For more information and additional tables, refer \n\nto ”Cisco VNI: Forecast and Methodology, 2014–2019.” \n\nTable 8. \n\nGlobal IP Traffic, 2014–2019 \n\nIP Traffic, 2014–2019 \n\n             \n\n2014 \n\n2015 \n\n2016 \n\n2017 \n\n2018 \n\n2019 \n\nCAGR \n\n(2014–2019) \n\nBy Type (Petabytes [PB] per Month) \n\nFixed Internet \n\nManaged IP  \n\nMobile data  \n\nConsumer \n\nBusiness \n\nAsia Pacific \n\nNorth America \n\nWestern Europe \n\nBy Segment (PB per Month) \n\nBy Geography (PB per Month) \n\n 39,909  \n\n 47,803  \n\n 58,304  \n\n 72,251  \n\n 90,085  \n\n 111,899  \n\n17,424  \n\n20,460  \n\n23,371  \n\n26,087  \n\n29,274  \n\n31,858  \n\n2,514  \n\n4,163  \n\n6,751  \n\n10,650  \n\n16,124  \n\n24,221  \n\n 47,740  \n\n 58,137  \n\n 71,453  \n\n 88,730  \n\n 111,015  \n\n 138,415  \n\n12,108  \n\n14,289  \n\n16,973  \n\n20,258  \n\n24,469  \n\n29,563  \n\n20,729 \n\n24,819 \n\n29,965 \n\n 36,608  \n\n 44,223  \n\n 54,434  \n\n 19,628  \n\n 23,552  \n\n 28,219  \n\n 33,641  \n\n 41,458  \n\n 49,720  \n\n 9,601  \n\n 11,231  \n\n 13,506  \n\n 16,396  \n\n 20,046  \n\n 24,680  \n\nCentral and Eastern Europe \n\n 4,087  \n\n 5,270  \n\n 6,896  \n\n 9,385  \n\n 12,601  \n\n 16,863  \n\n4,297 \n\n1,505 \n\n5,373 \n\n6,663 \n\n8,299 \n\n10,356 \n\n12,870 \n\n 2,180  \n\n 3,178  \n\n 4,659  \n\n 6,800  \n\n 9,412  \n\n23% \n\n13% \n\n57% \n\n24% \n\n20% \n\n21% \n\n20% \n\n21% \n\n33% \n\n25% \n\n44% \n\nTotal IP traffic \n\n 59,848  \n\n 72,426  \n\n 88,427  \n\n 108,988  \n\n 135,484  \n\n 167,978  \n\n23% \n\nLatin America \n\nMiddle East and Africa \n\nTotal (PB per Month) \n\nSource: Cisco VNI, 2015 \n\nDefinitions \n\n●  Consumer: Includes fixed IP traffic generated by households, university populations, and Internet cafés \n\n●  Business: Includes fixed IP WAN or Internet traffic, excluding backup traffic, generated by businesses \n\nand governments \n\n●  Mobile: Includes Internet traffic that travels over 2G, 3G, or 4G mobile access technology \n\n●  Internet: Denotes all IP traffic that crosses an Internet backbone \n\n●  Non-Internet IP: Includes corporate IP WAN traffic, IP transport of TV and VoD, and mobile \n\n“walled-garden” traffic \n\n \n\n \n\n \n\n \n\n© 2015 Cisco and/or its affiliates. All rights reserved. This document is Cisco Public. \n\nPage 28 of 29 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", " \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nPrinted in USA \n\n \n\n© 2015 Cisco and/or its affiliates. All rights reserved. This document is Cisco Public. \n\nFLGD 12352  05/15 \n\nPage 29 of 29 \n\n\n\n\n"]], ["C:\\Users\\mjols\\Documents\\DS UWEC courses\\730 Big Data\\lesson5 pig\\ds730_lesson5_pig_latin_basics.pdf", [" \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nImportant note: Transcripts are not substitutes for textbook assignments. \n\n1 \n\n\n\n\n", " \n\nNotes: \n\nPig data relationships are similar to a table in a relational database but don’t require all \n\ntuple to contain the same number of fields or all fields have the same type. \n\n \n\n \n\n \n\n \n\n2 \n\n\n", "There are several data types built into Pig that you can use when you're defining the \n\nschema of your relation. Most of these are ones that you're already familiar with. An \n\nint, a double, A boolean, and so on. A tuple is your normal row that you would find in \n\na database. A bag is simply a collection of tuples, whereas a map is just a key value \n\npair similar to the HashMaps that you've seen before. \n\n \n\nNotes:  \n\nPig Data Types and More – \n\nhttp://pig.apache.org/docs/r0.17.0/basic.html#Data+Types+and+More \n\n \n\n \n\n \n\n \n\n \n\n3 \n\n\n\n", " \n\nAll of the arithmetic operations that you know are available to you. Addition, \n\nsubtraction, division, and so on. There are a few others that you might be familiar with \n\nbut they are explained here just in case you were not familiar with them. The modulo \n\noperator returns the remainder after the division. So, for example, 4 module 2 would be \n\n0, because 2 evenly divides 4 and the \n\nremainder is 0. Whereas 15 module 6 is 3, because 15 divided by 6 is 2 with a \n\nremainder of 3. \n\n \n\nA biconditional is also called a ternary operator. It is essentially an if statement without \n\nthe if part. In this particular example, what it is saying is that if x is equal to 5 then set \n\nthe value of y to 10, otherwise set the value of y to 20. \n\n \n\nNotes: \n\nArithmetic Operators and More – \n\nhttp://pig.apache.org/docs/r0.17.0/basic.html#artichmetic-ops \n\n \n\n \n\n \n\n \n\n4 \n\n\n\n", " \n\nNotes: \n\nRelational Operators – \n\nhttp://pig.apache.org/docs/r0.17.0/basic.html#Relational+Operators \n\n \n\n \n\n \n\n \n\n5 \n\n\n\n", "There are a ton of built-in functions provided for you. And a few of the more common \n\nones are listed here. I encourage you to check out the Pig documentation website to see \n\nall of the functions that are available for you to use. You don't want to reinvent the \n\nwheel by writing your own lowercase function or MIN function if it has already been \n\ndone for you. \n\n \n\nNotes: \n\nBuilt-In Functions – \n\nhttp://pig.apache.org/docs/r0.17.0/func.html \n\n \n\n \n\n \n\n \n\n \n\n6 \n\n\n\n", "As alluded to on the previous slide, if there is a function that you want to use that isn't \n\nalready built into Pig you can create your own. We will see a couple of short examples \n\non how to create a user defined function in Python. \n\n \n\nNotes: \n\nUser Defined Functions – \n\nhttp://pig.apache.org/docs/r0.17.0/basic.html#udf-statements \n\n \n\n \n\n \n\n \n\n \n\n7 \n\n\n\n", " \n\nThis is a typical user-defined function file. You would generally create your functions in \n\none file and then simply load them up into Pig. We'll see exactly how to do this in this \n\nweek's activity. The first line of your Python file is just a line of code that you need in \n\norder to import to the Pig utilities that are needed for these functions. \n\n \n\nThe first function is a very simple function that just returns the square of a number. It \n\nshould be pretty self-explanatory on how it works. The only line that might be strange is \n\nthe output schema line. The output schema is there to tell Pig what type of thing this \n\nfunction will be returning, which is the int that you see at the end. And it's also giving a \n\nname to the thing that's being returned, which here is just called the generic output field \n\nname. \n\n \n\nThe second function is also fairly self-explanatory. It just duplicate a word and the type \n\nthat it returns is a string. \n\n \n\n \n\n \n\n \n\n8 \n\n\n", " \n\n \n\n9 \n\n\n"]], ["C:\\Users\\mjols\\Documents\\DS UWEC courses\\730 Big Data\\lesson5 pig\\ds730_lesson5_pig_programming_intro.pdf", [" \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nImportant note: Transcripts are not substitutes for textbook assignments. \n\n1 \n\n\n\n\n", "As I said, Pig is a tool used to analyze a ton of data. Instead of having to write Python \n\nmappers and reducers, or even writing MapReduce code in Java, Pig abstracts all of that \n\naway, and allows you to write code that is much easier to read and write. Pig will then \n\ntake the scripts that you write and convert them automatically into MapReduce jobs. \n\n \n\n \n\n \n\n \n\n \n\n2 \n\n\n", "Pig comes with something called the grunt shell. It is very similar to what Python offers. \n\nYou can load up Python and start writing Python code, and you can do the same thing \n\nwith the grunt shell with Pig. The best way to use Pig, however, is to create Pig scripts, \n\nand then have Pig run them, just as you do with Python and Python scripts. \n\n \n\n \n\n \n\n \n\n \n\n3 \n\n\n", "Similar to what we've been doing when we're learning the syntax of other languages, \n\nwhat I'm going to do is go over some of the key syntax of Pig Latin, and then have you \n\npick up the rest on your own. I certainly don't want to bore you with all the tedious \n\ndetails on how to do every little thing, nor do I want to just say, good luck learning it all \n\non your own. So hopefully this presentation, and the accompanying activity, are just the \n\nright amount. \n\n \n\nNotes: \n\nFull Pig Documentation – \n\nhttp://pig.apache.org/docs/r0.17.0/basic.html \n\n \n\n \n\n \n\n \n\n \n\n4 \n\n\n\n", " \n\nNotes: \n\nPig Latin Case Sensitivity – \n\nhttp://pig.apache.org/docs/r0.17.0/basic.html#case-sensitivity \n\n \n\n \n\n \n\n \n\n5 \n\n\n\n", " \n\nAlmost every Pig Latin statement will take a relation as input, and produce a relation as \n\noutput. You can think of a relation as a table in some database. In this particular example, \n\nlet's assume that some relation is a relation that has a bunch of rows, and each of those \n\nrows has columns. And what you want to do is you want to limit it to just five rows. \n\n \n\nThe right-hand side will take that some relation, and limit it to just five rows. But it \n\ndoesn't change some relation. Some relation is the same relation it was before this \n\nstatement executes. \n\n \n\nIn order to actually save that new five-row relation, you have to assign it to a different \n\nvariable. And in this case, we have the filter variable on the left-hand side. Filter is now \n\na relation, with just five rows. \n\n \n\n \n\n \n\n \n\n6 \n\n\n", " \n\nA Pig script almost always has the following template-- instead of me going over a \n\nbunch of examples in this presentation, most of the learning will actually happen \n\nduring the activity, where you'll be writing a script, and I'll be explaining what's going \n\non during each step. However, this is a good starting point on how Pig scripts work. \n\n \n\nIn general, the first thing that you're going to do is load up the data from either a file on \n\nthe local machine, or from HDFS. In this particular example, we have a comma separated \n\nfile of stock prices. We use the \"as\" keyword to specify how the relation is going to be \n\ndescribed. In this example, the first column of every row is the exchange string, or a char \n\narray, the second column is the stock symbol, the third column is the date, and so on. \n\n \n\nThe next statement is a describe statement, and it does exactly that-- it describes the \n\ndata. In the database world, or in SQL terms, what we're doing is we're displaying the \n\nschema of our relation. The next few lines are ways that we're going to transform and \n\nwork with the data. And essentially what we're doing is we're writing SQL-like \n\ncommands to answer some question. \n\n \n\nThe first one has this limit the number of rows to 100-- maybe our relation is huge, and \n\nwe only want 100 rows. And again, the stock A relation doesn't change with this \n\nstatement, so what we need to do is take that newly created 100-row relation and \n\nstore that into another variable, which we call B. \n\n \n\nThe next statement is a simple way, than, to filter out the columns that we don't want. \n\nIt is saying, for every row in relation B, generate another row, but only give me the \n\nsymbol, the date, and the close column. Those names are \n\n \n\n \n\n7 \n\n\n", "from the \"as\" part that we got in the first statement. \n\n \n\nThe final statement, then, that you're going to have with your Pig scripts will usually be \n\na dump or a store command, and that will actually save the relation into a file. Dump \n\nand store commands are customizable, so you can save the relation in any format that \n\nyou like. \n\n \n\n \n\n \n\n8 \n\n", " \n\nPig Latin has a handful of data types that you need to know about. A field is just like a \n\nnormal field in a regular database. It's just one piece of information, whether it's an int, \n\nor a char array, or whatever. \n\n \n\nA tuple is like a tuple in a normal database, it's just an ordered set of fields. A bag is a \n\ncollection of tuples. Now, it might seem a bit strange that you can have a tuple be the \n\ncolumn of another tuple, but this is possible. \n\n \n\nFor instance, a tuple could consist of a char array-- an integer-- and maybe another \n\nfield, which is itself a tuple, and that tuple could have maybe three different integers. \n\nThe outermost tuple is what we call a relation. \n\n \n\nThe biggest takeaway is that if you're familiar with a normal RDBMS-- a relational \n\ndatabase-- then a Pig relationship isn't going to be that foreign to you. Pig offers a bit \n\nmore flexibility in that the tuples don't have to contain the same number of fields, and \n\nthe fields don't have to be the same type. \n\n \n\n \n\n9 \n\n\n", " \n\n \n\n10 \n\n\n"]], ["C:\\Users\\mjols\\Documents\\DS UWEC courses\\730 Big Data\\project1\\PROJECT01_MapReduce.pdf", ["Working with MapReduce\n\nDS730\n\nIn this project, you will be working with input, output, Python and Hadoop framework.\n\nYou will be writing multiple mappers and reducers to solve a few different problems.\n\nIf you recall, the map and reduce functions are stateless and this is especially\n\nimportant when dealing with Hadoop and distributed work. We can’t guarantee that any\n\n1 mapper will read in all of the data. Nor can we guarantee that certain inputs will end\n\nup on the same machine for mapping. Rather, 1 mapper will likely read in a small\n\nportion of the data. The output that your mapper produces must only depend on the\n\ncurrent input value. For the reducer, you can only guarantee that (key,value) pairs with\n\nthe same key will end up on the same reducer.\n\nYour mapper and reducer cannot be trivial. For example, do not have all of your\n\nmappers map use the same key and then solve everything in the reducer. Such a\n\nsolution defeats the purpose of MapReduce because all (key,value) pairs will end up on\n\nthe same reducer. If you are unsure if your keys are trivial, post a private message to\n\nthe message board for the instructors and we will let you know if your keys are trivial. A\n\ncouple of very important things:\n\n1. Make sure your key is separated by your value using a tab. Hadoop will\n\nonly work if this is the case. Otherwise, Hadoop has no idea what your\n\n“key” is nor will it know what your “value” is.\n\n2. Make sure that this is the first line of your mapper and reducer:\n\n#!/usr/bin/env python\n\nYou must write 1 mapper file and 1 reducer file to solve each of the following problems.\n\nMake sure you name your files mapperX.py and reducerX.py where X is the problem\n\nnumber. For each problem, Hadoop will define what your input files are so there is no\n\nneed to read in from any file. Simply read in from the command line. You are\n\nencouraged to use the “starter” mapper and reducer as shown in the activity.\n\nProblem 1: Finding Big Spenders (33 points)\n\nAssume you work for a large business and have access to all orders made in any given\n\ntime period (download the orders.csv file from\n\n\n", "1\n\nhttp://faculty.cs.uwosh.edu/faculty/krohn/ds730/orders.csv) . When we test your code,\n\nthe input file will have an identical format to the orders.csv file . The column headings\n\nare fairly self-explanatory. Your company wants you to find the big spenders for each\n\nmonth and country so they can market more heavily to them. Your goal is this: for each\n\nmonth/country combination, display the customerID of the top spender (i.e. the sum of\n\nhow much that customer spent) for that month/country combination. The amount spent\n\nin each row is determined by multiplying the Quantity by the UnitPrice. A few caveats:\n\n2\n\na. The InvoiceDate is in Month/Day/Year format.\n\nb. An InvoiceNo that starts with a C is a return. You must ignore these rows.\n\nc. A row may not have a CustomerID. These rows must be ignored.\n\nd.\n\nI am not looking for month/year/country combinations here. I am only\n\ninterested in the month and country.\n\nThis final output file should contain the following data in this format:\n\nMonth,Country:CustomerID\n\nThe month must be a two-digit number (i.e., 01, 02, ..., 09, 10, 11, 12) and must be\n\nseparated from the Country using a comma. The Month,Country portion is separated by\n\nthe CustomerID using a colon. If there is a tie, you must print out all customers who tied\n\nseparating the CustomerID’s by a comma with the CustomerId’s being in ascending\n\norder.\n\nProblem 2: Words with exact same vowels (33 points)\n\nThis problem is similar to the problem we worked on in lecture with a small twist. Instead of\n\nprinting out how many times a word appears in the file, you want to print out how many words\n\nhave the exact same type of vowels. For this problem, only the number of vowels matters and\n\nthe case does not matter (i.e cat is the same as CAt). A vowel is any letter from this set\n\n{a,e,i,o,u,y}. A word is any sequence of characters that does not contain whitespace.\n\nWhitespace is defined as: space, newline or tab. All of the following are 1 single word:\n\ncats\n\nc@ts\n\nca7s\n\ncat’s.and:d0gs!\n\n1 Data set used: Daqing Chen, Sai Liang Sain, and Kun Guo, Data mining for the online retail industry: A\n\ncase study of RFM model-based customer segmentation using data mining, Journal of Database\n\nMarketing and Customer Strategy Management, Vol. 19, No. 3, pp. 197-208, 2012 (Published online\n\nbefore print: 27 August 2012. doi: 10.1057/dbm.2012.17)\n\n2 In other words, there will be a header row and there will be any number of rows of actual data.\n\n\n\n\n", "The output will be the vowel set, followed by a colon, followed by the number of words that\n\ncontained exactly the vowel set. The output will have one answer per line (see example below).\n\n‘hEllo’ and ‘pOle’ both contain exactly 1 e and exactly 1 o. The order of the vowels and the\n\ncase from the original input word does not matter.\n\nImagine the following example:\n\nhEllo    moose\n\npOle cccttt.ggg\n\nWe would end up with the following output:\n\nExample:\n\n:1\n\neo:2\n\neoo:1\n\nThe format should be as seen above: the vowels on each line are in alphabetical order, followed\n\nby a colon, then followed by the number of words that contained exactly those vowels. If there\n\nare words with no vowels, nothing is printed before the colon.\n\nProblem 3: Discovering Contacts (44 points)\n\nOn many social media websites, it is common for the company to provide a list of\n\nsuggested contacts for you to connect with. Many of these suggestions come from your\n\nown list of current contacts. The basic idea behind this concept being: I am connected\n\nwith person A and person B but not person C. Person A and person B are both\n\nconnected to person C. None of my contacts are connected to person D. It is more likely\n\nthat I know person C than some other random person D who is connected to no one I\n\nknow. For this problem, all connections are mutual (i.e. if A is connected to B, then B is\n\nconnected to A). In this problem, you will read in an input file that is delimited in the\n\nfollowing manner:\n\nPersonA : PersonX PersonY PersonZ PersonQ\n\nPersonB : PersonF PersonY PersonX PersonG PersonM\n\n…\n\nFor example, the person to the left of the colon will be the current person. All people to\n\nthe right of the colon are the people that the current person is connected to. All people\n\nwill be separated by a single space. In the example above, PersonA is connected to\n\n\n\n\n\n\n", "PersonX, Y, Z and Q. In all inputs, all people will be replaced with positive integer ids to\n\nkeep things simple. The following is a sample input file:\n\n6 : 2 9 8 10\n\n1 : 3 5 8 9 10 12\n\n4 : 2 5 7 8 9\n\n2 : 3 4 7 6 13\n\n12 : 1 7 5 9\n\n3 : 9 11 10 1 2 13\n\n10 : 1 3 6 11\n\n5 : 4 1 7 11 12\n\n13 : 2 3\n\n8 : 1 6 4 11\n\n7 : 5 2 4 9 12\n\n11 : 3 5 10 8\n\n9 : 12 1 3 6 4 7\n\nThe ordering of people on the right hand side of the input can be in any order. Your goal\n\nis this: you must output potential contacts based on the following 2 criteria:\n\n1. Someone who might be someone you know. For someone to be suggested\n\nhere, the person must not currently be a connection of yours and that person\n\nmust be a connection of exactly 2 or 3 of your current connections. For example,\n\nconsider person 2 in the above example. Person 2 is connected with 3, 4, 6, 7\n\nand 13. Person 4 is connected to 8, person 6 is connected to 8, person 3 is not\n\nconnected to 8, person 7 is not connected to 8 and person 13 is not connected to\n\n8. Therefore, person 2 has two connections (4 and 6) that are connected to 8 and\n\nperson 2 is not currently connected to 8. Therefore, person 2 might know person\n\n8.\n\n2. Someone you probably know. For someone to be suggested here, the person\n\nmust not currently be a connection of yours and that person must be a\n\nconnection of 4 or more of your current connections. For example, consider\n\nperson 2 in the above example. Person 2 is connected with 3, 4, 6, 7 and 13.\n\nPerson 4 is connected to 9, person 6 is connected to 9, person 3 is connected to\n\n9 and person 7 is connected to 9. Therefore, person 2 has at least four\n\nconnections that are connected to 9 and person 2 is not currently connected to 9.\n\nTherefore, person 2 probably knows person 9.\n\n\n", "Your output must be formatted in the following fashion:\n\npersonID:Might(personA,…,personX) Probably(personA,…personX)\n\nFor each line you have a personID following by a colon. The colon is followed by the list\n\nof Might’s separated by commas (but no space). If a person has no one they might be\n\nconnected to, this list is not printed at all (see person 13 below for example). The Might\n\nlist is followed by a single space and then followed by the Probably list separated by\n\ncommas (but no space). If a person has no one they probably are connected to, this list\n\nis not printed at all (see person 3 for example). If a person has neither a might list nor a\n\nprobably list, that person only has their id along with a colon (see person 13 for\n\nexample). The Might list must appear before the Probably list. If there is no Might list but\n\nthere is a Probably list, there is no space between the colon and the Probably list. The\n\nintegers within each list must appear in increasing order. However, the order the rows\n\nappear in the output need not be in any specific order. For example, the row for 5 might\n\nappear before the row for 3. As a concrete example from the above sample input, this\n\nwould be a potential sample output:\n\n1:Might(4,6,7) Probably(11)\n\n2:Might(5,8,10) Probably(9)\n\n3:Might(4,5,6,7,8,12)\n\n4:Might(1,3,6,11,12)\n\n5:Might(2,3,8,10) Probably(9)\n\n6:Might(1,3,4,7,11)\n\n7:Might(1,3,6)\n\n8:Might(2,3,5,9,10)\n\n9:Might(8,10) Probably(2,5)\n\n10:Might(2,5,8,9)\n\n11:Might(4,6) Probably(1)\n\n12:Might(3,4)\n\n13:\n\n9:Might(8,10) Probably(2,5)\n\n1:Might(4,6,7) Probably(11)\n\n3:Might(4,5,6,7,8,12)\n\n4:Might(1,3,6,11,12)\n\nFor each question, the rows do not have to be in any specific order. The following is\n\nalso a valid output for number 3:\n\n\n", "5:Might(2,3,8,10) Probably(9)\n\n11:Might(4,6) Probably(1)\n\n6:Might(1,3,4,7,11)\n\n7:Might(1,3,6)\n\n2:Might(5,8,10) Probably(9)\n\n8:Might(2,3,5,9,10)\n\n10:Might(2,5,8,9)\n\n12:Might(3,4)\n\n13:\n\nOther Important Information\n\n1. You can use any Python libraries as long as they are installed by default on the\n\nHortonworks machine and your code works on Hadoop. All projects will be\n\ntested using Hadoop on Hortonworks. You should ensure that your code\n\nexecutes correctly on that platform before submitting.\n\n2. Although not a coding question per se, asking publicly about (key,value) pairs or\n\nwhat keys one should use is a major question that should be done privately. If\n\nyou know what the (key,value) pair is, then it's really just a programming task of\n\nimplementing MapReduce correctly. Coming up with the (key,value) pair is a crux\n\nof solving these problems. Therefore, if you have a question about whether or not\n\nyour key is good, please make it a private question.\n\n3. When you are working with Hadoop and other software at your job, you will likely\n\nbe given some large dataset and have to work with it. This is the case with\n\nnumber 1. However, you may not be given a set of sample input and output files\n\nto test your code. Therefore, one of the objectives for this project is being able to\n\ntest your solution without having a sample input/output file. Coming up with your\n\nown sample input and output based on the input description (and the specified\n\noutput) is a key skill to have. These samples can be completely random or you\n\ncan generate them using some real world data. You can expand some of the\n\nexamples given in the questions and manually check if your answer is correct.\n\nYou can also make smaller input files for problem 1 and then manually check if\n\nyour answer is correct. If it is correct for several small cases, then you should feel\n\ngood about your answer being correct for a large case that you can’t manually\n\ncheck.\n\n\n", "4. The order of the rows in the output does not matter. This is shown in the last part\n\nof problem 3. For problem 2, for example, the :1 row could come after the eo:2\n\nrow.\n\nWhat to Submit\n\nWhen you are finished testing your code, zip up your mappers and reducers for all\n\nproblems into a zipped file called p1.zip and upload only this zip file to the Project 1\n\ndropbox.\n\n\n"]], ["C:\\Users\\mjols\\Python39\\my_first_jupyter_notebook.pdf", ["(cid:87)(cid:396)(cid:349)(cid:374)(cid:410)(cid:381)(cid:437)(cid:410)\n\n(cid:38)(cid:396)(cid:349)(cid:282)(cid:258)(cid:455)(cid:853)(cid:3)(cid:38)(cid:286)(cid:271)(cid:396)(cid:437)(cid:258)(cid:396)(cid:455)(cid:3)(cid:1009)(cid:853)(cid:3)(cid:1006)(cid:1004)(cid:1006)(cid:1005)\n\n(cid:1005)(cid:1004)(cid:855)(cid:1005)(cid:1009)(cid:3)(cid:4)(cid:68)\n\n\n", "\n\n\n", "\n\n\n"]], ["C:\\Users\\mjols\\Python39\\python type checking libraries.pdf", ["Python 3 Types in the Wild:\n\nA Tale of Two Type Systems\n\nIngkarat Rak-amnouykit\n\nRensselaer Polytechnic Institute\n\nNew York, USA\n\nrakami@rpi.edu\n\nDaniel McCrevan\n\nRensselaer Polytechnic Institute\n\nNew York, USA\n\nmccred@rpi.edu\n\nAna Milanova\n\nRensselaer Polytechnic Institute\n\nNew York, USA\n\nmilanova@cs.rpi.edu\n\nMartin Hirzel\n\nIBM TJ Watson Research Center\n\nNew York, USA\n\nhirzel@us.ibm.com\n\nJulian Dolby\n\nIBM TJ Watson Research Center\n\nNew York, USA\n\ndolby@us.ibm.com\n\nAbstract\n\nPython 3 is a highly dynamic language, but it has introduced\n\na syntax for expressing types with PEP484. This paper ex-\n\nplores how developers use these type annotations, the type\n\nsystem semantics provided by type checking and inference\n\ntools, and the performance of these tools. We evaluate the\n\ntypes and tools on a corpus of public GitHub repositories. We\n\nreview MyPy and PyType, two canonical static type checking\n\nand inference tools, and their distinct approaches to type\n\nanalysis. We then address three research questions: (i) How\n\noften and in what ways do developers use Python 3 types?\n\n(ii) Which type errors do developers make? (iii) How do type\n\nerrors from different tools compare?\n\nSurprisingly, when developers use static types, the code\n\nrarely type-checks with either of the tools. MyPy and PyType\n\nexhibit false positives, due to their static nature, but also flag\n\nmany useful errors in our corpus. Lastly, MyPy and PyType\n\nembody two distinct type systems, flagging different errors\n\nin many cases. Understanding the usage of Python types can\n\nhelp guide tool-builders and researchers. Understanding the\n\nperformance of popular tools can help increase the adoption\n\nof static types and tools by practitioners, ultimately leading\n\nto more correct and more robust Python code.\n\nCCS Concepts: • Software and its engineering → Lan-\n\nguage features; • Theory of computation → Type struc-\n\ntures.\n\nKeywords: Python, type checking, type inference\n\nPermission to make digital or hard copies of all or part of this work for\n\npersonal or classroom use is granted without fee provided that copies are not\n\nmade or distributed for profit or commercial advantage and that copies bear\n\nthis notice and the full citation on the first page. Copyrights for components\n\nof this work owned by others than ACM must be honored. Abstracting with\n\ncredit is permitted. To copy otherwise, or republish, to post on servers or to\n\nredistribute to lists, requires prior specific permission and/or a fee. Request\n\npermissions from permissions@acm.org.\n\nDLS ’20, November 17, 2020, Virtual, USA\n\n© 2020 Association for Computing Machinery.\n\nACM ISBN 978-1-4503-8175-8/20/11. . . $15.00\n\nhttps://doi.org/10.1145/3426422.3426981\n\nACM Reference Format:\n\nIngkarat Rak-amnouykit, Daniel McCrevan, Ana Milanova, Martin\n\nHirzel, and Julian Dolby. 2020. Python 3 Types in the Wild: A Tale\n\nof Two Type Systems. In Proceedings of the 16th ACM SIGPLAN\n\nInternational Symposium on Dynamic Languages (DLS ’20), Novem-\n\nber 17, 2020, Virtual, USA. ACM, New York, NY, USA, 14 pages.\n\nhttps://doi.org/10.1145/3426422.3426981\n\n1 Introduction\n\nDynamic languages in general and Python in particular1\n\nare increasingly popular. Python is particularly popular for\n\nmachine learning and data science2. A defining feature of\n\ndynamic languages is dynamic typing, which, essentially, for-\n\ngoes type annotations, allows variables to change type and\n\ndoes nearly all type checking at runtime. This is in contrast\n\nto static typing, which (typically) requires type annotations,\n\nfixes variable types, and does nearly all type checking before\n\nprogram execution. Dynamic typing allows for rapid proto-\n\ntyping, whereas static typing flags errors early and generally\n\nimproves program correctness and robustness.\n\nTo enable static checking, Python has introduced PEP\n\n484 [18], which gives a syntax for optional type annotations.\n\nIt leaves the semantics of type checking largely unspecified,\n\nand multiple type checking and inference tools have been\n\ndeveloped [3, 9, 10, 12, 15, 20]. MyPy3 and PyType4 appear\n\nto be the canonical tools in this space5. They are relatively\n\nrobust, actively developed and maintained, and they have\n\nestablished themselves as baseline tools for the evaluation\n\nof new Python type inference analyses [3, 10].\n\nThis paper presents a study of Python 3 type usage by\n\ndevelopers, as well as a study of the performance of MyPy\n\nand PyType on a corpus of 2,678 repositories (with a total of\n\n173,433 files) that have partial type annotations. Surprisingly,\n\nonly a small percentage, 2,678 out of over 70,000 repositories\n\nhttps://www.aitrends.com/data-science/here-are-the-top-5-\n\n1see http://pypl.github.io/PYPL.html\n\n2see\n\nlanguages-for-machine-learning-data-science/\n\n3http://mypy-lang.org/\n\n4https://github.com/google/pytype\n\n5PEP 484 “is strongly inspired by mypy”\n\n\n\n", "DLS ’20, November 17, 2020, Virtual, USA\n\nIngkarat Rak-amnouykit, Daniel McCrevan, Ana Milanova, Martin Hirzel, and Julian Dolby\n\nwe started out with, have (partial) type annotations. Also\n\nsurprisingly, annotated repositories rarely type-check. This\n\npaper is accompanied by a web page with detailed data6.\n\nWe believe that our study can benefit the Python com-\n\nmunity in two ways. Understanding the usage of types can\n\nhelp guide type-system designers and tool builders. Under-\n\nstanding the performance of popular tools can help increase\n\ndevelopers’ adoption of static types and tools, and ultimately\n\nlead to more correct and robust Python code. While there\n\nare other studies of the behavior of dynamic languages, ours\n\nis the first study of Python 3 types in the wild.\n\nThis paper reviews the semantics of MyPy and PyType and\n\ncontrasts the two type systems. It then proceeds to address\n\nthree research questions:\n\n• RQ1: How often and in what ways do developers use types?\n\n• RQ2: Which type errors do developers make?\n\n• RQ3: How do type errors from different tools compare?\n\nWe find that developers write type annotations that are\n\nuser-defined class types more frequently than individual sim-\n\nple types (e.g., int, str). Developer-written type annotations\n\nare difficult to infer, while at the same time PyType can infer\n\nnon-trivial types for a large number of variables that lack\n\ndeveloper-written annotations. We find that MyPy and Py-\n\nType exhibit false positives, due to their static nature, but at\n\nthe same time they flag many likely runtime errors. Among\n\nother findings, we observe that the Optional[<type>] type,\n\nwhich indicates that a variable is either None or of type, is a\n\nsignificant source of both false positives and likely runtime\n\nerrors. Lastly, we perform a larger-scale empirical study of\n\nMyPy and PyType errors and show that the two type sys-\n\ntems flag largely disjoint sets of errors. Arguably, having\n\ntwo fundamentally different type systems violates the Zen of\n\nPython7, which famously states that “There should be one–\n\nand preferably only one –obvious way to do it.”\n\n2 Background\n\nThis section describes a core Python syntax and the seman-\n\ntics of MyPy and PyType as gleaned from the documentation\n\nand trial and error. We stress that our intention is not to\n\ngive complete formal treatment of the two type systems. For\n\ninstance, we omitted class definitions and included only lim-\n\nited forms of assignments to properties or indices. MyPy,\n\nPyType, and the Python language are expansive and rapidly\n\nevolving. Our intention is to highlight the two type systems,\n\ncompare and contrast them, and set the stage for our study\n\nof Python types and Python type checking in the wild.\n\nMyPy and PyType both combine manual type annotations\n\nbased on PEP 484 [18] with type checking; however, their\n\napproaches to type checking differ. Philosophically, MyPy\n\nprovides a conventional static type system in which variables\n\ncan be declared to have fixed types, and misuse of such types\n\n6https://py3typeinthewilddls20.github.io/Py3TypeInTheWildDLS20/\n\n7https://www.python.org/dev/peps/pep-0020/\n\nβ ::= int | float | str | bool | None base types\n\nτ ::= β | C | C[σ ] | Optional[σ ]\n\n+ constructed types\n\n| List[σ ] | Dict[σ1, σ2] | Tuple[σ ]\n\n| Callable[[σ1], σ2] | Union[σ ]\n\nσ ::= τ | TypeVar | Any\n\n+ type variables and Any\n\nFigure 1. Type syntax (simplified). C is a user-defined class.\n\nf ::= def f(x : σ1) → σ2 : s\n\nv ::= x : σ = e\n\ne ::= const | x | e.attr | e1[e2] | e1(e2)\n\n}\n\n1, ..., en : e ′\n\nn\n\ns ::= x = e | x.attr = e | x[e1] = e2\n\n| [e1, ...en ] | {e1 : e ′\n\n| s | if e : s1 else : s2 | return e\n\nfunction definition\n\nvariable declaration\n\nexpressions\n\nstatements\n\nFigure 2. Language syntax (simplified).\n\nis an error. PyType conforms more to legal Python usage,\n\nwhere types can change in a function. More concretely, there\n\nare three broad differences in their semantics: (1) MyPy’s\n\nanalysis is intra-procedural, while PyType does some inter-\n\nprocedural reasoning, (2) PyType is generally less strict than\n\nMyPy, and (3) MyPy reports errors early whereas PyType\n\ndelays error reporting.\n\n2.1 Syntax\n\nFig. 1 defines a core Python type syntax. It defines a hierarchy\n\nof types where types in τ can be instantiated and types in\n\nσ − τ cannot be instantiated. Types in σ can be used as type\n\nannotations and as arguments of generics.\n\nWe consider the core Python syntax in Fig. 2 and proceed\n\nto describe the semantics of MyPy and PyType over this\n\nsyntax. As is standard, each system evaluates expressions and\n\nstatements in a type environment Γ where Γ maps variables\n\nto types. Expressions have no effect on Γ but declarations\n\nand statements may change Γ, as MyPy and PyType infer\n\ntypes for variables.\n\n2.2 Expressions\n\nTable 1 illustrates expression typing. Given a type environ-\n\nment Γ, helper function type(Γ, e) returns the type of expres-\n\nsion e. For expressions, type is essentially the same for MyPy\n\nand PyType. (List and dictionary literals are an exception.)\n\nHowever, even though the semantics of retrieval are essen-\n\ntially the same, the types that are retrieved generally differ\n\nbecause the two type systems construct Γ in different ways.\n\nAs an example, given e1[e2], the system retrieves the type of\n\ne1 from its environment, and if the type is indexable, i.e., it is\n\neither List[σ ] or Dict[_, σ ], it returns the element type σ .\n\nMyPy and PyType differ in how they type list literals\n\n[e1, ..., en] and dictionary literals {e1 : e ′\n\n}. To\n\nimplement the join (∨), PyType creates a Union, whereas\n\nMyPy finds the least common ancestor in the subtype lattice.\n\n1, ..., en : e ′\n\nn\n\n\n\n\n\n\n\n", "Expression\n\nconst\n\nx\n\ne.attr\n\ne1[e2]\n\ne1(e2)\n\n[e1, ..., en ]\n\n{e1 : e ′\n\nStatement\n\nx = e\n\nx.attr = e\n\nx[e1] = e2\n\nPython 3 Types in the Wild: A Tale of Two Type Systems\n\nDLS ’20, November 17, 2020, Virtual, USA\n\nTable 1. Expression typing.\n\nMyPy and PyType typing\n\ntype(Γ, const)\n\nΓ(x)\n\ntype(Γ, e.attr)\n\nσ , where type(Γ, e1) = List[σ ] or type(Γ, e1) = Dict[_, σ ]\n\nσ , where type(Γ, e1) = Callable[_, σ ]\n\nList[type(Γ, e1) ∨...∨ type(Γ, en )]]\n\nDict[type(Γ, e1) ∨...∨ type(Γ, en ), type(Γ, e ′\n\n1) ∨...∨ type(Γ, e ′\n\nn )]\n\nCode example\n\n1\n\nx = ’Ni’\n\na.f = 1\n\nli = [1,2]\n\ndef id(x)->int:return x\n\n[1,2 ]\n\n{’Ni’: 1 }\n\nType for example\n\ntype(1) = int\n\ntype(x) = str\n\ntype(a.f) = int\n\ntype(li[i]) = int\n\ntype(id(5)) = int\n\ntype([1, 2]) = List[int]\n\ntype({’Ni’:1 })= Dict[str, int]\n\n1, ..., en : e ′\n\nn\n\n}\n\nif e: s1 else: s2\n\ncompute the join (∨) after the if/else\n\nΓ[x.attr ← type(Γ, e) ∨ σ ] where Γ(x.attr) = σ\n\nΓ[x ← type(Γ, e2) ∨ σ ], where Γ(x) = List[σ ] or Dict[_, σ ]\n\nTable 2. Statement typing.\n\nPyType\n\nΓ[x ← type(Γ, e)]\n\nMyPy\n\nΓ[x ← type(Γ, e)], if x (cid:60) Γ\n\n–, if type(Γ, e) <: Γ(x)\n\nerror, otherwise\n\nΓ[x.attr ← type(Γ, e) ∨ σ ] where Γ(x.attr) = σ\n\n–, if type(Γ, e2) <: σ , where Γ(x) = List[σ ] or Dict[_, σ ]\n\nerror, otherwise\n\nrequire same type on both branches\n\nTable 3. Typing examples.\n\nx[e1] = e2\n\n2\n\n3\n\n4\n\n5\n\n10\n\n11\n\nx = e\n\n1 def foo(x, y : int):\n\ni = input()\n\nif (i == 'Camelot'):\n\nz = 1\n\nelse:\n\nz = 'coconut'\n\n6\n\n7 #MyPy: ERROR in line 6\n\n8 #PyType: OK,\n\n9 # type(z) = Union[int, str]\n\nx = 1\n\nx = 'coconut'\n\n12\n\n13 #MyPy: OK, type(x) = Any\n\n14 #PyType: OK, type(x) = str\n\nx.attr = e\n\n1 ...\n\n2 a = A(1)\n\n3 b = a\n\n4 i = input()\n\n5 if (i == 'Camelot'):\n\na.attr = 'coconut'\n\n6\n\n7 else:\n\na.attr = 1\n\n8\n\n9 #MyPy: OK,\n\n10 # type(a.attr) = object\n\n11 # type(b.attr) = Any\n\n12 #PyType: OK,\n\n13 # type(a.attr) = type(b.attr)\n\n14 # = Union[str, int]\n\n1 li = [1]\n\n2 #MyPy: OK, type(li) = List[int]\n\n3 #PyType: OK, type(li) = List[int]\n\n4\n\n5 li[1] = 'coconut'\n\n6 #MyPy: ERROR\n\n7 #PyType: OK,\n\n8 # type(li) = List[Union[int,str]]\n\nConsider the list literal [42,’parrot’]. PyType types li as\n\nList[Union[int,str]] but MyPy types it as List[object],\n\nwhich is the join of int and str and the top of the type\n\nhierarchy. Now let i be an integer variable. PyType types\n\nexpressions li[i] and li[0] as Union[int,str] and int,\n\nrespectively (the latter is somewhat surprising). MyPy types\n\nboth expressions as object. On a side note, PyType types\n\n[1,1.5 ] as List[Union[int,float]], while MyPy types it\n\nas List[float]. Dictionary literals are treated analogously.\n\n2.3 Statements\n\nTable 2 illustrates statement typing. Statements build Γ, and\n\nMyPy and PyType differ significantly. Consider assignment\n\nx = e. MyPy updates Γ only if x is not in Γ (i.e., (1) it is not\n\nannotated by the user, (2) it is not assigned earlier in the\n\ncode, and (3) it is not an unannotated function parameter,\n\nin which case MyPy assigns type Any). If the type of the\n\nright-hand side expression e is a subtype of the type of x in\n\nΓ, MyPy leaves Γ intact and proceeds; otherwise, it issues\n\nan error (more on the error code later). PyType, on the other\n\nhand, unconditionally updates Γ for any subsequent code.\n\nTable 3 shows code examples that illustrate the key differ-\n\nences between the MyPy and PyType type systems. Consider\n\nthe left-most column, which illustrates the case x = e. MyPy\n\nassigns a single type to a given variable, a standard practice\n\nin many type systems. It therefore reports an error in Line 6,\n\nat z = ’coconut’, because z has already been mapped to int\n\nin Line 4. In contrast, PyType maintains separate environ-\n\nments for the true and false branches; it updates z along the\n\nfalse branch, then merges the two environments to type z as\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "DLS ’20, November 17, 2020, Virtual, USA\n\nIngkarat Rak-amnouykit, Daniel McCrevan, Ana Milanova, Martin Hirzel, and Julian Dolby\n\nUnion[int,str]. Thus, PyType maintains different typings\n\nfor the same variable at different program points; however, it\n\nis not clear precisely how it does this8, and there is no formal\n\ndefinition in the documentation. Note that MyPy does not\n\nissue an error on Line 12; this is because x is a parameter,\n\nand untyped parameters always map to Any. Had x been just\n\nan unannotated local variable, Line 12 would have triggered\n\nan error. PyType, on the other hand, treats x just as it would\n\nhave treated a local variable — it updates the type at the\n\nreassignment.\n\nNext, consider the middle column, illustrating x.attr = e.\n\nBoth MyPy and PyType compute the join (∨), but they do\n\nso differently. While MyPy types a.attr as object, PyType\n\nassigns a more specific type, Union[str,int]. While MyPy\n\ndoes not detect that a.attr and b.attr are aliases and types\n\nb.attr with the default attribute type Any, PyType detects\n\nthe aliasing and types b.attr as Union[str,int].\n\nFinally, consider the right-most column, which illustrates\n\nx[e1] = e2. MyPy sets the type of the list element at list initial-\n\nization and disallows the addition of incompatible elements.\n\nIn contrast, PyType refines the type of the list element as we\n\nadd new elements.\n\n2.4 Subtyping\n\nWe write σ1 <: σ2 to denote that σ1 is a subtype of σ2 in\n\nthe sense of MyPy and PyType. The key idea is that σ1 is a\n\nsubtype of σ2 if “a σ1 object can be used where a σ2 object\n\nis expected”. This is the standard notion of true subtyping.\n\nBelow is a core subset of subtyping rules:\n\n1. Subtype checks involving Any always succeed: σ <: Any\n\nand Any <: σ . This immediately renders the type system\n\nunsound, in the sense that an expression may produce a\n\nvalue whose type differs from its static type.\n\n2. Subtyping is reflexive: σ <: σ .\n\n3. Class types form a hierarchy as defined by the subclassing\n\nrelation: C(...,B[T],...) implies that C[σ ] <: B[σ ]. For\n\nexample, List[A] <: Sequence[A].\n\n4. Generic instantiations are invariant in their type argu-\n\nments except for the case when one of the arguments is\n\nAny. E.g., List[int] ≮: List[float], but List[int] <:\n\nList[Any] and List[Any] <: List[int] both hold.\n\n5. Union[σ1, ...σn] <: Union[σ ′\n\n{σ1, ...σn } there is a σj ∈ {σ ′\n\n] iff for every σi ∈\n\n1, ...σ ′\n\nm\n\nm } such that σi <: σj .\n\n1, ...σ ′\n\n2.5 Error Codes\n\nTable 4 describes the error codes of MyPy and PyType. As\n\ndiscussed earlier, MyPy reports errors at assignments, specif-\n\nically, if the right-hand side of the assignment is not com-\n\npatible with the left-hand side. In contrast, PyType never\n\nreports errors at assignments. Instead, it updates the type of\n\n8trivial aliasing can result in wrong types, see https://github.com/google/\n\npytype/issues/616\n\nExpr/Stmt MyPy error code\n\ne.attr\n\nPyType error code\n\nattribute-error\n\nTable 4. Error codes.\n\nattr-defined or union-attr\n\nif type(Γ, e), or one of its mem-\n\nbers, has no attribute attr\n\nindex\n\nif type(Γ, e1) is neither List[_]\n\nnor Dict[_, _]\n\noperator\n\nif type(Γ, e1) (cid:44) Callable[_, _]\n\narg-type\n\nif\n\ntype(Γ, e1) = Callable[[σ ], _]\n\nand type(Γ, e2) ≮: σ\n\nvar-annotated\n\nvar-annotated\n\nassignment\n\nif type(Γ, e) ≮: Γ(x)\n\nassignment\n\nif type(Γ, e) is not compatible\n\nwith attr annotation\n\nlist-item or dict-item\n\nif Γ(x) = List[σ ] or Γ(x) =\n\nDict[_, σ ] and type(Γ, e2) ≮: σ\n\nreturn-value\n\nif type(Γ, e) is not compatible\n\nwith function annotation\n\nif x (cid:60) Γ\n\nif x (cid:60) Γ\n\n–\n\n–\n\n–\n\n–\n\n–\n\ne1[e2]\n\ne1(e2)\n\ne1(e2)\n\nx = []\n\nx = {}\n\nx = e\n\nx.attr = e\n\nx[e1] = e2\n\nreturn e\n\nunsupported-\n\noperands\n\nnot-callable\n\nwrong-arg-types\n\nbad-return-type\n\nthe left-hand side, disregarding user annotations when nec-\n\nessary. PyType is “more dynamic” in nature, as it propagates\n\nthe object “downward” and delays error reporting until the\n\nobject is actually used, e.g., as an argument, as a function\n\nvalue, or as an indexable value. To illustrate:\n\nThis example fails in MyPy but is type-correct in PyType be-\n\ncause the final type of z does not conflict with the return type\n\nof f, which is not manually annotated and hence defaults to\n\nAny. In contrast,\n\n2\n\n1 def f(x : int):\n\nz : int = 1\n\nz = 'coconut'\n\nreturn z\n\n3\n\n4\n\n1 def foo(x: int):\n\nprint(x)\n\n2\n\n3 def f(x : int):\n\n4\n\n5\n\nz : int = 'coconut'\n\nreturn foo(z)\n\nfails in Line 5 because the actual argument type str is in-\n\ncompatible with the formal parameter type int. Somewhat\n\nsurprisingly, but consistently with the semantics we outlined\n\nabove, PyType disregards the int annotation on z.\n\nWe classify the error codes into the following categories\n\n(for mapping concrete error codes into each category, see\n\nFig. 6 and 7):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "Python 3 Types in the Wild: A Tale of Two Type Systems\n\nDLS ’20, November 17, 2020, Virtual, USA\n\n1. Syntax errors. These are standard parse errors. For exam-\n\nple, MyPy’s syntax flags Python parse errors.\n\n2. Shallow semantic errors. While these are flagged by seman-\n\ntic analysis, that analysis is less sophisticated than full-\n\nfledged type analysis. For instance, it can mostly perform\n\nits reasoning locally or based on names. Typically there\n\nare no false positives. An example is MyPy’s call-arg\n\nwhich checks that the number and names of arguments at\n\nthe function call match the function definition.\n\n3. Deep semantic errors. These are standard type errors, where\n\nflagging the error requires deep semantic analysis that\n\ninfers a type for variables/expressions. For example, MyPy\n\nraises attr-defined at expression e.attr if its inferred type\n\nfor e does not have attribute attr. Tab. 4 lists the essential\n\ndeep semantic errors in MyPy and PyType.\n\n4. Import/other errors. These are import or other (rare) errors\n\nthat do not fit into the above categories.\n\nWhile there is no universally agreed upon distinction be-\n\ntween these error categories, we found the categorization\n\nuseful for the purpose of this paper. The distinction is not\n\nalways that sharp, but it helps characterize type errors and\n\ntools. Sect. 4 makes use of this categorization of MyPy and\n\nPyType error codes.\n\n3 How Often and in What Ways Do\n\nDevelopers Use Python 3 Types? (RQ1)\n\nWe examine a collection of 70,826 Python repositories from\n\npublic GitHub from 49,546 organizations. Only 2,678 had\n\n(partial) Python 3-style type annotations. We refer to this\n\nset as the set of typed repositories. Of these, 195 are mal-\n\nformed repositories, meaning that MyPy could not further\n\ntype check the repositories due to specific errors “duplicate\n\nmodule” and “no parent module”. We used a flag to sup-\n\npress unresolved imports; however, some subsequent errors,\n\nsuch as attr-defined: “Module has no attribute xyz” and\n\nname-defined will still be reported. We analyzed all user\n\nannotations on variables across the 2,678 typed repositories\n\nwhich contain 173,433 files.\n\n3.1 Statistic Across Typed Repositories\n\nWhile the repositories are from GitHub, we collected them\n\nwith a query to Google BigQuery that focused on recent\n\nPython repositories that had been watched more than once.\n\nThe query9 was issued in August 2019, but reflects a capture\n\nof GitHub from March 20, 2019, by Google; it was meant to\n\nfetch Python code in the wild, so they are an arbitrary fetch\n\nof Python code and we did not filter for type annotations.\n\nIndeed, one goal of our study is to find out how much Python\n\ntypes are currently used.\n\nOverall, we were able to find little configuration of the\n\ntools we evaluate. The initial query gathered only Python\n\n9https://github.com/wala/graph4code/blob/master/extraction_queries/\n\nbigquery.sql\n\nFigure 3. Number of type annotations per file across 2,678\n\nrepositories (log scale).\n\nFigure 4. Histogram of the average number of type annota-\n\ntions per file per repo, across 2,678 repositories.\n\nfiles, but, by checking the current version of the repositories\n\non GitHub, only 101 out of 2,678 typed repositories have the\n\nstandard mypy.ini at the top level. Furthermore, we examined\n\n50 random repositories. Three of them have mypy.ini, and six\n\nof them either have MyPy in their requirement or mention\n\nit in the repository. Similarly, ten repositories have PyLint\n\nin their requirement, or mention it somewhere. Only one\n\nrepository mentioned PyType, and did so in an issue.\n\nFig. 3 and Fig. 4 illustrates the concentration of annotations\n\nover 2,678 typed repositories. Fig. 3 shows the concentration\n\nby file. About 80% of all files do not have any type annota-\n\ntion, 5% have fewer than 11 annotations, and only 0.5% have\n\nmore than 40 annotations. Fig. 4 shows the concentration by\n\nrepository. 1,144 repositories have an average of less than 1\n\nannotation per file, and 50 repositories have more than 20\n\nannotations per file. Overall, the vast majority of files and\n\nrepositories have a low concentration of type annotations;\n\nfew files and repositories use type annotations extensively.\n\n3.2 How Do Developers Write Types?\n\nFig. 5 groups developer-written type annotations into the\n\nmost popular categories of types, including simple types\n\n\n\n\n", "DLS ’20, November 17, 2020, Virtual, USA\n\nIngkarat Rak-amnouykit, Daniel McCrevan, Ana Milanova, Martin Hirzel, and Julian Dolby\n\nTable 5. Comparing developer-written types against PyType-\n\ninferred types over a set of 4,079 files.\n\nFigure 5. User type hints. The top graph is a histogram of\n\nannotations per type (log scale); the bottom graph shows the\n\ndistribution of individual type annotations among function\n\nparameters, function return types, and assignments.\n\nsuch as str, int, etc., and built-in composite types such\n\nas List, Dict, etc. Category “Other Typing” includes the\n\ntypes defined in the typing package10 but excludes List,\n\nDict, and the other frequent types that are plotted separately.\n\nType annotations that do not fall into any of these categories\n\nare counted in “User-defined”. “User-defined” includes user-\n\ndefined classes, classes defined in built-in Python modules\n\nand third-party modules, as well as type variables and type\n\naliases. In general, it is impossible to accurately distinguish\n\nthese subcategories based on string matching, as develop-\n\ners can use arbitrary names for user-defined classes, type\n\nvariables, type aliases, etc. While a deeper semantic analysis\n\nmay be able to tease those apart, this is beyond the scope of\n\nour current work.\n\nFig. 5 (top) illustrates the frequency of each type cate-\n\ngory. As expected, str, int, and bool feature prominently.\n\nBuilt-in collections List, Dict, and Tuple and their legacy\n\ncounterparts list, dict, and tuple feature prominently as\n\nwell. MyPy accepts the lowercase annotations and treats\n\nthem as Any-instantiated versions of their uppercase coun-\n\nterparts. There are 69,063 annotations with types in cate-\n\ngory “User-defined”. We examined two repositories, and the\n\nuser-defined types were predominantly non-local classes.\n\nThere are 11,114 Optional types, making Optional one of\n\n10https://docs.python.org/3/library/typing.html\n\nthe most prominent annotations. Fig. 5 (bottom) illustrates\n\nthe distribution of annotations among function parameters,\n\nfunction return types, and global assignments. Parameters\n\ndominate nearly every category because they account for\n\nthe largest share of all type annotations. However, None and\n\nTuple (unsurprisingly) are predominantly used in function\n\nreturn types.\n\nTo further understand the nature of developer-written\n\nannotations, we conducted an additional experiment. We se-\n\nlected 4,079 random files out of the 173,433 typed-repository\n\nfiles, stripped the annotations, and ran PyType on each file,\n\nwhich essentially amounts to intra-module type inference.\n\nTab. 5 shows the results. Tab. 5(a) compares all places in the\n\ncode that had either developer-written or PyType-inferred\n\ntypes. Tab. 5(b) focuses only on places where the original\n\ncode had an explicit developer-written type annotation. Py-\n\nType infers a “meaningful” type (i.e., non-Any and non-None)\n\nfor 39% of all developer-written or PyType-inferred type an-\n\nnotations (sum of all columns except Any and None). On the\n\nother hand, looking at the developer-written annotations\n\nalone, inference matches a “meaningful” user type for only\n\n6.1%; when we include Any and None matches, it matches\n\n13.7% (Tab. 5(b)). It fails to infer a type for 77% of the user-\n\nwritten annotations. This leads to the conclusion that (1) user\n\nannotations are infrequent (as they are only a small fraction\n\nof the PyType-inferred ones) and (2) user annotations are\n\n\n\n\n\n\n", "Python 3 Types in the Wild: A Tale of Two Type Systems\n\nDLS ’20, November 17, 2020, Virtual, USA\n\ndifficult to infer and are therefore likely to be informative\n\n(as annotations appear to carry non-local information that is\n\nbeyond the power of PyType’s abstract interpretation-based\n\nstatic analysis). This matches the results of our earlier exper-\n\niment (Fig. 5), where we found that there is a large number\n\nof “User-defined” types and that those user-defined types\n\ntend to be non-local classes.\n\n3.3 Are Typed Repositories Type Correct?\n\nOnly 318, or 15%, of the 2,678 repositories are type correct in\n\nMyPy; the remaining repositories produce a total of 41,607\n\nerrors of which 22,556 are non-import related errors. Fig. 6\n\nshows the distribution of errors across typed repositories.\n\nA key question arises: why are so few repositories type-\n\ncorrect? One hypothesis that we extend is that MyPy, by\n\nvirtue of being a static type system, may be too conservative,\n\nproducing false-positive warnings more often than catching\n\nactual run-time errors. Therefore, developers are discouraged\n\nfrom spending valuable time fixing type errors. At the same\n\ntime, developers still see value in writing type annotations as\n\nthey serve as documentation. We conduct additional studies\n\ntowards this question and present our results in Sect. 4.\n\nAnother hypothesis that we extend is that MyPy is not the\n\ntool of choice for developers and that developers type-check\n\ntheir code with other tools that are less strict or more readily\n\navailable in popular IDEs. Specifically, we studied PyLint\n\nand PyType. We explored several questions. Does the code in\n\ntyped repositories type-check with any of these alternative\n\ntools? How do these tools compare to each other, e.g., do\n\nthey catch the same set of errors or do they catch distinct\n\nsets of errors? We conduct additional studies and present\n\nour results in Sect. 4.3 and in Sect. 5.\n\n4 Which Type Errors Do Developers Make?\n\n(RQ2)\n\nWhen a developer goes through the trouble of adding manual\n\ntype annotations, why are there so many type errors from\n\nMyPy? Are these errors mostly false positives, or do devel-\n\nopers really make that many mistakes? What happens to the\n\nfalse positive rate when using PyType instead of MyPy for\n\ntype-checking? How do the answers to the above questions\n\nvary when we break them down by different error codes?\n\nThis section examines type errors in 2,678 Python github\n\nrepositories, each of which contains at least one Python 3\n\ntype annotation. We first count errors by their error codes,\n\nwhich can be done automatically and is thus easy to do at\n\nscale. Then, we classify errors into three categories: false\n\npositives vs. two kinds of true positives, namely likely run-\n\ntime error vs. incorrect type annotation. This latter analysis\n\nrequires manual inspection and cannot be fully automated,\n\nso we sampled 15 errors for each of several of the most com-\n\nmon or most interesting error codes, and hand-inspected\n\nthose.\n\nFigure 6. MyPy error code distribution (log scale). Solid\n\nblack bars denote deep semantic errors, dotted bars denote\n\nsyntax errors, striped bars denote shallow semantic errors,\n\nand solid red bars denote import/other errors.\n\nFigure 7. PyType error code distribution (log scale).\n\n4.1 Error Code Distributions\n\nFigure 6 shows error code distributions for MyPy and Fig-\n\nure 7 shows error code distributions for PyType. In both\n\ncases, we ran the respective tool to type-check all 2,678 typed\n\nrepositories. Each occurrence of an error counted separately;\n\nfor instance, when one of the repositories had 100 errors of a\n\ngiven error code, we add 100 to the count for that error code.\n\nOverall, the raw numbers are higher for PyType, because\n\nMyPy ignores untyped functions.\n\nThere are nine MyPy error codes with more than 1,000\n\nerrors each. Out of these, four indicate deep semantic errors:\n\nassignment, arg-type, union-attr, and return-value.\n\nThere are also nine PyType error codes with more than 1,000\n\nerrors each. Out of these, three indicate deep semantic er-\n\nrors: wrong-arg-types, unsupported-operands, and bad-\n\nreturn-type. We hand-analyze all of these in the following\n\nsubsections. We also hand-analyzed MyPy’s var-annotated\n\nerrors. These are mostly caused by initializing variables from\n\nempty collections (e.g., lst = [] or dct = {}) and do not\n\nconstitute deep semantic errors, so we did not include them\n\n\n\n", "DLS ’20, November 17, 2020, Virtual, USA\n\nIngkarat Rak-amnouykit, Daniel McCrevan, Ana Milanova, Martin Hirzel, and Julian Dolby\n\nin the discussion below. We did not hand-analyze name-\n\ndefined, misc, and attr-defined from MyPy or attribute-\n\nerror from PyType, because these errors are largely caused\n\nby missing imports and thus shed little light on the quality\n\nof type annotations or type checking tools.\n\n4.2 Examples of MyPy Errors\n\nThis section presents our hand-analysis of MyPy errors into\n\nfalse positives, likely runtime errors, and incorrect type an-\n\nnotations. Furthermore, this section exemplifies each error\n\ncode with snippets of idiomatic Python code written by pro-\n\ngrammers that do not pass the MyPy type checker. Doing so\n\nmay help programmers use MyPy more effectively and may\n\nhelp designers of type checkers improve their tools.\n\n4.2.1 assignment. There are 3,104 assignment errors.\n\nWe sampled 15 and found 15 false positives, 0 likely runtime\n\nerrors, and 0 incorrect type annotations. In all but one case,\n\nthe false positive was due to the issue of redefinitions with\n\nincompatible types.11 Below is a typical example:\n\n1 values = map(repr, result[0].values())\n\n2 values = zip(labels, values)\n\n3 values = [... for (label, value) in values]\n\nMyPy infers type Iterator[str] for variable values based\n\non the signature for map. This clashes with the type MyPy\n\ninfers for the expression in Line 2, based on the signature of\n\nzip: Iterator[Tuple[Any, str]]. This is a false positive,\n\nas the use of values in Line 3 is consistent with the return\n\nvalue of zip.\n\n4.2.2 union-attr. There are 1,592 union-attr errors.\n\nWe sampled 15 and found 5 false positives, 10 likely runtime\n\nerrors, and 0 incorrect type annotations. A typical example\n\nfor a likely runtime error is:\n\n1 m = re.match(RE_COMP_LOCUS, raw)\n\n2 d = m.groupdict()\n\n3 d['length'] = int(d['length'])\n\nIt leads to the error ‘\"None\" of \"Optional[Match[Any]]\" has\n\nno attribute \"groupdict\"’. If re.match fails to find a match\n\nand returns None, there is a runtime error at Line 2. Inter-\n\nestingly, in 4 out of 10 references to groupdict in the same\n\nfile, the developers had added a check for None:\n\n1 if m is None:\n\nreturn {'definition': None}\n\n2\n\n3 d = m.groupdict()\n\nWe conjecture that the code failed during testing leading to\n\nthe addition of the None-check. In contrast, paths with no\n\nruntime checks may have not been exercised with a None\n\nvalue during testing. In 5 cases, the type error was a false pos-\n\nitive, because there was a runtime check for None. Typically,\n\nthe check immediately preceded the access of the attribute,\n\n11https://mypy.readthedocs.io/en/stable/common_issues.html#\n\nredefinitions-with-incompatible-types\n\nas in the above example. 89% of the union-attr errors, 1,418\n\nout of 1,592, involved Optional[ type] and None.\n\n4.2.3 arg-type. There are 1,851 arg-type errors. We sam-\n\npled 15 and found 8 false positives, 5 likely runtime errors,\n\nand 2 incorrect type annotations. The 5 likely runtime errors\n\nwe observed were analogous to the errors in Sect. 4.2.2—\n\npassing an Optional[ type] argument for a parameter ex-\n\npecting type. In 2 cases, the type error was due to an incor-\n\nrect annotation. For example, error ‘Argument 1 to \"Fernet\"\n\nhas incompatible type \"ByteString\"; expected \"bytes\"’ at\n\n1 ..., encryption_key, ... = encryption_attrs(\n\n2\n\n3 fer = Fernet(encryption_key)\n\nhw_session, label)\n\nis due to the incorrect annotation ByteString in the return\n\ntype of encryption_attrs(...). That function retrieves\n\nencryption_key from base64.b64decode(), which is of\n\ntype bytes. The actual type of encryption_key is bytes as\n\nexpected by the Fernet constructor.\n\nOut of the 8 false positives, 7 were caused by a None check\n\nimmediately preceding the call that passed the variable of\n\ntype Optional[ type] as an argument. Optional[ type]\n\nfeatured prominently in arg-type errors as well; there were\n\n647 errors that involved Optional[ type].\n\nThe last false-positive error was ‘Argument 1 to \"to_bytes\"\n\nof \"int\" has incompatible type \"object\"; expected \"int\"’ for\n\n1 int.to_bytes(o['valueSat'], 8, byteorder='little')\n\nwhere o is the following dictionary:\n\n1 { 'address': 'XmqUtfzxgSx7WzYkEd14ug2UrJgaCmANzV',\n\n2\n\n'valueSat': 1664710 }\n\nAs shown in Sect. 2, MyPy infers type object for the dic-\n\ntionary element which is the join of all element types. It\n\ncannot distinguish that the reference element at ’valueSat’\n\nis indeed int, as expected. This is expected behavior of a\n\nstatic type system though.\n\n4.2.4 return-value. There are 1,399 return-value er-\n\nrors. We sampled 15 and found 1 false positive, 0 likely run-\n\ntime errors, and 14 incorrect type annotations. Those 14 were\n\ncaused by a mismatch between the returned value and the\n\nreturn type annotation. For example:\n\n1 def load_proper_noun_data() -> List[str]:\n\n2\n\n3\n\n4\n\n5\n\n...\n\nret = set()\n\n...\n\nreturn ret\n\ntriggers type error ‘Incompatible return value type (got\n\n\"Set[Any]\", expected \"List[str]\")’. It is unclear whether any\n\nof these will trigger a runtime error, it depends on the expec-\n\ntations of the caller. We observed several cases where the\n\nreturn annotation was incorrect and was ignored by the code\n\n\n", "Python 3 Types in the Wild: A Tale of Two Type Systems\n\nDLS ’20, November 17, 2020, Virtual, USA\n\nas the callers expected the actual return type (see bytes vs.\n\nByteString example in Sect. 4.2.3.\n\n4.3 Examples of PyType Errors\n\nLike the previous section did for MyPy, this section presents\n\nour hand-analysis of PyType errors into false positives, likely\n\nruntime errors, and incorrect type annotations. We again\n\nillustrate these with real-world examples, which we hope are\n\nuseful for the users and developers of type-checking tools.\n\n4.3.1 wrong-arg-types. There are 4,938 wrong-arg-\n\ntypes errors. We sampled 15 and found 8 false positives, 6\n\nlikely runtime errors, and 1 incorrect type annotation. One\n\nreason for false positives is incorrect or missing library stubs.\n\nFor example, in\n\n1 new_tree = ET.ElementTree(new_root)\n\n2 ET.dump(new_tree)\n\nET.dump does accept an ElementTree argument, however\n\nPyType issues a wrong-arg-types error. Another reason for\n\nfalse positives is intricate control flow checks that render the\n\ncode safe, checks that one cannot reasonably expect a static\n\ntype system to reason about. For example, PyType issues a\n\nwrong-arg-types error at call\n\n1 optimize_all(expression, None,\n\n2\n\nspecific=False, general=True)\n\nThis is because the second parameter of optimize_all, rels,\n\nis of type ContextDict but is passed None. Any static type\n\nsystem is expected to flag this as an error. There is no runtime\n\nerror though, because all access to rels in optimize_all is\n\nguarded by specific==True, and in this context of invoca-\n\ntion we have specific=False.\n\nWe observed many likely runtime errors as well. They\n\nwere overwhelmingly due to misuse of standard libraries:\n\n1 dotfile = NamedTemporaryFile(suffix='.dot')\n\n2 dotfile.write('digraph G {\\n')\n\nHere NamedTemporaryFile.write expects a bytes argu-\n\nment and passing a string results in a runtime error. We\n\nobserved the string/bytes misuse several times, across differ-\n\nent repositories. In one interesting case, PyType recognized\n\nthat built-in function open was used with the binary flag\n\nb, and it flagged several write’s to the corresponding file\n\nbecause they passed str arguments instead of bytes ar-\n\nguments. We also observed library functions with Boolean\n\nformal parameters being called with actual parameters that\n\nare integers 0 or 1.\n\n4.3.2 unsupported-operands. There are 2,887 errors.\n\nWe sampled 15 and found 6 false positives, 9 likely runtime\n\nerrors, and 0 incorrect type annotations. In several cases, the\n\nerror was due to a mismatch of library versions.\n\n1 tb = traceback.extract_stack()\n\n2 for back in tb:\n\n3\n\nkey = back[:3]\n\nTable 6. Summary of hand-examined error reports.\n\nfalse positives\n\ntrue positives\n\nMyPy\n\nPyType\n\n52 (49%)\n\n34 (44%)\n\nlikely runtime\n\nerrors\n\n29 (28%)\n\n32 (42%)\n\nincorrect\n\nannotations\n\n24 (23%)\n\n11 (14%)\n\nThe above code was written against version 2.4 of Python’s\n\ntraceback library, where traceback.extract_stack() re-\n\nturns a list of (filename, line number, function name,\n\ntext) tuples, each tuple representing a stack frame summary.\n\nIn version 3.5 of the library, the signature changes and now\n\ntraceback.extract_stack() returns a list of FrameSummary\n\nobjects. PyType, which checks against extensive Python 3\n\nstubs, issues an unsupported-operands error in line 5 as\n\nFrameSummary is not indexable.\n\nThere were interesting likely runtime errors in scipy:\n\n1 w = fftfreq(n)*h*2*pi/period*n\n\n2 w[0] = 1\n\n3 w = 1j/tanh(w)\n\n4 w[0] = 0j\n\nw, which is initialized as a complex number in Line 3, is not\n\nindexable. PyType reports an unsupported-operands error\n\nin Line 4. If the function executes, there is a runtime error.\n\nWith respect to the false positives, we again observed\n\nmostly complex control flow we cannot expect a static type\n\nsystem to handle. Consider another example from scipy:\n\n1 arglist = get_arglist(I_type, T_type)\n\n2 if T_type is None:\n\n3\n\ndispatch = \"%s\" % (I_type,)\n\nPyType reports an error inside get_arglist because T_type,\n\nwhich is an Optional type, is used in a string concatena-\n\ntion. A detailed look reveals that the string concatenation\n\nis guarded by checks (on different values, not the T_type\n\nparameter), and the checks ensure that T_type is not None\n\nat the string concatenation.\n\n4.3.3 bad-return-type. There are 2,627 bad-return-\n\ntype errors. We sampled 15 and found 4 false positives, 2\n\nlikely runtime errors, and 9 incorrect type annotations. Un-\n\nsurprisingly, the nature of the errors we saw is similar to\n\nthat of MyPy’s return-value errors.\n\n4.4 Discussion and Analysis\n\nSect. 4 started by asking why there are so many type errors\n\nfrom MyPy. Given what we have learned, the answer is\n\nthat many programmers do not run MyPy on their code\n\nand MyPy has many false positives. While programmers can\n\nmake false positives go away by changing their code, it is less\n\nwork to just ignore them. Tab. 6 summarizes the quantitative\n\nfindings from our manual inspection of 105 MyPy errors and\n\n77 PyType errors. In addition to the 60 MyPy errors discussed\n\n\n\n\n\n\n", "DLS ’20, November 17, 2020, Virtual, USA\n\nIngkarat Rak-amnouykit, Daniel McCrevan, Ana Milanova, Martin Hirzel, and Julian Dolby\n\nin Sect. 4.2 and the 45 PyType errors discussed in Sect. 4.3,\n\nwe also inspected various other categories of deep semantic\n\nerrors from both tools, omitted from the detailed discussion\n\nbut included in Table 6.\n\nAbout 49% of the MyPy errors and 44% of the PyType\n\nerrors we examined were false positives. MyPy’s false pos-\n\nitives were overwhelmingly due to either redefinition of\n\nvariables with incompatible types or Optional[<type>] be-\n\ning guarded by None checks. One highly frequent error,\n\nvar-annotated, does not catch actual runtime errors, and\n\nanother frequent error, assignment, exhibits false positives.\n\nWe found less frequent error categories, particularly union-\n\nattr and override, to be more likely to report type errors\n\nthat may lead to runtime errors.\n\nEven though PyType avoids the false positives of MyPy’s\n\nassignment, list-item, and dict-item as it pushes errors\n\ndown towards operations, its false positive rate is not much\n\nlower. This indicates that false positive MyPy assignment\n\nerrors have likely turned into false positive PyType wrong-\n\nargument-types or unsupported-operands errors.\n\nBoth MyPy’s arg-type and PyType’s wrong-argument-\n\ntypes flagged a good number of likely runtime errors each,\n\nand the nature of errors was similar. In both cases, false pos-\n\nitives often involved Optional[<type>] types. MyPy and\n\nPyType flagged potential flows of None values to operations\n\nthat expected non-None arguments but they were rendered\n\nfalse positives due to sometimes complex non-None checks.\n\nBoth MyPy’s return-value and PyType’s bad-return-\n\ntype revealed large percentages of incorrect user annota-\n\ntions: a method m actually returns a value incompatible with\n\nthe annotation on the return.\n\nWe conclude that MyPy would benefit if it allowed redef-\n\ninition of variables with incompatible types in some form.\n\nThis could be accomplished by using Static Single Assign-\n\nment (SSA) form for local variables in straight-line code. For\n\nexample, if it could rewrite the earlier example as follows:\n\n1 values1 = map(repr, result[0].values())\n\n2 values2 = zip(labels, values1)\n\n3 values3 = [... for (label, value) in values2]\n\nMyPy would infer separate types for values1, values2, and\n\nvalues3, and check corresponding usage with these types.\n\nThe above example is type correct, but suppose for the sake\n\nof argument values2 was used as a string in Line 3. MyPy\n\nwould flag the error at the usage point in Line 3. assignment\n\nerrors we observed typically involved straight-line code like\n\nthis, which will benefit from simple SSA.\n\nAnother lesson learned is that Optional[<type>] fea-\n\ntures prominently, both in user annotations (see Fig. 5) and\n\nin MyPy and PyType type errors. Optional[<type>] was\n\na large source of error messages, particularly, 90% of the\n\nunion-attr errors were due to the option of None. In some\n\ncases the error messages signaled a likely runtime error,\n\nfor example, passing an Optional argument, which can be\n\nNone, to a library method that expected a non-None param-\n\neter. In other cases the errors were false positives, as the\n\nruntime check for non-None value immediately preceded\n\nthe expected non-None assignment. We conjecture that both\n\nMyPy and PyType would benefit from reasoning about local\n\nchecks for non-None values. The long history of work on\n\nnon-null types in Java [1, 2, 5, 14] can bring insights into\n\nPython type checking.\n\nBoth type systems produce useful warnings as well. Even\n\nthose errors that were false positives may have been true\n\npositives at first and only made safe with complex checks\n\nafter developers experienced runtime errors.\n\n5 How Do Type Errors From Different\n\nTools Compare? (RQ3)\n\nDo different type-checking tools just differ at the surface,\n\ne.g., by using different error codes for the same errors? If yes,\n\nwhat is the mapping between error code? When one tool\n\nreports more errors than another, does it more-or-less report\n\na superset? Or if no, do the tools differ more fundamentally,\n\nusing error codes with little correspondence or overlap? We\n\ncounted matches between errors reported on the same source\n\ncode line to answer these questions. This section directly\n\ncompares MyPy to PyType. We also include PyLint as it is\n\nthe default linter in Visual Studio Code and other IDEs and\n\nhas received praise in developer blogs12. This section studies\n\nthe same repositories as the earlier sections in this paper. We\n\nfirst describe how we run the tools, then compare MyPy to\n\nPyType in Sect. 5.1 and MyPy to PyLint in Sect. 5.2.\n\nTools. MyPy, PyType, and PyLint each have a plethora of\n\ncommand-line options that impact the number and kinds of\n\nmessages reported, so we detail our exact usage here. We\n\nuse MyPy 0.770 with the following flags:\n\nmypy -- show - error - codes -- namespace - packages\n\n-- ignore - missing - imports -- show - column - numbers\n\nPATH / file1 . py PATH / file2 . py PATH / file3 . py ...\n\nWe use PyType 2020.04.01 with the following flags:\n\npytype -- keep - going PATH / file1 . py\n\nWe run PyType on each file because it stops checking\n\nwhen it finds the first error in a file.\n\nWe use PyLint 2.5.2 with the following flags:\n\npylint -d all -e typecheck\n\n-- unsafe - load - any - extension = y\n\nPATH / file1 . py PATH / file2 . py PATH / file3 . py ...\n\nWe are interested in errors reported by MyPy, PyType,\n\nand PyLint on the same line of code13 in order to compare the\n\ntools. This section counts errors differently than Sect. 4: Fig. 6\n\n12see https://en.wikipedia.org/wiki/Pylint and https://www.slant.co/topics/\n\n2692/~best-python-code-linters for a summary.\n\n13Only MyPy and PyLint can report more-precise positions.\n\n\n", "Python 3 Types in the Wild: A Tale of Two Type Systems\n\nDLS ’20, November 17, 2020, Virtual, USA\n\nand Fig. 7 counted every error, while here multiple errors of\n\nthe same category on the same line count as one error.\n\n5.1 MyPy vs. PyType\n\nExcluding import-related errors, MyPy reports 23,665 errors\n\nand PyType reports 65,938 errors. For example, there are 617\n\nof MyPy’s operator errors versus 2,887 of PyType’s similar\n\nunsupported-operands errors.\n\n5.1.1 Deep Semantic Errors. Fig. 8 contrasts deep se-\n\nmantic errors from the two tools. Our supplementary repos-\n\nitory, Py3TypeInTheWildDLS20, contains a dynamic ver-\n\nsion of this figure (graph 1 ), which breaks down each\n\nMyPy category into corresponding matching PyType sub-\n\ncategories. There is a good connection between MyPy’s\n\narg-type, union-attr, return-value and the correspond-\n\ning PyType error codes. Other error codes such as assign-\n\nment are not captured by PyType. Most notably, the fig-\n\nure shows that the vast majority of MyPy errors remain\n\nunmatched by PyType, and analogously, the majority of Py-\n\nType errors remain unmatched by MyPy. In the repository\n\nPy3TypeInTheWildDLS20 (graph 8 ) plots only match-\n\ning errors, excluding the dominating No Match category;\n\nthey emphasize matching errors across all error categories.\n\nWe detail the deep semantic error categories below.\n\nreturn-value: Out of 1,399 MyPy return-value errors,\n\n535 match with PyType’s bad-return-type. The remaining\n\n864 errors are due to the different semantics (Sect. 2). To\n\nillustrate, consider the following code:\n\n1 def f(a:int=None) -> str:\n\n2\n\n3\n\na = 'str'\n\nreturn a\n\nMyPy rejects this function, flagging both the assignment at\n\nLine 2 and the return in Line 3.14 PyType, however, does not\n\nreport an error.\n\nOn the other hand, there are 2,577 total PyType bad-\n\nreturn-type. Most of those unmatched with MyPy errors\n\nhave two causes. PyType views a function consisting of\n\na pass statement as returning None and rejects any mis-\n\nmatched return type. MyPy allows this.15 The second cause\n\nis that MyPy does not type-check unannotated functions.\n\nE.g.,\n\n1 def g(b):\n\nreturn 1\n\n2\n\n3 def f(a:int) -> str:\n\n4\n\nreturn g(a)\n\nPyType infers that g returns int and flags a bad-return-\n\ntype error. MyPy does not check g assigning return type Any\n\nwhich agrees with str.\n\n14As a side note, MyPy automatically upgrades the type of parameter a to\n\nan Optional[int]. Had a been a local variable, the assignment of None\n\nwould have been a type error, as in Sect. 2.\n\n15They recognize this in https://github.com/python/mypy/issues/2350\n\nOperator: There are 32 matching pairs between the 617\n\nMyPy operator errors and 2,525 PyType unsupported-\n\noperands. The overlap is relatively small due to the dif-\n\nference in the inference semantics. Apart from unary and\n\nbinary operations, unsupported-operands also checks if\n\nthe variable type supports indexing, which map to 105 out\n\nof 820 MyPy’s index errors. For example\n\n1 def __ne__(self, other):\n\nif hasattr(other,\"__getitem__\") and len(other)==2:\n\nreturn self.x != other[0] or self.y != other[1]\n\n2\n\n3\n\n4\n\n5\n\nelse:\n\nreturn True\n\nPyType has inferred that other has type int. Although\n\nLine 2 prevents invalid indexing, PyType still reports a unsup-\n\nported-operands false positive error at Line 3. MyPy does\n\nnot check this function because it is unannotated.\n\nArg-type: There are 345 matching pairs between 1,554\n\nMyPy arg-type errors and 4,657 PyType wrong-arg-types\n\nerrors. Both categories detect actual argument types that\n\nare incompatible with the corresponding formal parameter\n\ntype. Similar to other deep semantic errors, the difference\n\nin the number of errors is due to the different inference\n\nsemantics, and MyPy not checking unannotated functions.\n\nThe large number of PyType wrong-arg-types errors is\n\npartly because PyType delays the error report until the object\n\nis actually used; when the object is passed as an argument,\n\nit is used and PyType flags the error.\n\nVar-annotated: There are 2,743 MyPy var-annotated\n\nerrors but only 6 of them match with PyType errors. (var-\n\nannotated is a shallow semantic error, but since it is one of\n\nMyPy’s most frequent errors, we have included it in Fig. 8.)\n\nThe 6 matches are unrelated to the var-annotated error\n\non the same line. As shown in Sect. 2, MyPy reports var-\n\nannotated when an empty list or dictionary is assigned to a\n\nnew variable without annotation; PyType does not consider\n\nthis an error.\n\nassignment, list-item, and dict-item: As discussed in\n\nsection Sect. 2, PyType allows changing the type of a variable\n\nand the type of a collection element. Thus, these three MyPy\n\nerror codes have no matching PyType categories.\n\n5.1.2 Shallow Semantic Errors. There is little overlap\n\nbetween MyPy’s and PyType’s shallow semantic errors, as\n\nshown in Py3TypeInTheWildDLS20 (graph 3 and 4 ).\n\n85.2% of MyPy’s shallow semantic errors do not match with\n\nPyType’s errors, and 95.7% of PyType’s do not match with\n\nMyPy’s. The number of shallow semantic errors is small.\n\nThere are 2,472 and 3,132 MyPy and PyType errors respec-\n\ntively compared to 19,145 and 45,856 deep semantic errors.\n\nThere is one noticeable matching error. MyPy’s call-arg\n\nchecks the number and names of arguments at function calls.\n\nOut of 411 errors, 45 match to PyType’s missing-parameter\n\nand 29 match to wrong-arg-count.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "DLS ’20, November 17, 2020, Virtual, USA\n\nIngkarat Rak-amnouykit, Daniel McCrevan, Ana Milanova, Martin Hirzel, and Julian Dolby\n\nFigure 8. Comparison between MyPy’s and PyType’s errors that appear on the same line. Left: matches between MyPy’s\n\n19,145 deep semantic errors and PyType. Right: matches between PyType’s 45,856 deep semantic errors and MyPy.\n\nEach tool has different categories for uncommon cases.\n\nFor example, there are 3 MyPy exit-return errors that get\n\nreported because the __exit__ function that always returns\n\nFalse has return type bool. PyType does not catch this error.\n\n5.1.3 Syntax Errors. In Py3TypeInTheWildDLS20\n\n(graph 8 ), 2 of the 5 large categories, valid-type and syn-\n\ntax, account for 631 and 417 matching pairs, or 18.6% and\n\n12.3% of total matchings. They catch wrong annotation syn-\n\ntax and wrong language syntax. 97.0% of valid-type and\n\n93.0% of syntax match to one of 3 PyType categories: invalid-\n\nannotation, python-compiler-error, and name-error.\n\nFor example:\n\n1 def __init__(self, title: string,\n\n2\n\nsize: int, parent=None): ...\n\nfails both with MyPy’s valid-type and with PyType’s invalid-\n\nannotation. The tight connections between these 2 MyPy\n\nerror codes and 3 PyType error codes indicates that MyPy\n\nand PyType capture syntax errors in a similar way.\n\n5.1.4 Import Errors. Py3TypeInTheWildDLS20\n\n(graph 7 ) shows the overall relationship between MyPy and\n\nPyType errors when excluding the dominating no match\n\ncategory. Out of 3,270 MyPy misc matching errors, 84.2%\n\nand 11.7% match to PyType import-error and invalid-\n\nannotation, respectively. There are 7,839 name-defined er-\n\nrors, 2,488 of those match to PyType errors, and 97.5% match\n\nto PyType’s name-error. MyPy’s attr-defined matches\n\nwith PyType’s import-error and attribute-error for 65.7%\n\nand 20.2% respectively. Both tools are fairly consistent with\n\nreporting import-related errors.\n\n5.1.5 Conclusion. The results of the comparison support\n\nour argument from Sect. 2 that MyPy and PyType are essen-\n\ntially two different type systems. For most major program-\n\nming languages, such a finding would come as a shock, so\n\nit is somewhat surprising to find Python in this situation.\n\nOne of the lessons learned is that MyPy behaves more like a\n\ntraditional type system and Pytype behaves more like a static\n\nanalysis tool. These seem like two directions that types can\n\n\n\n\n\n\n\n\n\n\n", "Python 3 Types in the Wild: A Tale of Two Type Systems\n\nDLS ’20, November 17, 2020, Virtual, USA\n\n5.2.2 Matching Errors. Excluding import-related errors,\n\nthere are only 190 MyPy errors (out of 23,665) that match\n\nwith PyLint’s typecheck errors. Py3TypeInTheWildDLS20\n\n(graph 14 ) plots the matching MyPy errors.\n\n5.2.3 Conclusion. Generally, PyLint fails to detect deep\n\nsemantic errors. It catches some shallow semantic errors but\n\nthe overlap is relatively small. The advantage of PyLint is\n\nthat it offers quality checking which can be appealing to\n\nusers who are looking for a linter as well as some shallow\n\nsemantic checking.\n\n6 Related Work\n\nOur paper should be viewed in a tradition of studies about\n\nthe behavior of dynamic programming languages in the wild.\n\nSuch studies help guide language designers, compiler writ-\n\ners, and tool builders with data on how certain language\n\nfeatures actually get used. Holkner and Harland [11] mea-\n\nsured the dynamic behavior of Python programs, finding that\n\nuse of dynamic objects (e.g., adding fields) and dynamic code\n\n(e.g., eval) is not only prevalent during program startup but\n\ncontinues throughout program execution. Unlike our paper,\n\ntheirs does not focus on Python 3 types; it pre-dates PEP\n\n484 [18]. Richards et al. analyze runtime traces of JavaScript\n\nprograms and find that dynamic objects and eval are ram-\n\npant [16]. Bierman et al. define a core calculus for TypeScript,\n\nwhich adds types to JavaScript via a compile-and-erase ap-\n\nproach [7]. Just like with Python 3, this means type annota-\n\ntions are ignored at runtime, leading to unsoundness. The\n\ntype checker does not statically check downcasts or conver-\n\nsions to and from the Any type, nor are they checked at run-\n\ntime, and thus, the type of a runtime value of an expression\n\ncan differ from its static type. Their paper does not include\n\na corpus study. Morandat et al. study the R programming\n\nlanguage and find that besides dynamic types in the usual\n\nsense, a distinguishing feature of R is lazy evaluation [13].\n\nTo the best of our knowledge, ours is the first study of how\n\nreal-world programs use Python 3 types.\n\nThere have been multiple interesting typed Python di-\n\nalects. RPython [4] is a Python subset that was initially de-\n\nsigned for PyPy [17], a Python just-in-time compiler written\n\nin Python. RPython restricts Python’s dynamic features to\n\nsimplify type inference and enable efficient execution on the\n\nCLI and the JVM. Cython is a Python superset that is used\n\nin popular libraries for scientific computing and machine\n\nlearning [6]. Cython adds explicit type annotations to enable\n\ncompiling to efficient C code. Reticulated Python offers grad-\n\nual typing for Python: it statically type-checks code based on\n\ntype annotations, then inserts dynamic checks at the bound-\n\naries to untyped code [19]. Having been published in the\n\nsame year as PEP 484 [18], it uses different type names than\n\nthose supported by Python 3 today. Whereas these papers\n\noffer typed Python dialects, our paper studies types in the\n\nmother language from which these dialects calved.\n\nFigure 9. PyLint error code distribution of typecheck sub-\n\ncategory across 2,673 Python-3 style annotated repositories.\n\ngo for dynamic languages like Python. This leads to differ-\n\nences in reported errors, as well as in when and where errors\n\nget detected. MyPy and PyType would serve the Python\n\ncommunity better if they increased the overlap between re-\n\nported errors. We hope that the comparisons in this section\n\ncan serve as a starting point for such a convergence journey.\n\nThere is overlap in deep semantic errors arg-type/wrong-\n\narg-types and return-value/bad-return-value, but there\n\nare also significant numbers of no-matches. The tools define\n\ndifferent kinds of shallow semantic errors that generally do\n\nnot overlap. For syntax errors, the tools overlap. Lastly, Py-\n\nType reports larger numbers of errors because unlike MyPy,\n\nit also checks unannotated functions.\n\n5.2 MyPy vs. PyLint\n\nNext, we compare error messages from MyPy and PyLint.\n\nPyLint is a static analyzer that offers a wide range of checking.\n\nUnlike the other tools, PyLint checks the quality of the code;\n\nfor example, it enforces a coding standard and reports code\n\nsmells. There are about 200 error codes. We are interested\n\nin the error codes in the typecheck category, which most\n\nclosely relates to the MyPy and PyType errors we study.\n\nFig. 9 illustrates the distribution of errors in typecheck.\n\nThe largest error category, no-member, is reported when\n\nan unknown member in a variable is accessed. Similarly to\n\nMyPy, many are import-related. Moreover, no-member is\n\none of the most common false positive errors in PyLint16.\n\nThus, we excluded this category from the comparison.\n\n5.2.1 Deep Semantic Errors. Out of MyPy’s 19,145 deep\n\nsemantic errors, 89.9% do not match with PyLint’s errors and\n\n9.1% match with no-member. Py3TypeInTheWildDLS20\n\n(graph 9 ) illustrates. For the remaining 1%, there is a small\n\noverlap between MyPy’s index and PyLint’s unsubscriptable-\n\nobject which account for 0.3% of the MyPy errors.\n\n16https://github.com/PyCQA/pylint/issues/3512\n\n\n\n\n\n\n\n\n\n\n\n", "DLS ’20, November 17, 2020, Virtual, USA\n\nIngkarat Rak-amnouykit, Daniel McCrevan, Ana Milanova, Martin Hirzel, and Julian Dolby\n\nThere have been several papers on type inference for\n\nPython, in spite of (or perhaps because of?) Python’s dy-\n\nnamic features and the unsoundness of Python 3 types. Maia\n\net al.’s type inference pre-dates Python 3 type annotations\n\nand focuses on RPython instead, offering fairly standard\n\ntype rules [12]. Xu et al. augment standard type rules with a\n\nprobabilistic approach to exploiting additional information\n\nsuch as identifier naming conventions [20]. Fritz and Hage\n\nimplement type inference via abstract interpretation and\n\nexperiment with tuning the precision by modifying flow sen-\n\nsitivity and context sensitivity [9]. Hassan et al. implement\n\ntype inference via a MaxSMT solver, maximizing optional\n\nequality constraints while satisfying all mandatory type con-\n\nstraints [10]. Dolby et al. use type inference to find bugs in\n\nPython-based deep learning code by inferring tensor shapes\n\nand dimensions [8]. Allamanis et al. use deep learning to\n\nimplement Python type inference, based on manual type\n\nannotations as ground truth labels [3]. TypeWriter infers the\n\nargument types and return types of functions using prob-\n\nabilistic type prediction and search-based refinement [15].\n\nIn contrast, our paper studies how types are being used and\n\nhow MyPy and PyType (the most popular practical type\n\nchecking and inference tools) behave on a large corpus of\n\nopen-source repositories.\n\n7 Conclusion\n\nThis paper presents a study of a comprehensive corpus of\n\nopen-source code with Python 3 type annotations. The pic-\n\nture is mixed. On the one hand, types can already help catch\n\nmany bugs, such as in the use of Optional types that may\n\nbe None and in function return type annotations. On the\n\nother hand, type checking tools frequently disagree with\n\nuser annotations and with each other. Most open-source\n\nprojects do not yet use Python 3 types, and of those that\n\ndo, few type-check. We hope that our paper will help guide\n\npractitioners to make the best use of what is available today\n\nwhile inspiring researchers to improve upon the status quo.\n\nWith apologies to Dickens: It was the best of types, it was the\n\nworst of types ...\n\nReferences\n\n[1] 2020. Infer: Eradicate. https://fbinfer.com/docs/eradicate/\n\n[2] 2020. The Checker Framework. https://checkerframework.org/\n\n[3] Miltiadis Allamanis, Earl T. Barr, Soline Ducousso, and Zheng Gao.\n\n2020. Typilus: Neural Type Hints. In Conference on Programming\n\nLanguage Design and Implementation (PLDI). https://arxiv.org/abs/\n\n2004.10657\n\n[4] Davide Ancona, Massimo Ancona, Antonio Cuni, and Nicholas D.\n\nMatsakis. 2007. RPython: a Step Towards Reconciling Dynamically\n\nand Statically Typed OO Languages. In Dynamic Languages Symposium\n\n(DLS). https://doi.org/10.1145/1297081.1297091\n\n[5] Subarno Banerjee, Lazaro Clapp, and Manu Sridharan. 2019. NullAway:\n\nPractical Type-based Null Safety for Java. In Foundations of Software\n\nEngineering (FSE). 740–750. https://doi.org/10.1145/3338906.3338919\n\n[6] Stefan Behnel, Robert Bradshaw, Craig Citro, Lisandro Dalcín,\n\nDag Sverre Seljebotn, and Kurt Smith. 2011. Cython: The Best of\n\nBoth Worlds. Computing in Science and Engineering (CISE) 13, 2 (2011),\n\n31–39. https://doi.org/10.1109/MCSE.2010.118\n\n[7] Gavin Bierman, Martín Abadi, and Mads Torgersen. 2014. Understand-\n\ning TypeScript. In European Conference for Object-Oriented Program-\n\nming (ECOOP). 257–281. https://doi.org/10.1007/978-3-662-44202-\n\n9_11\n\n[8] Julian Dolby, Avraham Shinnar, Allison Allain, and Jenna Reinen. 2018.\n\nAriadne: Analysis for Machine Learning Programs. In Workshop on\n\nMachine Learning and Programming Languages (MAPL). 1–10. http:\n\n//doi.acm.org/10.1145/3211346.3211349\n\n[9] Levin Fritz and Jurriaan Hage. 2017. Cost versus Precision for Ap-\n\nproximate Typing for Python. In Workshop on Partial Evaluation and\n\nProgram Manipulation (PEPM). 89–98. https://doi.org/10.1145/3018882.\n\n3018888\n\n[10] Mostafa Hassan, Caterina Urban, Marco Eilers, and Peter Müller. 2018.\n\nMaxSMT-Based Type Inference for Python 3. In Conference on Com-\n\nputer Aided Verification (CAV). 12–19. https://doi.org/10.1007/978-3-\n\n319-96142-2_2\n\n[11] A. Holkner and J. Harland. 2009. Evaluating the dynamic behaviour\n\nof Python applications. In Australasian Computer Science Conference\n\n(ACSC). 17–25. https://crpit.scem.westernsydney.edu.au/abstracts/\n\nCRPITV91Holkner.html\n\n[12] Eva Maia, Nelma Moreira, and Rogério Reis. 2011. A Static Type Infer-\n\nence for Python. In Workshop on Dynamic Languages and Applications\n\n(DYLA). http://scg.unibe.ch/download/dyla/2011/dyla11_submission_\n\n3.pdf\n\n[13] Floréal Morandat, Brandon Hill, Leo Osvald, and Jan Vitek. 2012. Eval-\n\nuating the Design of the R Language. In European Conference for Object-\n\nOriented Programming (ECOOP). 104–131. https://doi.org/10.1007/978-\n\n3-642-31057-7_6\n\n[14] Matthew M. Papi, Mahmood Ali, Telmo Luis Correa Jr., Jeff H. Perkins,\n\nand Michael D. Ernst. 2008. Practical Pluggable Types for Java. In\n\nInternational Symposium on Software Testing and Analysis (ISSTA). 201–\n\n212. https://doi.org/10.1145/1390630.1390656\n\n[15] Michael Pradel, Georgios Gousios, Jason Liu, and Satish Chandra. 2020.\n\nTypeWriter: Neural Type Prediction with Search-Based Validation. In\n\nFoundations of Software Engineering (FSE). \"https://doi.org/10.1145/\n\n3368089.3409715\"\n\n[16] Gregor Richards, Sylvain Lebresne, Brian Burg, and Jan Vitek. 2010. An\n\nanalysis of the dynamic behavior of JavaScript programs. In Conference\n\non Programming Language Design and Implementation (PLDI). 1–12.\n\nhttps://doi.org/10.1145/1806596.1806598\n\n[17] Armin Rigo and Samuele Pedroni. 2006. PyPy’s approach to virtual\n\nmachine construction. In Dynamic Languages Symposium (DLS). 944–\n\n953. https://doi.org/10.1145/1176617.1176753\n\n[18] Guido van Rossum, Jukka Lehtosalo, and Lukasz Langa. 2014. PEP 484\n\n– Type Hints. https://www.python.org/dev/peps/pep-0484/\n\n[19] Michael M. Vitousek, Andrew M. Kent, Jeremy G. Siek, and Jim Baker.\n\n2014. Design and Evaluation of Gradual Typing for Python. In Dy-\n\nnamic Languages Symposium (DLS). 45–56. http://doi.acm.org/10.\n\n1145/2661088.2661101\n\n[20] Zhaogui Xu, Xiangyu Zhang, Lin Chen, Kexin Pei, and Baowen Xu.\n\n2016. Python Probabilistic Type Inference with Natural Language\n\nSupport. In Foundations of Software Engineering (FSE). 607–618. http:\n\n//doi.acm.org/10.1145/2950290.2950343\n\n"]], ["C:\\Users\\mjols\\Python39\\reticulate- comms btwn python and R.pdf", ["Use Python with R with reticulate : : CHEAT SHEET \n\nThe reticulate package lets you use Python and R together seamlessly in R code, in R Markdown documents, and in the RStudio IDE.\n\nPython in R Markdown\n\n(Optional) Build Python env to use. \n\nAdd knitr::knit_engines$set(python = \n\nreticulate::eng_python) to the setup \n\nchunk to set up the reticulate Python \n\nengine (not required for knitr >= 1.18). \n\nSuggest the Python environment  \n\nto use, in your setup chunk. \n\nBegin Python chunks with ```{python}. \n\nChunk options like echo, include, etc. all \n\nwork as expected.  \n\nUse the py object to access objects created \n\nin Python chunks from R chunks. \n\nPython chunks all execute within a  \n\nsingle Python session so you have access  \n\nto all objects created in previous chunks. \n\nUse the r object to access objects created \n\nin R chunks from Python chunks. \n\nOutput displays below chunk,  \n\nincluding matplotlib plots.\n\nPython in R\n\nCall Python from R code in three ways:\n\nIMPORT PYTHON MODULES\n\nUse import() to import any Python module. \n\nAccess the attributes of a module with $. \n\n• import(module, as = NULL, convert = \n\nTRUE, delay_load = FALSE) Import a \n\nPython module. If convert = TRUE, \n\nPython objects are converted to  \n\ntheir equivalent R types. Also \n\nimport_from_path. import(\"pandas\") \n\n• import_main(convert = TRUE)  \n\nImport the main module, where Python \n\nexecutes code by default. import_main() \n\n• import_builtins(convert = TRUE) \n\nImport Python's built-in functions. \n\nimport_builtins()\n\nSOURCE PYTHON FILES\n\nUse source_python() to source a Python script \n\nand make the Python functions and objects it \n\ncreates available in the calling R environment. \n\n• source_python(file, envir = parent.frame(), \n\nconvert = TRUE) Run a Python script, \n\nassigning objects to a specified R \n\nenvironment. source_python(\"file.py\")\n\nRUN PYTHON CODE\n\nExecute Python code into the main Python \n\nmodule with py_run_file() or py_run_string(). \n\n• py_run_string(code, local = FALSE, \n\nconvert = TRUE) Run Python code \n\n(passed as a string) in the main  \n\nmodule. py_run_string(\"x = 10\"); py$x \n\n• py_run_file(file, local = FALSE, convert = \n\nTRUE) Run Python file in the main \n\nmodule. py_run_file(\"script.py\") \n\n• py_eval(code, convert = TRUE) Run  \n\na Python expression, return the result. \n\nAlso py_call. py_eval(\"1 + 1\") \n\nAccess the results, and anything else in Python's \n\nmain module, with py. \n\n• py An R object that contains  \n\nthe Python main module and  \n\nthe results stored there. py$x\n\nObject Conversion\n\nTip: To index Python objects begin at 0, use integers, e.g. 0L\n\nHelpers\n\nReticulate provides automatic built-in conversion \n\nbetween Python and R for many Python types.\n\ndict(..., convert = FALSE) Create a Python dictionary \n\nobject. Also py_dict to make a dictionary that uses \n\nPython objects as keys. dict(foo = \"bar\", index = 42L) \n\npy_capture_output(expr, type = c(\"stdout\", \n\n\"stderr\")) Capture and return Python output. Also \n\npy_suppress_warnings. py_capture_output(\"x\") \n\nR\n\nPython\n\nSingle-element vector\n\nMulti-element vector\n\nList of multiple types\n\nScalar\n\nList\n\nTuple\n\nNamed list\n\nMatrix/Array\n\nData Frame\n\nFunction\n\nNULL, TRUE, FALSE\n\nDict\n\nNumPy ndarray\n\nPandas DataFrame\n\nPython function\n\nNone, True, False\n\nOr, if you like, you can convert manually with\n\npy_to_r(x) Convert a Python object to \n\nan R object. Also r_to_py.  py_to_r(x) \n\ntuple(..., convert = FALSE) Create a \n\nPython tuple. tuple(\"a\", \"b\", \"c\")\n\nnp_array(data, dtype = NULL, order = \"C\") Create \n\nNumPy arrays. np_array(c(1:8), dtype = \"float16\") \n\narray_reshape(x, dim, order = c(\"C\", \"F\")) Reshape \n\na Python array. x <- 1:4; array_reshape(x, c(2, 2)) \n\npy_func(object) Wrap an R function in a Python \n\nfunction with the same signature. py_func(xor) \n\npy_main_thread_func(object) Create a function \n\nthat will always be called on the main thread.  \n\niterate(..., convert = FALSE) Apply an R function to \n\neach value of a Python iterator or return the values \n\nas an R vector, draining the iterator as you go. Also \n\niter_next and as_iterator. iterate(iter, print) \n\npy_iterator(fn, completed = NULL) Create a Python \n\niterator from an R function. seq_gen <- function(x){n \n\n<- x; function() {n <<- n + 1; n}};py_iterator(seq_gen(9))\n\npy_get_attr(x, name, silent = FALSE) Get an \n\nattribute of a Python object. Also py_set_attr, \n\npy_has_attr, and  py_list_attributes. py_get_attr(x) \n\npy_help(object) Open the documentation  \n\npage for a Python object. py_help(sns) \n\npy_last_error() Get the last Python  \n\nerror encountered. Also py_clear_last_error  \n\nto clear the last error. py_last_error() \n\npy_save_object(object, filename, pickle = \"pickle\") \n\nSave and load Python objects with pickle. Also \n\npy_load_object. py_save_object(x, \"x.pickle\") \n\nwith(data, expr, as = NULL, ...) Evaluate an \n\nexpression within a Python context manager.  \n\npy <- import_builtins(); with(py$open(\"output.txt\", \n\n\"w\") %as% file, { file$write(\"Hello, there!\")})\n\nRStudio® is a trademark of RStudio, Inc.  •  CC BY SA  RStudio •  info@rstudio.com  •  844-448-1212 • rstudio.com •  Learn more at rstudio.github.io/reticulate/ • reticulate  1.12.0 •   Updated: 2019-04\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "Python in the IDE Requires reticulate plus RStudio v1.2 or higher.\n\nSyntax \n\nhighlighting for \n\nPython scripts \n\nand chunks\n\nTab completion for Python \n\nfunctions and objects (and \n\nPython modules imported \n\nin R scripts)\n\nSource \n\nPython \n\nscripts.\n\nExecute Python \n\ncode line by line \n\nwith Cmd +  Enter \n\n(Ctrl + Enter)\n\nPress F1 over a \n\nPython symbol \n\nto display the \n\nhelp topic for \n\nthat symbol.\n\nConfigure Python\n\nmatplotlib \n\nplots display \n\nin plots pane.\n\nReticulate binds to a local instance of Python when you first call import() directly \n\nor implicitly from an R session. To control the process, find or build your desired \n\nPython instance. Then suggest your instance to reticulate. Restart R to unbind.\n\nFind Python\n\n• py_discover_config() Return all detected \n\nversions of Python. Use py_config to check \n\nwhich version has been loaded. py_config() \n\n• py_available(initialize = FALSE) Check if  \n\nPython is available on your system. Also \n\npy_module_available, py_numpy_module. \n\npy_available()\n\nCreate a Python env\n\n• virtualenv_create(envname) Create a new \n\nvirtualenv. virtualenv_create(\"r-pandas\") \n\n• conda_create(envname, packages = NULL, \n\nconda = \"auto\") Create a new Conda env. \n\nconda_create(\"r-pandas\", packages = \n\n\"pandas\")\n\nInstall Packages\n\nInstall Python packages with R (below) or the shell: \n\npip install SciPy \n\nconda install SciPy\n\n• py_install(packages, envname = \"r-\n\nreticulate\", method = c(\"auto\", \"virtualenv\", \n\n\"conda\"), conda = \"auto\", ...) Installs Python \n\npackages into a Python env named  \n\n\"r-reticulate\". py_install(\"pandas\") \n\n• virtualenv_install(envname, packages, \n\nignore_installed = FALSE) Install a package \n\nwithin a virtualenv. virtualenv_install( \n\n\"r-pandas\", packages = \"pandas\") \n\n• virtualenv_remove(envname, packages = \n\nNULL, confirm = interactive()) Remove \n\nindividual packages or an entire virtualenv. \n\nvirtualenv_remove(\"r-pandas\", packages = \n\n\"pandas\") \n\n• conda_install(envname, packages, forge = \n\nTRUE, pip = FALSE, pip_ignore_installed = \n\nTRUE, conda = \"auto\") Install a package \n\nwithin a Conda env. conda_install( \n\n\"r-pandas\", packages = \"plotly\") \n\n• conda_remove(envname, packages = NULL, \n\nconda = \"auto\") Remove individual packages \n\nor an entire Conda env. conda_remove( \n\n\"r-pandas\", packages = \"plotly\")\n\n• virtualenv_list() List all available virtualenvs.  \n\nAlso virtualenv_root(). virtualenv_list() \n\n• conda_list(conda = \"auto\") List all  \n\navailable conda envs. Also conda_binary() \n\nand conda_version(). conda_list()\n\nSuggest an env to use\n\nTo choose an instance of Python to bind to, \n\nreticulate scans the instances on your computer in \n\nthe following order, stopping at the first instance \n\nthat contains the module called by import().\n\n1. The instance referenced by the environment  \n\nvariable RETICULATE_PYTHON (if specified).  \n\nTip: set in .Renviron file. \n\n• Sys.setenv(RETICULATE_PYTHON = PATH)  \n\nSet default Python binary. Persists across \n\nsessions! Undo with Sys.unsetenv. \n\nSys.setenv(RETICULATE_PYTHON =  \n\n\"/usr/local/bin/python\") \n\n2. The instances referenced by use_ functions  \n\nif called before import(). Will fail silently if \n\ncalled after import unless required = TRUE. \n\n• use_python(python, required = FALSE)  \n\nSuggest a Python binary to use by path.  \n\nuse_python(\"/usr/local/bin/python\") \n\n• use_virtualenv(virtualenv = NULL, \n\nrequired = FALSE) Suggest a Python  \n\nvirtualenv. use_virtualenv(\"~/myenv\") \n\n• use_condaenv(condaenv = NULL,  \n\nconda = \"auto\", required = FALSE)  \n\nSuggest a Conda env to use. \n\nuse_condaenv(condaenv = \"r-nlp\",  \n\nconda = \"/opt/anaconda3/bin/conda\") \n\n3. Within virtualenvs and conda envs that carry  \n\nthe same name as the imported module.  \n\ne.g. ~/anaconda/envs/nltk for import(\"nltk\") \n\n4. At the location of the Python binary \n\ndiscovered on the system PATH \n\n(i.e. Sys.which(\"python\")) \n\n5. At customary locations for Python, e.g./usr/\n\nlocal/bin/python, /opt/local/bin/python...\n\nRStudio® is a trademark of RStudio, Inc.  •  CC BY SA  RStudio •  info@rstudio.com  •  844-448-1212 • rstudio.com •  Learn more at rstudio.github.io/reticulate/ • reticulate  1.12.0 •   Updated: 2019-04\n\nA Python REPL opens in the console when you run Python code with a keyboard shortcut. Type exit to close.\n\nPython REPL\n\nA REPL (Read, Eval, Print Loop) is a command \n\nline where you can run Python code and view \n\nthe results. \n\n1. Open in the console with repl_python(), \n\nor by running code in a Python script with \n\nCmd + Enter (Ctrl + Enter) . \n\n• repl_python(module = NULL, quiet = \n\ngetOption(\"reticulate.repl.quiet\", \n\ndefault = FALSE)) Launch a Python \n\nREPL. Run exit to close. repl_python()  \n\n2. Type commands at >>> prompt \n\n3. Press Enter to run code \n\n4. Type exit to close and return to R console\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"]], ["C:\\Users\\mjols\\Python39\\silly_html_example.pdf", ["silly_html_example\n\nFebruary 5, 2021\n\nhello world\n\nYO DAWG WHAT WE DO???!!!\n\nok, I'm calm, i'm calm</li>\n\n1\n\n"]], ["C:\\Users\\mjols\\Python39\\writing_python_libraries.pdf", ["Writing Python \n\nLibraries\n\nImport Statements and Packaging\n\n\n", "Basics\n\nA Python ﬁle is called either a script or a module, depending on how it’s run: \n\n• Script: Run ﬁle as a top-level script\n\n- python\tfile.py\t\n\n- __name__\t==\t“__main__” \n\nField containing module name\n\n• Module: Import ﬁle as a module\n\n- python\t-m\tpackage.file\t\n\n- import\tpackage.file\t(inside some other ﬁle)\n\n- __name__\t==\t“package.file”\t\n\nRun a ﬁle as a module\n\nName depends on root package\n\nPython packages are collections of modules. In a directory structure, in order for \n\na folder containing Python ﬁles to be recognized as a package, an __init__.py \n\nﬁle is needed (even if empty). \n\nIf a module's name has no dots, it is not considered to be part of a package.\n\n\n\n\n\n\n\n\n", "Package Basics\n\nPython packages are collections of modules. In a directory structure, in order for \n\na folder containing Python ﬁles to be recognized as a package, an __init__.py \n\nﬁle is needed (even if empty). \n\nCannot be accessed from root directory  \n\nusing non_package.module1\n\nCan be accessed from root directory  \n\nusing package.subpackage.module3\n\n\n\n\n\n\n\n", "Installable Packages\n\nThen the package can be installed by running: \n\n• python\tsetup.py\tinstall\n\n- This command will install the package in the site-packages directory of the \n\ncurrent Python distribution so it can be imported in any Python ﬁle using \n\nsimply:\timport\tproject \n\n• python\tsetup.py\tdevelop\n\n- This command will install symbolic links to the current package source \n\ncode in the site-packages directory of the current Python distribution so it \n\ncan be imported in any Python ﬁle using simply:\timport\tproject\n\n- Any changes made to the local project ﬁles, will be reﬂected in the \n\ninstalled version of the project \n\nThe --user option can optionally be used to install in the current user site-\n\npackages directory instead of the system site-packages directory.\n\n\n", "Import Basics\n\nPackages and modules can be imported in other Python ﬁles. Absolute imports \n\nare relative to every path in the module search path (sys.path) for the packages \n\nalong with the current directory. \n\nmodule2 shall use:  \n\nimport\tmodule1\n\nmodule1 shall use:  \n\nimport\tsubpackage.module3\n\n\n\n\n\n\n\n", "Relative Imports\n\n• Relative imports use the module's name to determine where it is in a package. \n\nIf __name__\t==\t“package.subpackage.module”, then: \n\nfrom\t..\timport\tother, resolves to a module with  \n\n__name__\t==\t“package.other” \n\n• __name__\tmust have at least as many dots as there are in the import \n\nstatement. \n\n• If __name__ has no dots (“__main__”), then a  \n\n“relative-import\tin\tnon-package” error is raised.\t\n\nIf you use relative imports in a Python ﬁle and you want to run it use the \n\ncommand: python\t-m\tpackage.subpackage.module\n\n\n\n\n\n\n", "Package Name Space\n\nWhen a Python package is imported, we want to be able to deﬁne its name \n\nspace. This is the set of names (modules, packages, functions, ﬁelds, or \n\nclasses) that this package contains. \n\nSometimes we might want to expose names of a sub-package to the root \n\npackage, for convenience. For example: numpy.core.ndarray\t->\tnumpy.ndarray\t\n\nWe can do that using: \n\n• __all__ ﬁeld of modules \n\n• __init__.py ﬁle of packages \n\nCare must always be taken to prevent name space pollution and collisions    \n\n(i.e., overloaded names).\n\n\n\n\n\n\n", "__all__ Field\n\nThe __all__.py ﬁeld can be used to specify which symbols of a module to \n\nexport. The exported symbols are the ones imported when * is used. \n\nIf omitted, all names not starting with an underscore (_) are exported. \n\nmodule.py\t\n\nImports fn1, fn2, fn3  \n\nif __all__ is omitted\n\nImports fn1, fn2 \n\nif __all__ is speciﬁed\n\n\n\n\n\n\n\n\n", "__init__.py\n\nThe __init__.py ﬁle can be used to export module or sub-package symbols to \n\nthe package namespace. \n\nCommon Pattern\n\nExposes module3 to  \n\nthe package name  \n\nspace\n\n\n\n\n\n\n\n\n", "Common Practices\n\n• Python project directory structure: \n\nAs we will soon see, this structure \n\nalso helps with making our libraries \n\ninstallable.\n\n• Add an __author__ ﬁeld to each ﬁle with the author’s name/ID. This helps with \n\nknowing who to contact when questions/bugs arise with the relevant ﬁle. \n\n• Add TODO items in the code using a comment line with format: \n\n#\tTODO(author):\tThis\tneeds\tto\tbe\tdone.\n\n\n\n\n\n\n\n\n", "Installable Packages\n\nWe often want to make our packages/libraries installable for distribution or for \n\ninstalling them on a production server. We can do that using the setuptools \n\npackage. Simply add a setup.py script in the project’s root directory:\n\nExample / Template\n\npackage location mapping\n\npackage dependencies\n\n\n\n\n\n\n\n\n", "References\n\n• Ofﬁcial Python documentation at http://docs.python.org \n\n•\n\nhttps://stackoverﬂow.com/questions/14132789/relative-imports-for-the-billionth-time\n\n\n\n\n"]], ["C:\\Users\\mjols\\Python39\\delicious\\silly_html_example.pdf", ["silly_html_example\n\nFebruary 5, 2021\n\nhello world\n\nYO DAWG WHAT WE DO???!!!\n\nok, I'm calm, i'm calm</li>\n\n1\n\n"]], ["C:\\Users\\mjols\\Python39\\Lib\\site-packages\\matplotlib\\mpl-data\\images\\back.pdf", ["\n"]], ["C:\\Users\\mjols\\Python39\\Lib\\site-packages\\matplotlib\\mpl-data\\images\\filesave.pdf", ["\n\n\n\n"]], ["C:\\Users\\mjols\\Python39\\Lib\\site-packages\\matplotlib\\mpl-data\\images\\forward.pdf", ["\n"]], ["C:\\Users\\mjols\\Python39\\Lib\\site-packages\\matplotlib\\mpl-data\\images\\hand.pdf", ["\n\n"]], ["C:\\Users\\mjols\\Python39\\Lib\\site-packages\\matplotlib\\mpl-data\\images\\help.pdf", ["\n\n"]], ["C:\\Users\\mjols\\Python39\\Lib\\site-packages\\matplotlib\\mpl-data\\images\\home.pdf", ["\n\n"]], ["C:\\Users\\mjols\\Python39\\Lib\\site-packages\\matplotlib\\mpl-data\\images\\matplotlib.pdf", ["\n\n\n\n\n\n\n\n\n"]], ["C:\\Users\\mjols\\Python39\\Lib\\site-packages\\matplotlib\\mpl-data\\images\\move.pdf", ["\n"]], ["C:\\Users\\mjols\\Python39\\Lib\\site-packages\\matplotlib\\mpl-data\\images\\qt4_editor_options.pdf", ["\n\n"]], ["C:\\Users\\mjols\\Python39\\Lib\\site-packages\\matplotlib\\mpl-data\\images\\subplots.pdf", ["\n\n\n\n\n\n\n\n\n"]], ["C:\\Users\\mjols\\Python39\\Lib\\site-packages\\matplotlib\\mpl-data\\images\\zoom_to_rect.pdf", ["\n\n"]], ["C:\\Users\\mjols\\Documents\\DS UWEC courses\\740 data mining\\lesson3\\Cafe_correlation.pdf", ["l\n\nd\n\no\n\nS\n\n.\n\nd\n\nn\n\na\n\nS\n\n.\n\nd\n\na\n\ne\n\nr\n\nB\n\nl\n\nd\n\no\n\nS\n\n.\n\ns\n\np\n\na\n\nr\n\nW\n\nl\n\nd\n\no\n\nS\n\n.\n\np\n\nu\n\nC\n\n.\n\nt\n\ni\n\nu\n\nr\n\nF\n\nl\n\nd\n\no\n\nS\n\n.\n\ns\n\ne\n\nk\n\no\n\no\n\nC\n\ni\n\np\n\nm\n\ne\n\nT\n\n.\n\nx\n\na\n\nt M\n\ne\n\ne\n\nf\n\nf\n\no\n\nC\n\n.\n\nd\n\nn\n\na\n\n.\n\na\n\nd\n\no\n\nS\n\n.\n\nl\n\na\n\nt\n\no\n\nT\n\ns\n\na\n\nd\n\no\n\nS\n\nl\n\ns\n\ne\n\na\n\nS\n\ne\n\nt\n\ns\n\na\n\nW\n\n.\n\np\n\nu\n\nC\n\n.\n\nt\n\ni\n\nu\n\nr\n\nF\n\ne\n\nt\n\ns\n\na\n\nW\n\n.\n\ns\n\ne\n\nk\n\no\n\no\n\nC\n\ni\n\ne\n\nt\n\ns\n\na\n\nW\n\n.\n\ns\n\np\n\na\n\nr\n\nW\n\nd\n\ne\n\nt\n\ns\n\na\n\nW\n\n.\n\ns\n\nm\n\ne\n\nt\n\nI\n\n.\n\nl\n\na\n\nt\n\no\n\nT\n\ne\n\nt\n\ns\n\na\n\nW\n\n.\n\nd\n\nn\n\na\n\nS\n\n.\n\nd\n\na\n\ne\n\nr\n\nB\n\ne\n\nt\n\ns\n\na\n\nW\n\n.\n\ns\n\nn\n\ni\n\nf\n\nf\n\nu\n\nM\n\nl\n\nd\n\no\n\nS\n\n.\n\ns\n\nn\n\ni\n\nf\n\nf\n\nu\n\nM\n\ne\n\nd\n\no\n\nC\n\n.\n\ny\n\na\n\nD\n\ns\n\ne\n\nc\n\nu\n\nJ\n\ni\n\ns\n\ne\n\ne\n\nf\n\nf\n\no\n\nC\n\ni\n\ns\n\np\n\nh\n\nC\n\nt\n\nMax.Temp\n\nFruit.Cup.Sold\n\nCookies.Sold\n\nBread.Sand.Sold\n\nWraps.Sold\n\nSodas\n\nTotal.Soda.and.Coffee\n\nSales\n\nFruit.Cup.Waste\n\nCookies.Waste\n\nWraps.Waste\n\nTotal.Items.Wasted\n\nBread.Sand.Waste\n\nMuffins.Waste\n\nMuffins.Sold\n\nJuices\n\nDay.Code\n\nChips\n\nCoffees\n\n1\n\n0.75\n\n0.5\n\n0.25\n\n0\n\n−0.25\n\n−0.5\n\n−0.75\n\n−1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"]], ["C:\\Users\\mjols\\Documents\\DS UWEC courses\\740 data mining\\lesson3\\ds740_lesson3_presentation1.pdf", [" \n\n \n\n \n\nDS 740\n\nData Mining\n\nGeneralized Linear Models\n\nLogistic Regression and ROC Curves\n\n \n\n \n\nImportant note: Transcripts are not substitutes for textbook assignments. \n\n1 \n\n \n\n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "Learning Objectives\n\nBy the end of this lesson, you will be able to:\n\n• Perform cross-validation to assess a logistic regression model.\n\n• Create and interpret a ROC curve.\n\n• Decide when logistic regression is an appropriate method.\n\n• Describe when you would use Poisson regression.\n\n \n\n \n\n \n\n \n\n \n\n2 \n\n\n\n\n", "Generalized Linear Model\n\n! ! = !!  +  !!!!  + ⋯ +  !!!!   +  !\t\n\nNoise term\n\n! ! = !!  +  !!!!\n\n! ! = !!  +  !!!!!!   +  !\t\n\n!   +  !\t\n\n \n\n \n\nRecall from data science 705, that a generalized linear model is a model with \n\nthe form shown here. Where the X sub I's are the predictor variables and the \n\nbetas are the coefficients. \n\n \n\nEpsilon is a random term representing noise or the effects of other variables \n\nwhich were unmeasured. \n\n \n\nIt's called a generalized linear model because this model is linear with respect \n\nto the betas, not the X sub I's. That means that we can take powers of various \n\npredictor variables or products of predictor variables and we still have a \n\ngeneralized linear model. \n\n \n\nF of X is a function of the response variable. For a simple linear model, F of X \n\njust represents Y, the value of the response.  \n\n \n\n \n\n \n\n \n\n3 \n\n\n\n\n\n\n\n\n\n", "Logistic Regression\n\nlog\n\n!(!)\n\n1 − !(!)\n\n=   !!  +  !!!!  + ⋯ +  !!!!   +  !\t\n\n! ! =\n\n!!!!!!!!!⋯!!!!!\n\n1 + !!!!!!!!!⋯!!!!!\n\n\t\n\n \n\nIn logistic regression, F of X is the logit or the log odds were P of X represents \n\nthe probability that the point will take on a response value of one. That means \n\nthat the estimated probability, based on the values of the predictors, takes the \n\nform shown here.  \n\n \n\n \n\n \n\n \n\n \n\n4 \n\n\n\n\n\n\n\n\n\n\n", "Logistic Regression in R\n\nlibrary(ISLR)\n\nfit = glm(default ~ student + balance,\n\ndata=Default, family=\"binomial\")\n\nsummary(fit)\n\nLogistic Regression\n\n \n\n \n\nTo perform logistic regression in R, we use the GLM function with the \n\nargument family equals binomial. The GLM function will be loaded \n\nautomatically when you start R. But in this case, we're also loading an \n\nadditional library, ISLR which contains the default data set. So we can do an \n\nanalysis on the probability that someone will default on their credit card debt.  \n\n \n\nNotes: \n\n \n\nlibrary(ISLR) \n\nfit = glm(default ~ student + balance, \n\n          data = Default, family = \"binomial\") \n\nsummary(fit) \n\n \n\n \n\n \n\n \n\n \n\n5 \n\n\n\n\n", "Logistic Regression in R\n\nlibrary(ISLR)\n\nfit = glm(default ~ student + balance,\n\ndata = Default, family=\"binomial\")\n\nsummary(fit)\n\nLogistic Regression\n\n \n\n \n\nThe first argument in the GLM function is a formula for the analysis we want to \n\ndo. The variable before the tilde or squiggle is the response variable. Because \n\nthis is logistic regression, it takes on values of zero or one or yes or no. \n\n \n\nThe plus sign between the two predictor variables indicates that we're not \n\ninterested in analyzing an interaction between these predictors. If we did want \n\nto include an interaction term, then we would use a star or multiplication \n\nsymbol.  \n\n \n\nNotes: \n\n \n\nlibrary(ISLR) \n\nfit = glm(default ~ student + balance, \n\n          data = Default, family = \"binomial\") \n\nsummary(fit) \n\n \n\n \n\n \n\n \n\n \n\n6 \n\n\n\n\n\n", "Specifying the data set\n\nlibrary(ISLR)\n\nfit = glm(default ~ student + balance,\n\ndata=Default, family=\"binomial\")\n\nsummary(fit)\n\nuse this\n\nnot this\n\nnew_point doesn’t contain \n\nDefault$student\n\npredict( ) will ignore newdata\n\n \n\n \n\nThe argument data equals default specifies the name of the data frame where \n\nthe variables are stored. This argument is technically optional. We could omit it \n\nand instead use dollar sign notation, for example, default $student to specify \n\nthat we want our model to include the student column in the default data \n\nframe.  \n\n \n\nHowever, using dollar sign notation when we're specifying a model can create \n\nproblems with predictions later on. For example, suppose we want to make a \n\nprediction for a new data point which I'm storing in the variable newPoint \n\nthat's a student with a balance of $500. Well, the newPoint variable doesn't \n\ncontain a column called default $student, which is what the model is \n\nexpecting.  \n\n \n\nThis means that the predict function will ignore its new data argument and \n\nsimply return predictions for all of the data points in the original default data \n\nset. This can create inaccuracies in your cross-validation that are very hard to \n\ndebug. So to make your life easier, always use data equals instead of dollar sign \n\nnotation to specify the locations of your variables. This is true not just for \n\nlogistic regression, but for all the models that will use this semester. \n\n \n\n \n\n \n\n \n\n \n\n7 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "Summary(fit)\n\n!\t\n\nCoefficients:\n\nEstimate Std. Error z value Pr(>|z|)\n\n(Intercept) -1.075e+01  3.692e-01 -29.116  < 2e-16 ***\n\nstudentYes\n\n-7.149e-01  1.475e-01  -4.846 1.26e-06 ***\n\nbalance      5.738e-03  2.318e-04  24.750  < 2e-16 ***\n\n---\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 '' 1\n\n! ! =\n\n!!!\".!\"!.!\"#$∗!\"#$%&\"!.!!\"#$%∗!\"#\"$%&\n\n1 + !!!\".!\"!.!\"#$∗!\"#$%&\"!.!!\"#$%∗!\"#\"$%&\t\n\n> contrasts(Default$default)\n\nyes\n\nNo    0\n\nYes   1\n\n \n\n \n\nBy using the summary function, we get the following output for the logistic \n\nregression. The first column contains the estimated coefficients, or betas, \n\nwhich we can use to get a formula for the estimated probability of taking on a \n\nresponse value of 1. We can double check how R is interpreting the response \n\nvariable by using the contrast function. \n\n \n\nIn this case, we see that when the variable default, with a lowercase d, is equal \n\nto the word yes, R interprets that as a response value of 1. So, for example, the \n\nnegative coefficient for student yes means that being a student is associated \n\nwith a lower probability of defaulting on credit card debt. \n\n \n\n \n\n \n\n8 \n\n\n\n\n\n\n\n\n\n\n", " \n\nLogistic Regression in R\n\n!\t\n\nCoefficients:\n\np-value of H0: β = 0\n\nEstimate Std. Error z value Pr(>|z|)\n\n(Intercept) -1.075e+01  3.692e-01 -29.116  < 2e-16 ***\n\nstudentYes\n\n-7.149e-01  1.475e-01  -4.846 1.26e-06 ***\n\nbalance      5.738e-03  2.318e-04  24.750  < 2e-16 ***\n\n---\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 '' 1\n\n! ! =\n\n!!!\".!\"!.!\"#$∗!\"#$%&\"!.!!\"#$%∗!\"#\"$%&\n\n1 + !!!\".!\"!.!\"#$∗!\"#$%&\"!.!!\"#$%∗!\"#\"$%&\t\n\n \n\nThe fourth column of the output gives the P value for a hypothesis test where \n\nthe null hypothesis is that that coefficient is equal to zero. And the alternative \n\nhypothesis is that the coefficient is not equal to zero. \n\n \n\nIn this case, the P values for both student yes and balance are very small \n\nindicating that we should reject the null hypothesis. So we have strong \n\nevidence that both being a student and the balance on your credit card is \n\nassociated with the probability of defaulting.  \n\n \n\n \n\n \n\n9 \n\n\n\n\n\n\n\n\n\n\n\n", "Plot of  !(!)\t\n\n \n\n \n\n \n\nHere's what it looks like if we plot the estimated probabilities. The R code to \n\nmake this plot is shown below. \n\n \n\nNotes: \n\n \n\nR Code: \n\nlibrary(ggformula) \n\nlibrary(dplyr) \n\n \n\nDefault2 <- Default %>% \n\n  mutate(pred = predict(fit, type = \"response\"), # Add a column of predictions \n\n         default01 = ifelse(default == \"Yes\", 1, 0) # Make a numeric version of \n\n`default` \n\n         ) \n\n \n\nDefault2 %>% \n\n  gf_line(pred ~ balance, color =~ student, lty =~ student, lwd = 1.3) %>% \n\n  gf_point(default01 ~ balance, color =~ student, shape =~ student, size = 2) \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n10 \n\n\n\n\n\n\n", "Cross-Validation of Logistic Regression 1\n\nn = dim(Default)[1]\n\nngroups = 10\n\ngroups = rep(1:ngroups, length = n)\n\n⬅ Number of observations\n\n⬅ Using 10-fold CV\n\nset.seed(123)\n\ncvgroups = sample(groups, n)\n\nall_predicted = numeric(length = n)\n\n⬅ Ensures replicable results\n\n⬅ places observations in random groups\n\n⬅ space to store predictions\n\nfor(ii in 1:ngroups){\n\n⬅ Iterate for each fold\n\ngroupii = (cvgroups == ii)\n\n⬅ All observations in each  \n\ngroup created previously\n\nWithin Loop: Fit model to training set and make predictions.\n\nCode is different than for linear regression and k-nearest neighbors.\n\n}\n\n \n\n \n\nJust like with linear regression and k-nearest neighbors, we can use cross-\n\nvalidation to estimate the error rate of logistic regression on new data points \n\nwhich were not used to build the model. The structure of the cross-validation \n\nis going to be exactly the same as what you've seen before. We start by setting \n\nn equal to the number of observations or data points. In this case, it's the \n\nnumber of rows of the default data set. n groups is equal to 10 for 10-fold \n\ncross-validation. And we've created a groups vector to store copies of the \n\nnumbers from 1 to 10.  \n\n \n\nThen I have set.seed to ensure that my results are replicable. And I'm using the \n\nsample function to randomly reorder the groups vector. So this is placing the \n\nobservations into random groups.  \n\n \n\nThen I'm using the function numeric with length equal to n to create an empty \n\nvector where I can store the predicted probabilities of each data point \n\nbelonging to category one. Then I have a for loop where I'm going to iterate \n\nover the values of ii from 1 up to 10. Then I'm using group ii equals \n\nparentheses cv groups double equals sign ii. So this is testing, which values of \n\ncv groups are equal to the current iteration index, say, 1, 2, 3, on up to 10, and \n\ncreating a vector of trues and falses. So the variable group ii is equal to true for \n\nall of the observations that are in the ii-th fold.  \n\n \n\nThen we have space within for loop where we're going to fit the model to the \n\ntraining set and make predictions on the test set. This is the place where our \n\ncode is different than for linear regression or k-nearest neighbors. \n\n \n\n \n\n \n\n \n\n11 \n\n\n\n\n\n\n\n\n\n\n\n", " \n\nNotes: \n\n \n\nn = dim(Default)[1] \n\nngroups = 10 # using 10-fold cross-validation \n\ngroups = rep(1:ngroups, length = n) \n\n \n\nset.seed(123) \n\ncvgroups = sample(groups, n) \n\nall_predicted = numeric(length = n) \n\n \n\nfor(ii in 1:ngroups){ \n\ngroupii = (cvgroups == ii) \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n12 \n\n", " \n\n \n\n \n\nInside the for loop, after we create the group ii vector containing trues and \n\nfalses to tell which data points are in the current fold, we use that to create the \n\ntraining set and test set. The training set is all the rows of the default data \n\nframe that are not in the current fold. And the test set is all the rows that are in \n\nthe current fold.  \n\n \n\nNow we're ready to fit the logistic regression model. We do this using GLM just \n\nlike we did before we started using cross-validation. The only thing that \n\nchanges is that we're now fitting the model to just the training set. Then we \n\nuse the predict function to make predictions for the test set.  \n\n \n\nAnd because this is logistic regression, we use the additional argument type \n\nequals response to get our predictions in the form of predicted probabilities \n\nrather than predicted log odds. And finally, we store those predictions in the \n\nvector that we initialized before the start of for loop. Here we're storing it in \n\nthe positions that correspond to the data points that are in the current fold, \n\nsquare bracket group ii.  \n\n \n\nAn alternative way to specify which data we want to use to build the model \n\nwould be to use data equals default, the whole data set, and the additional \n\nargument subset equals exclamation point group ii. This works for logistic \n\nregression as well as several other types of models, such as linear regression. \n\nHowever, there are some types of models that we'll cover in this course, such \n\nas boosted trees, that don't accept subset as an argument. So I think it's easier \n\nto simply always use data equals the training set so I don't have to remember \n\nwhich methods do and don't accept the subset argument.  \n\n \n\n \n\n \n\n13 \n\n\n", " \n\nNotes: \n\n \n\nfor(ii in 1:ngroups){ \n\n  groupii = (cvgroups == ii) \n\n  train_set = Default[!groupii, ] \n\n  test_set = Default[groupii, ] \n\n   \n\n  model_fit = glm(default ~ student + balance, \n\n            data = train_set, family=\"binomial\") \n\n  predicted = predict(model_fit, newdata = test_set, \n\n                                type=\"response\") \n\n  all_predicted[groupii] = predicted \n\n} \n\n \n\n \n\n \n\n \n\n \n\n14 \n\n", "Power and False Positive Rate: Predicted Probabilities\n\n> all_predicted[1:3]\n\n[1] 0.001215781 0.001154122 0.010201310\n\n> table(all_predicted > 0.5, Default$default)\n\nDefaulted?\n\nNo  Yes\n\nFALSE 9629  229\n\nTRUE    38  104\n\nPredicted\n\nto default?\n\n \n\nThe predict function for logistic regression returns predicted probabilities. If \n\nwe want hard and fast predictions of yes or no, zero or one, we could use a \n\nthreshold value. \n\n \n\nFor example, if the probability is greater than 0.5, we could predict that the \n\nperson will default. That produces a confusion matrix like the one shown here. \n\n \n\n \n\n \n\n \n\n \n\n \n\n15 \n\n\n\n\n", "Power and False Positive Rate\n\n> all_predicted[1:3]\n\n[1] 0.001215781 0.001154122 0.010201310\n\n> table(all_predicted > 0.5, Default$default)\n\nPredicted\n\nto default?\n\nDefaulted?\n\nNo  Yes\n\nFALSE 9629  229\n\nTRUE    38  104\n\n⬆ ⬆\n\nFalse positive rate = 38/(38+9629)\n\nTrue positive rate = 104/(104+229)\n\n1-Specificity\n\nAKA: Power, Sensitivity\n\n \n\n \n\nSuppose we had a test that came back positive for each person whose \n\npredicted to default, sort of like a medical test that comes back positive for \n\neach person who is predicted to have cancer. We could measure the success \n\nof such a test using the true positive rate. \n\n \n\nThis is saying out of all the people who really did default, what fraction were \n\nwe successful at predicting? \n\n \n\nAnother way to phrase the true positive rate is the power or the sensitivity of \n\nthe test. \n\n \n\nWe could also look at the false positive rate. This is looking at out of all people \n\nwho did not default, what fraction were we unsuccessful at predicting. It's \n\ncalled a false positive because the test came back positive, predicting that they \n\ndefaulted, but it was false. That is, they really didn't default. \n\n \n\nAnother way to phrase the false positive rate is as one minus the specificity of \n\nthe test.  \n\n \n\n \n\n \n\n \n\n16 \n\n\n\n\n", "Power and False Positive Rates at Different Thresholds\n\n> all_predicted[1:3]\n\n[1] 0.001215781 0.001154122 0.010201310\n\n> table(all_predicted > 0.5, Default$default)\n\nNo  Yes\n\nFALSE 9629  229\n\nTRUE    38  104\n\nFalse positive and true positive \n\nrates at 0.5 threshold.\n\nWhat about rates at multiple thresholds?\n\n \n\n \n\nThe Receiver-Operator Characteristic curve, or ROC curve, lets us summarize \n\nthe trade off between the true positive rate and false positive rate for a given \n\nmethod. Each point on this curve represents a different probability threshold. \n\nFor example, the threshold of 0.5 which we saw in the previous slide has a \n\nhigh specificity, meaning a low false positive rate.  \n\n \n\nThis means that among the people who don't default, we're very unlikely to \n\npredict that they will default. However, it has a mediocre sensitivity, meaning \n\nthat we miss a lot of the people who do default. If we change our probability \n\nthreshold to 0.25, we're now predicting that more people overall will default. \n\nThis increases our sensitivity, because we're catching more of the people who \n\ndefault, but at a cost of decreasing our specificity.  \n\n \n\n \n\n \n\n \n\n17 \n\n\n\n\n\n\n", "ROC Curve\n\nlibrary(pROC)\n\nmy_roc = roc(response=Default$default, \n\npredictor=all_predicted)\n\nplot.roc(my_roc)\n\nEach point represents \n\na different threshold\n\np = 0.25\n\np = 0.5\n\n \n\n \n\nThe receiver operator characteristic curve, or rock curve, lets us summarize \n\nthe trade off between true positive rate and false positive rate for a given \n\nmethod. Each point on this curve represents a different threshold. \n\n \n\nFor example for our logistic regression model, if we wanted a specificity of 0.9, \n\nwe could get it by using a threshold of 0.05. In that case, our sensitivity would \n\nbe 0.84. By changing the threshold, we could increase the sensitivity but at the \n\nexpense of decreasing the specificity or vice versa.  \n\n \n\n \n\n \n\nNotes: \n\n \n\nlibrary(pROC) \n\nmy_roc = roc(response=Default$default,  \n\n \n\n   predictor=all_predicted) \n\nplot.roc(my_roc) \n\n \n\n \n\n \n\n \n\n \n\n \n\n18 \n\n\n\n\n\n\n", "Interpreting ROC curves\n\nBetter models have ROC curves \n\ncloser to upper-left corner\n\n• Area under curve (AUC) close to 1\n\nGray line (\" = $) represents a \n\nmethod that does no better than \n\nrandom guessing\n\n \n\n \n\n \n\n \n\n \n\n19 \n\n\n\n\n\n", "Using ROC Curves to Compare Methods\n\n \n\n \n\nIn practice, ROC curves are rarely used to look at just one model at a time. \n\nThey're especially good for comparing different models or methods. For \n\nexample, when we're using logistic regression to predict whether a person will \n\ndefault on their credit card debt, a model that uses student and balance as \n\npredictor variables has an Area Under the Curve, or AUC, of 0.95, very close to \n\n1. In comparison, a model that uses student and income as predictor variables \n\nhas an area under the curve of 0.54, barely better than random guessing. \n\n \n\nNotes: \n\nR Code: \n\nresults = data.frame(sensitivity = c(my_roc$sensitivities, my_roc2$sensitivities), \n\n                     specificity = c(my_roc$specificities, my_roc2$specificities) \n\n                    ) \n\n \n\n# Add a column to specify the model. \n\n# While we're at it, compute the false positive rate: \n\nresults <- results %>% \n\n  mutate(model = c(rep(\"Student + Balance\", length(my_roc$thresholds)), rep(\"Student + \n\nIncome\", length(my_roc2$thresholds))), \n\n         false_positive_rate = 1 - specificity \n\n         ) \n\n \n\n# Make the graph: \n\nresults %>% \n\n  gf_line(sensitivity ~ false_positive_rate, color =~ model) %>% \n\n  gf_abline(slope = 1, intercept = 0, col = \"gray\") \n\n \n\n  \n\n \n\n \n\n \n\n \n\n20 \n\n\n\n\n\n\n", " \n\nNot just for logistic regression\n\n• May need to do extra work to get the predicted probabilities (not \n\njust classifications)\n\n \n\n \n\nROC curves are not just for logistic regression. They can be used to compare \n\nany methods for classification as long as they result in a continuous \n\nmeasurement such as predicted probabilities. Depending on the method, you \n\nmay need to do some extra work to get the predicted probabilities rather than \n\njust the predicted classifications of yes or no, 1 or 0.  \n\n \n\nFor example, to get the ROC curve for k-nearest neighbors shown here, we \n\nneed to start by using the argument prob equals true to get the predicted \n\nprobabilities from the KNN function. Then we need to extract the probability \n\nattribute from that set of predictions, but these predicted probabilities are the \n\nprobability of belonging to whatever category is the predicted classification. So \n\nwe then need to use an ifelse statement to take 1 minus those probabilities if \n\nthe predicted classification was no. This converts the predicted probabilities \n\ninto the probability of default rather than the probability of either default or \n\npaying off the debt. If you forget to do these steps and just use the predictions \n\nof the classifications, you'll end up with a ROC curve for KNN that looks \n\npiecewise linear and has an area under the curve that's much lower than what \n\nit should be.  \n\n \n\nNotes: \n\npredicted = knn(train = x_train, \n\n                  test  = x_test, \n\n                  cl = train_set$default, \n\n                  k = 41, prob = TRUE) \n\n \n\nprob = attr(predicted, \"prob\") \n\n \n\n \n\n21 \n\n\n\n\n\n\n\n\n\n\n\n\n", " \n\nprob_of_default = ifelse(predicted == \"No\", \n\n                           1-prob, prob) \n\nall_predicted_knn[groupii] = prob_of_default \n\n \n\n \n\n \n\n22 \n\n", " \n\nQuestion 1 \n\n \n\n \n\n \n\nAnswer is at the end of this transcript \n\n \n\n \n\n \n\n \n\n \n\n23 \n\n\n", " \n\nCost functions\n\nDefaulted?\n\nPredicted\n\nto default?\n\nNo\n\nGood (100)\n\nFALSE\n\nTRUE Not good (-10)\n\nYes\n\nVery bad (-1000)\n\nGood (10)\n\n \n\n \n\nAnother way to assess a model for classification is with a cost function. This \n\ncan be especially effective if the two types of errors, false positives and false \n\nnegatives, are not equally bad. For example, suppose we're planning to offer a \n\ncredit-limit increase to all of our customers who are not predicted to default.  \n\n \n\nIn that case, being in the upper left-hand corner of the confusion matrix, \n\npeople who don't default and are not predicted to default, is good. We might \n\nassign that a score of 100 to represent the amount in dollars that we expect to \n\nearn from additional interest on the raised credit limit for those customers. \n\nSimilarly, being in the lower right-hand corner of the confusion matrix is also \n\ngood. This represents customers who do default and who are predicted to \n\ndefault. We won't offer these customers a credit limit increase, but we might \n\nassign this a score of 10, representing the amount in interest we expect to gain \n\non their current debt before they default.  \n\n \n\nBeing in the lower left-hand corner of the confusion matrix is not so good. \n\nThis represents a false negative, meaning the customer isn't going to default, \n\nbut we predict that they will default. We might assign this a score of negative \n\n10 to represent the loss of customer goodwill from refusing a credit-limit \n\nincrease to a good customer.  \n\n \n\nHowever, the upper right-hand corner of the confusion matrix is very bad. This \n\nis a false negative, where we fail to identify a customer who is going to default \n\non their debt. We might assign this a score of negative 1,000 to represent the \n\namount of money we expect to lose if we offer a credit-limit increase and the \n\nperson doesn't pay it back.  \n\n \n\n \n\n \n\n24 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", " \n\nComputing the cost function\n\nDefaulted?\n\nPredicted\n\nto default?\n\nNo\n\nGood (100)\n\nFALSE\n\nTRUE Not good (-10)\n\nYes\n\nVery bad (-1000)\n\nGood (10)\n\n \n\n \n\nTo use our cost function to assess our model, we start by using the matrix \n\nfunction to input the cost function into R. Here, the first argument is a vector \n\nof the numbers we assigned listed in order going down the columns. So 100 is \n\nfollowed by negative 10. And the second argument is nc equal to 2 to say that \n\nwe want two columns in our matrix. Then we compute the confusion matrix as \n\nusual. And we multiply the confusion matrix by the cost.  \n\n \n\nNotice that when we use a regular old star or asterisk to do the multiplication, \n\nthe matrices get multiplied element by element. So the first entry in the cost \n\nmatrix gets multiplied by the first entry in the confusion matrix, which is what \n\nwe want here. The sum of the entries in the resulting matrix gives us the total \n\ncost associated with the predictions that this model made. In this case, we let \n\npositive numbers represent good outcomes. So we want the total cost to be as \n\nlarge as possible.  \n\n \n\nNotes: \n\n \n\ncost = matrix(c(100,-10, -1000, 10), nc = 2) \n\ncost \n\n \n\nconf_matrix = table(all_predicted > 0.5, \n\n \n\n \n\nconf_matrix \n\n \n\n \n\nconf_matrix * cost \n\n \n\nsum(conf_matrix * cost) \n\nDefault$default) \n\n \n\n \n\n25 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", " \n\nOptimize the total cost \n\n \n\n \n\nYou can then test out different models to find the one that optimizes the total \n\ncost based on the numbers you chose for how good or bad each entry in the \n\nconfusion matrix was. Here I'm using a for loop to iterate over possible values \n\nof the probability threshold from 0.01 up to 0.98. In this case, it turns out that \n\nwe should predict a default for anyone with a probability of default greater \n\nthan 0.11. So we would only offer credit-limit increases to people with a \n\nprobability of 0.11 or lower. I did test out 0.99 here as well. But it turned out \n\nthat, in that case, everybody was predicted to not default. So my confusion \n\nmatrix only had one row, which caused some problems with computing the \n\ncost function.  \n\n \n\nNotes: \n\nprob_thresh = seq(.01, .98, by = .01) \n\ntotal_cost = numeric(length = length(prob_thresh)) \n\n \n\nfor(ii in 1:length(prob_thresh)){ \n\n  conf_matrix = table(all_predicted > prob_thresh[ii], \n\n                      Default$default) \n\n  total_cost[ii] = sum(conf_matrix * cost) \n\n} \n\n \n\ngf_point(total_cost ~ prob_thresh) \n\n \n\n \n\n \n\n26 \n\n\n\n\n\n\n\n", "+\n\nLogistic Regression: Advantages\n\n1\n\n• Produces an interpretable model.\n\n• Estimates the influence of each predictor\n\n2\n\n• Typically better than KNN when true relationship between\n\npredictors and                       is linear.\n\nlog\n\n$(&)\n\n1 − $(&)\n\n\t\n\n \n\nOne of the advantages of logistic regression is that it produces an interpretable \n\nmodel that allows us to estimate the influence of each predictor variable based \n\non the size of its coefficient. We can even do a hypothesis test about whether \n\nthat coefficient is different from zero. \n\n \n\nLogistic regression is also typically better than K nearest neighbors when the \n\ntrue relationship between the predictor variables and the logit is linear.  \n\n \n\n \n\n \n\n \n\n \n\n \n\n27 \n\n\n\n\n\n\n\n\n\n\n", "–\n\nLogistic Regression: Disadvantages\n\n1\n\n• KNN typically better when \n\nrelationship is highly non-linear.\n\n2\n\n•\n\n&'‘s unreliable when classes are \n\nseparable.\n\n \n\n \n\nHowever, K nearest neighbors typically performs better than logistic regression \n\nwhen the relationship is highly non-linear. \n\n \n\nIn addition, the estimated coefficients of logistic regression become unreliable \n\nwhen the classes are separable, as in the example shown here. In a case like \n\nthis, it's likely that linear discriminant analysis or a support vector classifier \n\nwould do a better job. \n\n \n\n \n\n \n\n \n\n \n\n \n\n28 \n\n\n\n\n\n\n\n\n", " \n\nGeneralized Linear Models (GLMs) for Count Data\n\nInteger Data: \n\nError terms don’t have\n\nGaussian distribution. \n\nLinear regression\n\ndoesn’t work well.\n\n \n\n \n\nLogistic regression isn't the only kind of generalized linear model. Another \n\ncommon GLN is Poisson regression. \n\n \n\nFor example, here we have a graph of the number of matings a male elephant \n\nexperiences as a function of its age. I've included a jitter on the matings which \n\nadds a small random amount to each point. So that two elephants with the \n\nsame age and number of matings, don't show up as exactly the same circle on \n\nthe graph. \n\n \n\nIn this data set, elephants that are between 30 and 35 years old tend to have \n\nbetween zero and four matings. Whereas elephants that are between 40 and \n\n45 years old, tend to have between zero and nine matings. \n\n \n\nSo as elephants age increases, their mean number of matings increases, but so \n\ndoes the variance on the number of matings. Often an increase in variance as \n\nthe predictor variable increases is a good sign to try log transforming the Y \n\nvariable. \n\n \n\nHowever in this case, the number of matings has to be an integer value. You \n\ncan't have an elephant that mates 3.5 times. That means that our error terms in \n\nthe regression are not going to have a Gaussian distribution. \n\n \n\nSo a simple linear regression, either with or without a log transformation on \n\nthe Y variable, is not going to be a good fit. \n\n \n\n \n\n \n\n \n\n29 \n\n\n\n\n\n\n\n\n", "Notes: \n\n \n\nThe elephant data set can be found in the R package gpk. See the gpk \n\ndocumentation for more information on the data sets it contains. For more \n\nabout how to analyze the elephant data set, see these notes on Applied Linear \n\nRegression (Poisson_Reg_part1_4pp.pdf). \n\n \n\n \n\n30 \n\n \n\n \n\n \n\n\n\n\n\n", "Poisson Regression\n\nFits a straight line to log(y)\n\nAvoids predicting counts < 0\n\n \n\n \n\nInstead, we can use Poisson regression. This fits a straight line to the log of Y \n\nwith errors that are distributed according to a Poisson distribution. This avoids \n\npredicting count values, in this case, the count of the number of matings, as \n\nbeing less than 0.  \n\n \n\nNotes: \n\n \n\nfit_elephant = glm(Number_of_Matings ~ Age_in_Years, \n\n                   data = elephant, family = \"poisson\") \n\n \n\n \n\n \n\n \n\n \n\n31 \n\n\n\n\n\n\n\n", "Summary One\n\nLogistic regression is a good choice for 2-category classification \n\nproblems when: \n\n• you want an interpretable model \n\n• the classes are not separable\n\n• the relationship between predictors and                           could be \n\nlog\n\n\t\n\napproximately linear\n\n!(!)\n\n1 − !(!)\n\n \n\n \n\n \n\n32 \n\n \n\n \n\n \n\n \n\n\n\n\n\n\n\n", "Summary Two\n\n• ROC curves summarize the relationship between true positive rate \n\nand false positive rate for many different values of a threshold.\n\n• Higher ROC curves indicate better methods.\n\n \n\n \n\n \n\n \n\n \n\n33 \n\n\n\n\n", " \n\nQuestion 1 Answer \n\n \n\n \n\n \n\n34 \n\n\n"]], ["C:\\Users\\mjols\\Documents\\DS UWEC courses\\740 data mining\\lesson3\\ds740_lesson3_presentation2.pdf", [" \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nImportant note: Transcripts are not substitutes for textbook assignments. \n\n \n\n\n\n", " \n\n \n\n \n\n\n", " \n\nUsing too many predictor variables in your linear regression results in a model with very \n\nlow bias but very high variance. This means that the model is great at fitting your \n\ntraining data but terrible at predicting new data points. And it's also difficult to \n\ninterpret. \n\n \n\n \n\n \n\n\n", " \n\n \n\nOne simple step to avoid having too many predictor variables is to check for collinearity \n\nor multi-collinearity, which refers to when two or more predictor variables are highly \n\ncorrelated. This can inflate the standard error for your estimated regression coefficients, \n\nwhich results in larger P values for testing whether your individual coefficients are \n\ndifferent from 0. \n\n \n\nOne way to check for collinearity is by looking at a matrix of scatter plots using the pairs \n\nfunction in R. Exam this matrix for any sign of a linear relationship between two of the \n\nvariables. For example, in this data set, the total enrollment at colleges is highly \n\ncorrelated with the number of full-time undergraduates.  \n\n \n\nNotes: \n\n \n\nlibrary(ISLR) \n\nmy_colleges = College[-96,] # Remove an observation with an unrealistic graduation rate \n\n(> 100%) \n\nmy_colleges = my_colleges[ ,c(4, 7,11, 15,18)] # Examine a subset of variables so the \n\nscatterplots aren’t too small to read \n\npairs(my_colleges) \n\n \n\n \n\n \n\n\n", " \n\nA correlation plot is another good way to quickly check for correlations among variables. \n\nLook for predictor variables whose correlations are close to either positive 1 or negative \n\n1.  \n\n \n\n \n\n \n\n \n\n\n", " \n\nSome correlations that you see in the correlation graph are not a cause for concern. For \n\nexample, a variable always has correlation 1 with itself. So you don't need to worry \n\nabout the large number of red circles you see on the main diagonal of the correlation \n\ngraph.  \n\n \n\nIt's also expected to have high correlation between a variable and its transformation. \n\nFor example, if you were using a variable salary and decided to log transform it to \n\nreduce its right skew, then we expect to have high correlation between those two \n\nvariables. This isn't really a surprise, so it's not really a concern. Often, you were going \n\nto remove the original salary variable from the model anyway because of its skew.  \n\n \n\nFinally, strong correlations either positive or negative with the response variable are \n\ngood because those predictors will help us make good predictions. So for example, if \n\nwe're trying to predict enrollment, then we should be happy to see a large number of \n\npredictor variables with strong correlations with enrollment.  \n\n \n\n \n\n\n", " \n\nThe correlation plot and matrix of scatter plots can tell you about the collinearity \n\nbetween two variables, but it can't tell you about multicollinearity among three or more \n\nvariables. For example, the total enrollment at a college might be only weakly correlated \n\nwith the number of data science majors. But if you start to include the number of \n\nbiology majors, business majors, psychology majors, and English majors, then the \n\nassociation starts to increase  \n\n \n\n \n\n\n", " \n\n \n\nTo check for multi-collinearity, we can use the variance inflation factor, which is 1 over 1 \n\nminus the coefficient of determination that we get from regressing a particular \n\npredictor variable as a function of all the other predictor variables, ignoring the \n\nresponse variable. \n\n \n\nA variance inflation factor of 10 or more indicates that 90% or more of the variation in \n\nthis predictor variable can be explained by the other predictor variables. So in this case, \n\nyou may want to remove that predictor variable or replace it by its residuals from that \n\nregression. \n\n \n\n \n\n \n\n\n", " \n\n \n\nFor example, suppose our goal was to predict graduation rate based on a college's \n\nenrollment, number of full-time undergraduates, cost of books, and student-faculty \n\nratio. We would start by using a linear model to predict graduation rate like usual, and \n\nthen we would use the VIF function applied to the output from that linear model.  \n\n \n\nIn this case, we see that both enrollment and the number of full-time undergraduates \n\nhave very high variance inflation factors. That makes sense because we expect that the \n\nnumber of full-time undergraduates is closely correlated with a school's enrollment. This \n\nmeans that enrollment and number of full-time undergraduates are giving us mostly the \n\nsame information. So we could remove one of them from our model.  \n\n \n\nI'd probably remove number of full-time undergraduates because it has a slightly higher \n\nvariance inflation factor. This would decrease our collinearity, which would tend to \n\ndecrease the standard errors on our coefficient estimates in our linear model. And that \n\nin turn is likely to give us smaller p values. However, if we remove the number of full-\n\ntime undergraduates from our model, we are removing information only some of which \n\nis duplicated by including the variable enrollment.  \n\n \n\n \n\n \n\n\n", " \n\n \n\nAn alternative is to regress the number of full-time undergraduates on the other \n\npredictor variables and then replace it by its residuals. What this does is it extracts all of \n\nthe information that's in the number of full-time undergraduates that's not already \n\ncontained in the other predictor variables.  \n\n \n\nThe way to do this is by creating a temporary linear model where the number of full-\n\ntime undergraduates is the response variable and using all the other predictor variables \n\nas the predictor variables, so everything except for graduation rate, which was the \n\noriginal response variable.  \n\n \n\nIf we look at the summary of this temporary linear regression model, we find that it has \n\na multiple R squared of 0.9335, which corresponds to a variance inflation factor of 15, \n\njust as we saw by using the VIF function. We can then take our temporary regression \n\nmodel dollar sign residuals and store them as a new column in our data set and remove \n\nthe original number of full-time undergraduates. Then we can proceed with fitting our \n\nlinear regression model for graduation rate. You'll notice that the variance inflation \n\nfactors of the resulting model are much lower.  \n\n \n\n \n\n \n\n \n\n\n", "Self-Assessment Question 1 \n\n \n\n \n\nAnswer is at the end of this transcript \n\n \n\n \n\n \n\n\n", " \n\n \n\n \n\n\n", " \n\n \n\nThis slide represents a video/screencast in the lecture. The transcript does not \n\nsubstitute video content. \n\n \n\nLet's investigate whether multicollinearity is likely to be an issue when predicting sales \n\nat a student run cafe. I've already read in the data, and I'm using the function \n\nmake.names to take all of the variable names and replace the spaces with periods. That \n\nwill make them easier to refer to later on in our analysis.  \n\n \n\nThis data set contains 22 variables. So a pairs plot of scatter plots ends up being pretty \n\nhard to read. We could make scatter plots of subsets of variables at a time, but I'm going \n\nto make a correlation plot instead. I've started by using the select if function from the \n\ndplyr package to select only the numeric variables for which we're able to compute the \n\ncorrelations. Then I'll use the cor function to compute the correlation matrix of pairwise \n\ncorrelations between each pair of numeric variables. And I'll make the correlation plot \n\nusing the cor plot function from the cor plot library.  \n\n \n\nI'm using the function brewer.pal for Brewer palette to get the color scheme for my \n\ngraph. And this is from the R color Brewer package. I'd like to reverse this color scheme \n\nso that red represents large values, correlations close to positive 1, and blue represents \n\nlarge negative values close to negative 1. To me it's more intuitive for red to be high and \n\nblue to be low.  \n\n \n\nHere's our correlation plot. Here we're trying to predict sales. So we're happy to see \n\nthat sales is reasonably strongly correlated with a number of other variables in the data \n\nset. The other correlations that aren't with sales might be of concern, indicating \n\nmulticollinearity. It looks like coffees is strongly negatively correlated with the maximum \n\n\n", "temperature on a given day. That makes sense that people would tend to want to buy \n\ncoffee when it's cold out.  \n\n \n\nWe also see that the maximum temperature is correlated with t, which represents the \n\nday on which the data was gathered. That makes sense also because this data set was \n\ngathered during one semester, the spring semester, at a university. So day is going to \n\nroughly start in January and end in April. It makes sense that it would be positively \n\nassociated with the temperature.  \n\n \n\nWe do see some groups of variables that are correlated with each other, such as the \n\nnumbers of items sold of different types and the numbers of items wasted of different \n\ntypes. So it's going to be worth looking at the variance inflation factors to see if there's \n\nenough multicollinearity there that we need to be concerned.  \n\n \n\nThe correlation graph and pairs graph of scatter plots can only tell us about relationships \n\nbetween pairs of numeric variables. So it's a good idea to also look at the data dictionary \n\nand think about possible interpretations to get a sense of what relationships might exist \n\namong the categorical variables and among groups of more than two variables at a \n\ntime.  \n\n \n\nFor the cafe data set, right away you might notice that we might have an association \n\ngoing on among these first four variables. Let's check this out using R. We can get a \n\nbetter sense of what's in these variables by looking at the first few entries using the \n\nhead function.  \n\n \n\nSo we see that the first few entries of the t variable are just the numbers from 1 to 6. \n\nAnd by looking at the first few entries of a table of dates, it looks like each date only \n\noccurs once. We can verify this by looking at the length of the date variable and then \n\nreducing that to just the unique elements of date and comparing the length of that \n\nvector. We see that both of those vectors have length 42. So yes, indeed, the date \n\nvector is just a list of 42 different dates with no dates repeated. So it looks like the t \n\nvariable is just a numeric version of that same information. So we don't gain anything by \n\nincluding both of those variables.  \n\n \n\nBetween these two variables, t is definitely the better choice to use in our model. That's \n\nbecause date would be treated as a categorical variable. Recall that linear regression, \n\nlike many other modeling types, will automatically one-hot encode categorical variables, \n\nmeaning that our 42 different dates will be turned into 41 different 0, 1 indicator \n\nvariables. That would massively increase our number of predictor variables, which \n\nwould lead to overfitting. In fact, because we had as many different dates as we had \n\nobservations in our data set, we would end up with a model that perfectly predicted the \n\nsales of the data in our training set but which was useless for predicting the sales on any \n\nnew data.  \n\n \n\n", "Back in the data dictionary, it looked like day code was simply a numeric representation \n\nof the day of the week. We can verify this by making a table of these two variables \n\ntogether. And here we can see that yes, 5 represents Friday, 1 represents Monday, and \n\nso on. So these two variables are giving us the exact same information as well. We don't \n\nneed both of them in our analysis.  \n\n \n\nBetween these two variables, I actually am going to choose to use day of week instead \n\nof day code. That's because this is a linear regression model. So using day code as a \n\nnumeric variable would model the effect of the day as being either linearly increasing or \n\nlinearly decreasing as you go from Monday to Friday throughout the week. In \n\ncomparison, using day of week, which is a categorical variable, will allow us to estimate \n\nthe effect separately of Mondays, Tuesdays, Wednesdays, Thursdays, and Fridays. So \n\nmaybe at this university because of the way classes are scheduled, sales are higher on \n\nMondays, Wednesdays, and Fridays than they are on Tuesdays or Thursdays. Using a \n\ncategorical variable, day of week, would better allow us to model that.  \n\n \n\nSo let's go ahead and fit a linear regression model of sales as a function of all the other \n\nvariables in the data set except for date and day code. When we look at our model, we \n\nsee that two of our variables are listed as NA, meaning that our model was not able to \n\nestimate a coefficient for these two variables. This happened because our model was \n\ncompletely singular. That is, these two variables were perfectly predictable based on the \n\nother predictor variables in our model.  \n\n \n\nIf we look back at our data dictionary, we can see that the total soda and coffee variable \n\nprobably is closely related to the variables sodas and coffee. And similarly, the total \n\nitems wasted is probably closely related to some of these other variables, such as the \n\namount of fruit cup waste, the amount of cookies waste, and so on.  \n\n \n\nLet's investigate whether these are in fact perfectly associated. Here I'm using the \n\nmutate function from dplyr to create a temporary new variable called waste sum. So I'm \n\nadding together all of the other waste variables. And then I'll use gf point to make a \n\ngraph of total items wasted and waste sum. So we can see if total items wasted is, in \n\nfact, exactly the same as the sum of the other wastes.  \n\n \n\nHere we can see that the variables are perfectly on a line with each other. So this \n\nexplains the perfectly singular result we got in our linear regression. To keep things \n\nsimple, let's go ahead and use total items wasted and remove the other waste variables \n\nfrom our analysis.  \n\n \n\nTo investigate total soda and coffee, we could make a graph just like we did for the total \n\nwaste, but I'll try it a little bit differently. I'm still using mutate, but this time I'm creating \n\na variable called is equal, which will be equal to true if sodas plus coffees is exactly equal \n\nto the total soda and coffee. Then I'll group by the value of that new variable is equal, \n\n", "and I'll count how many rows are equal to true, meaning sodas and coffees added \n\ntogether gives the total soda and coffee.  \n\n \n\nHere we see that all 42 rows in the data set give a value of true, meaning that we can \n\nperfectly predict the value of total soda and coffee based on the value of sodas and \n\ncoffees. So for the purposes of our analysis, let's remove total soda and coffee and we'll \n\njust use sodas and coffees. So here's our new linear model where we're predicting sales \n\nbased on all of the other variables except for the ones that we decided to exclude for \n\nreasons of multicollinearity. Linearity  \n\n \n\nHere we can see that we no longer have any NAs in our model. But some of our \n\nstandard errors are still pretty large compared to the size of the coefficient that we're \n\nestimating. This could be a sign of additional multicollinearity that we haven't detected \n\nyet. To investigate this, let's look at the variance inflation factor.  \n\n \n\nHere we see that the variable day of week has a large variance inflation factor of 14.88. \n\nHowever, remember that that was a categorical variable with five possible levels, one \n\nfor each day of the week, meaning that it had four degrees of freedom. This means that \n\na variance inflation factor of 14 isn't quite as bad as it might seem for a variable that \n\nonly had one degree of freedom, such as a quantitative variable.  \n\n \n\nTo get a better comparison among variables with different degrees of freedom, we can \n\nlook at this third column, which contains the variance inflation factor raised to a power \n\nthat depends on the degrees of freedom. Here we can see that day of week isn't looking \n\nso extreme anymore. And sodas has a variance inflation factor of 3.05.  \n\n \n\nGenerally, you want to keep this third column less than the square root of 10, which is \n\nabout 3.16. So the variance inflation factor for sodas isn't quite over that threshold. But \n\nI would be more concerned about sodas than I would be about day of week. If this were \n\nall of the investigation of multicollinearity that I was planning to do with this data set, I \n\nmight consider either excluding sodas or regressing it on the other predictor variables \n\nand then replacing.  \n\n \n\n", " \n\nTo go deeper than checking for multi-collinearity, we need a way of comparing models \n\nto determine where they fall in the trade-off between bias and variance. \n\n \n\n \n\n \n\n\n", " \n\nRecall that the coefficient of determination is 1 minus the residual sum of squares \n\ndivided by the total sum of squares. This is not a good criterion for model comparison, \n\nbecause it can't decrease as predictor variables are added. So if you simply try to choose \n\nthe model that maximizes r-squared, you'll always end up choosing the most \n\ncomplicated model. \n\n \n\n \n\n \n\n\n", " \n\nThe adjusted r-squared takes r-squared and subtracts a penalty based on d, the number \n\nof predictors, not counting the y-intercept. This means that larger values of the adjusted \n\nr-squared indicate a better trade-off between fit to the training data and the number of \n\npredictors. \n\n \n\n \n\n \n\n\n", " \n\n \n\nThe analysis of deviance is a hypothesis test for comparing models. Unlike adjusted r-\n\nsquared and the other criteria for model comparison that we'll discuss in this lesson, the \n\nanalysis of deviance should only be used for comparing nested models, ones in which \n\nthe predictor variables of one model are a subset of the predictor variables of the other \n\nmodel. \n\n \n\nThe null hypothesis is that both models fit the data equally well. The alternative \n\nhypothesis is that the more complex model fits the data significantly better. In this \n\nexample, we get a large P-value, so we retain the null hypothesis that both models fit \n\nthe data equally well. \n\n \n\nSo in this case, there's not any good reason to use the more complex model, which \n\nincreases the variance. So we would prefer to use the simpler model. The analysis of \n\ndeviance can also be used for logistic regression models by including the argument test \n\nequals chi. \n\n \n\n \n\n\n", " \n\nMallows' Cp is another criterion for model comparison. It's based on the residual sum of \n\nsquares plus a penalty term based on d, the number of predictor variables and sigma \n\nsquared hat, the estimated variance of the noise terms in the linear regression. \n\n \n\nA better model will tend to fit the training data well, so it will have a small residual sum \n\nof squares, and it will have a small penalty term. So smaller values of Mallows' Cp \n\nindicate a better model. \n\n \n\n \n\n \n\n\n", " \n\n \n\nAkaike's Information Criterion is equal to 2 times the number of parameters minus 2 \n\ntimes the log likelihood, meaning the probability of observing this data set if the model \n\nwere true. I really like Akaike's Information Criterion, because it's based on the \n\nlikelihood. That means it's easy to apply to other types of models besides linear \n\nregression. \n\n \n\nIf you can write a statistical likelihood for the model, you can compute the AIC. \n\nHowever, AIC also has a special form when it's applied to least squares regression. You'll \n\nnotice that this form is very similar to the form for Mallows' Cp. In fact, in least squares \n\nregression, AIC will always choose the same preferred model as Mallows' Cp. \n\n \n\n \n\n\n", " \n\n \n\nLike Mallows' CP, smaller values of AIC indicate better models. Models with an AIC \n\ndifference of less than about 2 are generally considered to be reasonable alternatives. \n\nFor example, in the models shown here, fit 1 and fit 2 are reasonable alternatives to \n\neach other because their AICs are within 2 of each other, but fit 3 has a much lower AIC. \n\nSo based on AIC, this model is the best trade-off between accuracy and parsimony. \n\nNotes:  \n\n \n\nStyliano, Pickles, and Roberts recommend requiring AIC differences of 6 or more before \n\nrejecting a model: \n\nUsing Bonferroni, BIC and AIC to assess evidence for alternative biological pathways: \n\ncovariate selection for the multilevel Embryo-Uterus model (2013) \n\n \n\n \n\n\n", " \n\nThe Bayesian Information Criterion, or BIC, is similar to AIC except that the penalty for \n\nextra parameters depends on the sample size n. The larger the sample size, the bigger \n\nthe penalty. So BIC tends to select smaller models than either Mallows' Cp or AIC. \n\n \n\n \n\n \n\n\n", " \n\nIt's important to note that all three of these criteria for model comparison involve \n\nconstants which some functions omit. For example, the r functions AIC and extract AIC \n\nproduce very different values for the AIC of the same model. So when you're comparing \n\ntwo models, you should always use the same data set and the same function to \n\ncompute the model comparison criterion for both of the models. \n\n \n\n \n\n \n\n\n", " \n\nBest subset selection refers to a systematic approach for choosing the best possible \n\nmodel. Suppose we have P predictor variables available to us. Then for each possible \n\nnumber of predictors, from 0 up to P, will fit all of the models with that number of \n\npredictors. And we'll choose the one with the lowest residual sum of squares or, \n\nequivalently, the largest coefficient of determination. We'll call this model m sub d. That \n\nthe best model of size d. Then we'll choose the best model from m sub 0 up through m \n\nsub p using one of our criteria for model selection. \n\n \n\n \n\n \n\n\n", " \n\n \n\nWhen d is equal to 0, there's only one model to consider-- the null model, in which each \n\npoint's response value is predicted to equal the mean response value for the whole \n\nsample. When d is equal to p, we also only have one model to consider-- the full model, \n\nin which all of the predictor variables are used. \n\n \n\nAlso notice that Mallows' Cp, AIC, BIC, and adjusted r-squared all rely on the residual \n\nsum of squares. They only differ based on how they penalize the number of predictor \n\nvariables. That means that for any given number of predictor variables d, these four \n\ncriteria will all agree on which model is best. They'll only differ in their decisions in the \n\nlast step of best subset selection when we're comparing models of different sizes. \n\n \n\n \n\n\n", " \n\nTo perform best subset selection in R, we can use the regsubsets function. If we do this \n\nusing the argument method equals exhaustive, regsubsets will consider all possible \n\nmodels with numbers of variables between 1 and nvmax. In fact, exhaustive is the \n\ndefault value for the method argument. So we could omit this argument entirely and \n\nconsider all possible models.  \n\n \n\n \n\n \n\n\n", " \n\n \n\nIf there are more than 50 predictor variables, we can use a stepwise approach either by \n\nchanging the method argument in the regsubsets function or by using the step function, \n\nwhich is based on AIC. With this approach, instead of performing an exhaustive search \n\nof all possible models, at each stage we're doing one of two things-- either adding the \n\nmost useful predictor variable to the model in forward stepwise regression or removing \n\nthe least useful variable in backwards stepwise regression.  \n\n \n\nThe argument seqrep for regsubsets or direction equals both in the step function allows \n\nus to do either forward or backward stepwise regression at each stage. This approach is \n\nnot guaranteed to achieve the best possible model, but it can be useful if you have a lot \n\nof predictor variables. Forward stepwise regression can even be used if the number of \n\npredictor variables is larger than your sample size.  \n\n \n\nNotes: \n\n \n\nregfit.full = regsubsets(Grad.Rate ~ ., data = College, \n\n          method = \"seqrep\", nvmax = 17) \n\n \n\nfit = lm(Grad.Rate ~ ., data = College) \n\nsfit = step(fit, direction = \"both\") \n\n \n\n \n\n\n", " \n\n \n\nThe step function is nice because it automatically compares models of different sizes, \n\nwhereas reg subsets will just report the best model of each size and then leave the \n\ncomparison between different sizes up to you. However, reg subsets has better handling \n\nof missing data, and it also treats each level of a factor as a separate indicator variable. \n\n \n\nThis is nice, for example, if you have sales data from different days of the week, and you \n\nwant to consider the possibility that sales are different on Fridays but the same Monday \n\nthrough Thursdays. \n\n \n\n \n\n\n", " \n\n \n\n \n\n\n", " \n\n \n\nThis slide represents a video/screencast in the lecture. The transcript does not \n\nsubstitute video content. \n\n \n\nLet's perform Best Subset Selection on our cafe data, using an exhaustive search. To do \n\nthis, we'll apply the Regsubsets function, which is in the Leaps library. The syntax of the \n\nRegsubsets function is very similar to the lm function, which we use for linear \n\nregression. The only additional argument here is nvmax. This sets the maximum number \n\nof predictor variables you want to include in any of the models. \n\n \n\nI generally set this equal to the maximum number of predictor variables that you have in \n\nyour dataset, remembering to add additional predictor variables for any additional \n\nlevels of factor variables that you may have. If we simply look at the name of the object \n\nwhere we stored the regsubsets output, we get a summary of the analysis. And it's \n\ntelling us that including all levels of the factor level variables, we had 16 variables. So \n\ndouble checking this against the nvmax that we used, we see that yes, we included all \n\npotential variables. \n\n \n\nIf we want to know more about the models that it considered, we can use plot regfit. \n\nAnd here we get a plot summarizing each of the variables that were possible predictor \n\nvariables along the x-axis and the BIC of different models along the y-axis. And a dark \n\nrectangle indicates that that variable was included in that model. A lower BIC is better, \n\nwhich is indicated up at the top of this graph. \n\n \n\nSo here you can see that the best model with only a single predictor variable included \n\nthe variable wraps.sold. But that model had a fairly high BIC, so it's down at the bottom \n\n\n", "of the graph. The model that was slightly better than that included all the possible \n\npredictor variables, the full model, with a BIC of about negative 16. If we want to change \n\nthe y-axis, we can instead use the scale argument inside our plot function. Here I'll plot \n\nthe adjusted r squared. And here we get that in terms of adjusted r squared, the full \n\nmodel actually didn't do too badly. It's shown up here. We can extract more information \n\nby using the Summary function. And here I'll store the summary results inside a new \n\nobject. \n\n \n\nThen we can extract information from that summary, using the dollar sign to extract \n\npieces of that object. And I'll extract the BIC. Here we get the BIC of each model for one \n\npredictor variable up to 16 predictor variables. So we can plot this as a function of the \n\nnumber of variables. Here we see that as the number of predictor variables included in \n\nthe model increases, the BIC of the best possible model of that size first decreases and \n\nthen increases. We can use the which.min function to pick out which element of this \n\nvector of BICs is the lowest. It's the 10th element, which indicates that the model with \n\n10 variables had the lowest BIC, which matches what we saw on the graph. \n\n \n\nTo extract the best model in terms of Mallows's Cp, we can use which.min again but \n\nextracting the Cp part of the summary object. Or we can extract the best model in terms \n\nof adjusted r squared, using which.max. Based on adjusted r squared, the model with 12 \n\npredictor variables was the best. But Mallows's Cp in this case agreed with BIC that the \n\nmodel with 10 predictor variables was the best. We can see what the coefficients of that \n\nmodel were by using the function coef with arguments regfit, which was our regsubsets \n\nobject, and 10, the number of variables we wanted to be included in that model. So \n\nhere we see the variables included in the model and the estimated coefficients for each \n\nof those. So for example, the number of sodas is positively associated with the total \n\nsales for the day. \n\n \n\nNotes: \n\n \n\nNumber of variables = # of quantitative predictors + (# of levels – 1) for each categorical \n\npredictor. \n\nDataset: \"Student-run Café Business Data,\" submitted by Concetta A. DePaolo and David \n\nF. Robinson, Indiana State University. Dataset obtained from the Journal of Statistics \n\nEducation (http://www.amstat.org/publications/jse). Accessed 2 August 2016. Used by \n\npermission of author. \n\n \n\n \n\n\n", " \n\n \n\nReg subsets returns criteria for model comparison computed on the training data. \n\nHowever, if we want to estimate model performance on new data points, it's a good \n\nidea to use cross-validation. To do this, we need to be able to predict the response value \n\nfor each of the data points in the validation set. Unfortunately, there's not a built-in \n\npredict function for models that are the output from reg subsets. So we need to write \n\nour own. \n\n \n\nThe function shown here will do this by first extracting the appropriate columns from \n\nthe validation set and then multiplying them by the coefficients of the model. \n\n \n\n \n\n\n", " \n\n \n\nA common occurrence is that as we increase the number of variables in the model, the \n\ncross-validation error will drop sharply at first and then level off. So we get a large \n\nnumber of models with very similar cross-validation error rates. In the example shown \n\nhere, the model with nine variables had the lowest cross-validation error, but other \n\nmodels were very similar. \n\n \n\nIt might well be that if we repeated the cross-validation with a different random seed, \n\nwe might find that the model with 8 variables or 10 variables was best. In this case, it \n\ndoesn't really make sense to say that 9 is the absolute for certain optimal number of \n\nvariables. \n\n \n\nInstead, it makes more sense to say that all of the models with a mean squared error \n\nwithin a certain range, say, one standard error of the minimum, are near optimal. So we \n\nmight prefer to choose the most parsimonious model, the one with the fewest \n\npredictors. \n\n \n\n \n\n\n", " \n\nHere's the formula for computing the standard error of the cross-validation error. \n\nNotice that the first formula, cv sub j, is exactly like computing the mean squared error, \n\nbut we're doing it for a single fold of our k-fold cross-validation. \n\n \n\n \n\n \n\n\n", " \n\n \n\n \n\n\n", " \n\n \n\nThis slide represents a video/screencast in the lecture. The transcript does not \n\nsubstitute video content. \n\n \n\nTo perform cross-validation for best subset selection I'm going to start by defining a \n\npredict function that will work for objects that are the output of the regsubsets \n\nfunction. Then, I'll perform cross-validation as usual, setting n equal to the number of \n\nrows in the cafe data set, and setting my value of n groups.  \n\n \n\nThis is a fairly small data set, so I'm going to use leave-one-out cross-validation by \n\nsetting n groups equal to n, the size of the data set. Then I'm creating my groups vector, \n\nsetting my seed, and creating the cvgroups as a random permutation of the groups \n\nvector. Because we're doing leave-one-out cross-validation, we wouldn't really need to \n\nrandomize the order of the groups vector. We could have just said cvgroups equals the \n\nvector from 1 up to n. I've defined the variable nvar equal to 16, the maximum number \n\nof variables that I might want to include as predictors in my model.  \n\n \n\nAnd here, instead of a vector called all_predicted, I'm defining a matrix called \n\ngroup_error. So this is set up so that each row will represent one fold, and each column \n\nwill represent one model size, or number of variables.  \n\n \n\nThen we have our for loop, iterating over the folds as usual. We're setting up groupii, \n\ntrain_data and test_data as usual, and then I'm using regsubsets to fit my model.  \n\n \n\nHere we have data equals train_data, and nvmax equals nvar, which was 16. Here's \n\nwhere the difference comes in. Instead of simply computing the predictions for our test \n\nset, I'm now going to do a second for loop inside the first one, that's going to iterate \n\n\n", "over the different model sizes, from 1 up to 16. Inside that for loop, I'll make the \n\npredictions using my new predict function that I just wrote.  \n\n \n\nRecall that I wrote a function called predict dot regsubsets. Here I'm just calling predict. \n\nThat's because this is a generalized function, where if you call the function predict it will \n\nautomatically look at the object type of the first argument, see that it's a regsubsets \n\nobject, and then look for a function called predict dot regsubsets.  \n\n \n\nSo according to the way the arguments were set up in the function that I wrote, this first \n\nargument is our regsubsets object. The second argument is alldata equals cafe, the \n\nentire data set. Subset equals groupii, so the trues and falses defining which elements \n\nare in the test data set. And id equals jj, so that's the value of how many variables I want \n\nto include as predictors.  \n\n \n\nSo this gives us our predictions for the test set. Normally we would just want to store \n\nthis value in our all_predicted variable. But in this case, we're going to do it a little bit \n\ndifferently, and that's because we're going to want to compute the standard error of \n\nour mean squared errors. And the best way to do that is to compute the mean squared \n\nerror separately for each of the folds.  \n\n \n\nSo here I'm going to take the test data and the sales column, subtract my predictions, \n\nand then square it and take the mean to compute the mean squared error for this \n\nparticular fold and this number of variables, jj. And then I'm storing that value in the row \n\nthat corresponds to this fold, and the column that corresponds to this number of \n\nvariables in my group_error matrix.  \n\n \n\nSo now I'm going to come down here and actually compute the overall mean squared \n\nerror. So our for loop was filling up this group_error matrix with each entry being the \n\nmean squared error from one fold. Now I want to take the mean over all the folds for a \n\ngiven number of variables. So I want to take the mean over all the columns. So that's \n\nindex two of my group_error matrix.  \n\n \n\nThat will give us our mean squared error overall, which should be a vector of 16 \n\nnumbers. So one mean squared error for each number of variables. And then we can \n\ngraph that as a function of the number of variables.  \n\n \n\nIn this case, we see that the mean squared error starts out high, and then drops down to \n\na minimum at two variables. If we want to programmatically determine which number \n\nof variables is best, we can use the function which dot min, which tells us that indeed \n\ntwo variables is the best model in terms of mean squared error.  \n\n \n\nIn this case, it's pretty clear that having two variables both gives us a very low mean \n\nsquared error and a very parsimonious model. But what if the best model had been one \n\nof these up here, with 12 or 14 variables? In that case, we might want to check if there \n\n", "was a more parsimonious model with a mean squared error within one standard error \n\nof the minimum.  \n\n \n\nWe would do that by first computing the standard errors. We do that by taking the \n\nstandard deviation of each column of the mean squared errors, and then dividing by the \n\nsquare root of the number of folds. Then we'll use this to define a threshold by taking \n\nthe mean squared error overall from the lowest mean squared error model, and then \n\nadding the standard error of that model. And we want to take the most parsimonious \n\nmodel whose overall mean squared error is less than one standard error above the \n\noverall mean squared error of the lowest model.  \n\n \n\nSo here we see that models 2, 10, and 12 through 16 all had mean squared errors within \n\none standard error of the minimum. So we would want to pick the most parsimonious \n\nmodel from this list, which is the model with two variables.  \n\n \n\nTo see which two variables are in this best model, and what their coefficients were, we \n\nwant to go back to the model from the full data set, not the models from the cross-\n\nvalidation, which gave us a different model for each fold. So that was our regfit object \n\nfrom the previous video. \n\n \n\nNotes: \n\n \n\nDataset: \"Student-run Café Business Data,\" submitted by Concetta A. DePaolo and David \n\nF. Robinson, Indiana State University. Dataset obtained from the Journal of Statistics \n\nEducation (http://www.amstat.org/publications/jse). Accessed 2 August 2016. Used by \n\npermission of author. \n\n \n\n \n\n\n", " \n\n \n\n \n\n\n", "Question 1 Answer \n\n \n\n \n\n\n"]], ["C:\\Users\\mjols\\Python39\\gorpy\\gorp\\testDir\\silly_html_example.pdf", ["silly_html_example\n\nFebruary 5, 2021\n\nhello world\n\nYO DAWG WHAT WE DO???!!!\n\nok, I'm calm, i'm calm</li>\n\n1\n\n"]], ["C:\\Users\\mjols\\Python39\\gorpy\\gorp\\testDir\\walnut\\ln1 proofs.pdf", ["Massachusetts Institute of Technology \n\n6.042J/18.062J, Fall ’05: Mathematics for Computer Science \n\nProf. Albert R. Meyer and Prof. Ronitt Rubinfeld \n\nCourse Notes, Week 1 \n\nSeptember 7 \n\nrevised September 1, 2005, 856 minutes \n\nProofs \n\n1  What is a Proof? \n\nA proof is a method of ascertaining truth. But what constitutes a proof differs among ﬁelds. \n\n•\t Legal truth is ascertained by a jury based on allowable evidence presented at trial. \n\n•\t Authoritative truth is ascertained by a trusted person or organization. \n\n•\t Scientiﬁc truth is hypothesized, and the hypothesis is conﬁrmed or refuted by experiments. \n\n•\t Probable truth is obtained from statistical analysis of sample data. For example, public opin­\n\nion is ascertained by polling a small random sample of people. \n\n•\t Philosophical  proof  involves  careful  exposition  and  persuasion  based  on  consistency  and \n\nplausibility.  The best example is “Cogito ergo sum,”  a Latin sentence that translates as “I \n\nthink, therefore I am.”  It comes from the beginning of a 17th century essay by the Mathe­\n\nmatician/Philospher, Ren´e Descartes, and it is one of the most famous quotes in the world: \n\ndo a web search on the phrase and you will be ﬂooded with hits. \n\nDeducing your existence from the fact that you’re thinking about your existence is a pretty \n\ncool and persuasive­sounding ﬁrst axiom.  However, with just a few more lines of proof in \n\nthis vein, Descartes goes on to conclude that there is an inﬁnitely beneﬁcent God. This ain’t \n\nMath. \n\nMathematics also has a speciﬁc notion of “proof.” \n\nDeﬁnition.  A formal proof  of a proposition is a chain of logical deductions leading to the proposition \n\nfrom a base set of axioms. \n\nThe three key ideas in this deﬁnition are highlighted:  proposition, logical deduction, and axiom. \n\nIn  the  next  sections,  we’ll  discuss  these  three  ideas  along  with  some  basic  ways  of  organizing \n\nproofs. \n\nCopyright © 2005, Prof. Albert R. Meyer. \n\n\n\n", "2 \n\n2  Propositions \n\nCourse Notes, Week 1: Proofs \n\nDeﬁnition.  A proposition is a statement that is either true or false. \n\nThis deﬁnition sounds very general, but it does exclude sentences such as, “Wherefore art thou \n\nRomeo?” and “Give me an A!”. \n\nBut not all propositions are mathematical. For example, “Albert’s wife’s name is ‘Irene’ ” happens \n\nto be true, and could be proved with legal documents and testimony of their children, but it’s not \n\na mathematical statement. \n\nMathematically  meaningful  propositions  must  be  about  well­deﬁned  mathematical  objects  like \n\nnumbers, sets, functions, relations, etc., and they must be stated using mathematically meaning­\n\nful terminology, like ‘AND’ and ‘FORALL’. It’s best to illustrate this with a few examples about \n\nnumbers and planar maps that are all mathematically meaningful. \n\nProposition 2.1.  2 + 3 = 5. \n\nThis proposition is true. \n\nProposition 2.2.  Let p(n) ::=  n2  +  n +  41. \n\n∀n ∈ N.  p(n) is a prime number. \n\nThe symbol ∀ is read “for all”.  The symbol N stands for the set of natural numbers, which are 0, \n\n1, 2, 3, . . . (ask your TA for the complete list).  The period after the N is just a separator between \n\nphrases. \n\nA prime is a natural number greater than one that is not divisible by any other natural number \n\nother than 1 and itself, for example, 2, 3, 5, 7, 11, . . . . \n\nLet’s try some numerical experimentation to check this proposition:  p(0)  = 41  which is prime. \n\np(1)  =  43  which is prime.  p(2)  =  47  which is prime.  p(3)  =  53  which is prime. . . . p(20)  =  461 \n\nwhich is prime. Hmmm, starts to look like a plausible claim. In fact we can keep checking through \n\nn = 39  and conﬁrm that p(39)  =  1601  is prime. \n\nBut  if  n  =  40,  then  p(n)  = 402  +  40  +  41  =  41  · 41,  which  is  not  prime.  Since  the  expression \n\nis not prime for all n,  the proposition is false!  In fact,  it’s not hard to show that no nonconstant \n\npolynomial can map all natural numbers into prime numbers.  The point is that in general you \n\ncan’t check a claim about an inﬁnite set by checking a ﬁnite set of its elements,  no matter how \n\nlarge the ﬁnite set. Here are two even more extreme examples: \n\nProposition 2.3.  a4  + b4  + c4  =  d4  has no solution when a, b, c, d are positive integers. In logical notation, \n\nletting Z+  denote the positive integers, we have \n\n∀a ∈ Z+∀b ∈ Z+∀c ∈ Z+∀d ∈ Z+ .  a 4  +  b4  +  c  =  d4 . \n\n4\n\nStrings of ∀’s like this are usually abbreviated for easier reading: \n\n∀a, b, c, d ∈ Z+ .  a 4  +  b4  +  c  =  d4 . \n\n4\n\nEuler (pronounced “oiler”) conjectured this 1769.  But the proposition was proven false 218 years \n\nlater by Noam Elkies at a liberal arts school up Mass Ave.  He found the solution a  =  95800, b = \n\n217519, c =  414560, d =  422481. \n\n�\n\n�\n\n", "Course Notes, Week 1: Proofs \n\n3 \n\nProposition 2.4.  313(x3  +  y3) =  z3  has no solution when x, y, z  ∈ N. \n\nThis proposition is also false, but the smallest counterexample has more than 1000 digits! \n\nProposition 2.5.  Every map can be colored with 4 colors so that adjacent1  regions have different colors. \n\nThis  proposition  is  true  and  is  known  as  the  “four­color  theorem”.  However,  there  have  been \n\nmany incorrect proofs,  including one that stood for 10 years in the late 19th century before the \n\nmistake was found.  An extremely laborious proof was ﬁnally found in 1976 by mathematicians \n\nAppel and Haken who used a complex computer program to categorize the four­colorable maps; \n\nthe  program  left  a  couple  of  thousand  maps  uncategorized,  and  these  were  checked  by  hand \n\nby  Haken  and  his  assistants—including  his  15­year­old  daughter.  There  was  a  lot  of  debate \n\nabout  whether  this  was  a  legitimate  proof:  the  argument  was  too  big  to  be  checked  without  a \n\ncomputer,  and  no  one  could  guarantee  that  the  computer  calculated  correctly,  nor  did  anyone \n\nhave the energy to recheck the four­colorings of thousands of maps that was done by hand.  Fi­\n\nnally, about ﬁve years ago, a humanly intelligible proof of the four color theorem was found (see \n\nhttp://www.math.gatech.edu/ thomas/FC/fourcolor.html). 2 \n\nProposition 2.6 (Goldbach).  Every even integer greater than 2 is the sum of two primes. \n\nNo one knows whether this proposition is true or false. This is the “Goldbach Conjecture,” which \n\ndates back to 1742. \n\nFor a Computer Scientist, some of the most important questions are about program and system \n\n“correctness”  –  whether  a  program  or  system  does  what  it’s  supposed  to.  Programs  are  noto­\n\nriously buggy,  and there’s a growing community of researchers and practitioners trying to ﬁnd \n\nways to prove program correctness. These efforts have been successful enough in the case of CPU \n\nchips that they are now routinely used by leading chip manufacturers to prove chip correctness \n\nand avoid mistakes like the notorious Intel division bug in the 1990’s. \n\nDeveloping  mathematical  methods  to  verify  programs  and  systems  remains  an  active  research \n\narea. We’ll consider some of these methods later in the course. \n\n3  The Axiomatic Method \n\nThe standard procedure for establishing truth in mathematics was invented by Euclid, a mathe­\n\nmatician working in Alexadria, Egypt around 300 BC. His idea was to begin with ﬁve assumptions \n\nabout geometry, which seemed undeniable based on direct experience.  (For example, “There is \n\na straight line segment between every pair of points.)  Propositions like these that are simply ac­\n\ncepted as true are called axioms. \n\nStarting from these axioms, Euclid established the truth of many additional propositions by pro­\n\nviding “proofs”. A proof  is a sequence of logical deductions from axioms and previously­proved \n\nstatements that concludes with the proposition in question.  You probably wrote many proofs in \n\nhigh school geometry class, and you’ll see a lot more in this course. \n\n1Two regions are adjacent only when they share a boundary segment of positive length. They are not considered to \n\nbe adjacent if their boundaries meet only at a few points. \n\n2The  story  of  the  four­color  proof  is  told  in  a  well­reviewed  recent  popular  (non­technical)  book:  “Four  Colors \n\nSufﬁce. How the Map Problem was Solved.” Robin Wilson. Princeton Univ. Press, 2003, 276pp. ISBN 0­691­11533­8. \n\n\n", "4\t\n\nCourse Notes, Week 1: Proofs \n\nThere are several common terms for a proposition that has been proved.  The different terms hint \n\nat the role of the proposition within a larger body of work. \n\n•\t Important propositions are called theorems. \n\n•\t A lemma is a preliminary proposition useful for proving later propositions. \n\n•\t A corollary is an afterthought, a proposition that follows in just a few logical steps from a \n\ntheorem. \n\nThe deﬁnitions are not precise. In fact, sometimes a good lemma turns out to be far more important \n\nthan the theorem it was originally used to prove. \n\nEuclid’s axiom­and­proof approach, now called the axiomatic method, is the foundation for math­\n\nematics today.  In fact, there are just a handful of axioms, called ZFC, which, together with a few \n\nlogical deduction rules, appear to be sufﬁcient to derive essentially all of mathematics. \n\n3.1  Our Axioms \n\nThe ZFC axioms are important in studying and justifying the foundations of Mathematics.  But \n\nfor practical purposes, they are much too primitive— by one reckoning, proving that 2 + 2 =  4 \n\nrequires more than 20,000 steps! So instead of starting with ZFC, we’re going to take a huge set of \n\naxioms as our foundation: we’ll accept all familiar facts from high school math! \n\nThis will give us a quick launch, but you will ﬁnd this imprecise speciﬁcation of the axioms trou­\n\nbling at times.  For example, in the midst of a proof, you may ﬁnd yourself wondering, “Must I \n\nprove this little fact or can I take it as an axiom?” Feel free to ask for guidance, but really there is no \n\nabsolute answer.  Just be upfront about what you’re assuming, and don’t try to evade homework \n\nand exam problems by declaring everything an axiom! \n\n3.2  Proofs in Practice \n\nIn principle, a proof can be any sequence of logical deductions from axioms and previously­proved \n\nstatements that concludes with the proposition in question.  This freedom in constructing a proof \n\ncan seem overwhelming at ﬁrst. How do you even start a proof? \n\nHere’s the good news: many proofs follow one of a handful of standard templates. Proofs all differ \n\nin the details, of course, but these templates at least provide you with an outline to ﬁll in. We’ll go \n\nthrough several of these standard patterns, pointing out the basic idea and common pitfalls and \n\ngiving some examples. Many of these templates ﬁt together; one may give you a top­level outline \n\nwhile others help you at the next level of detail.  And we’ll show you other, more sophisticated \n\nproof techniques later on. \n\nThe recipes below are very speciﬁc at times, telling you exactly which words to write down on \n\nyour piece of paper.  You’re certainly free to say things your own way instead; we’re just giving \n\nyou something you could say so that you’re never at a complete loss. \n\n", "Course Notes, Week 1: Proofs\n\n\n5 \n\nThe ZFC Axioms \n\nFor  the  record,  we  list  the  axioms  of  Zermelo­Frankel  Set  Theory. \n\nEssentially  all  of  mathematics  can  be  derived  from  these  axioms  to­\n\ngether with a few logical deduction rules. \n\nExtensionality.  Two sets are equal if they have the same members.  In \n\nformal logical notation, this would be stated as: \n\n(∀ z.  (z  ∈ x \n\n←→ z  ∈ y))  −→ x \n\n=  y. \n\nPairing.  For any two sets x  and y, there is a set, { x,  y} , with x  and y  as \n\nits only elements. \n\nUnion.  The union of a collection, z, of sets is also a set. \n\n∃ u∀ x.  (∃ y.  x  ∈ y  ∧ y  ∈ z) ←→ x  ∈ u. \n\nInﬁnity.  There is an inﬁnite set;  speciﬁcally,  a nonempty set,  x,  such \n\nthat for any set y  ∈ x, the set { y} is also a member of x \n\nSubset.  Given any set, x, and any proposition P (y), there is a set con­\n\ntaining precisely those elements y  ∈ x  for which P (y) holds. \n\nPower Set.  All the subsets of a set form another set. \n\nReplacement.  The image of a set under a function is a set. \n\nFoundation.  For every non­empty set, x, there is a set y  ∈ x  such that \n\nx  and y  are disjoint. (In particular, this axiom prevents a set from \n\nbeing a member of itself.) \n\nChoice.  We  can  choose  one  element  from  each  set  in  a  collection  of \n\nnonempty  sets.  More  precisely,  if  f  is  a  function  on  a  set,  and \n\nthe  result  of  applying  f  to  any  element  in  the  set  is  always \n\na  nonempty  set,  then  there  is  a  “choice”  function  g  such  that \n\ng(y) ∈ y  for every y  in the set. \n\nWe’re not going to be working with the ZFC axioms in this course. We \n\njust thought you might like to see them. \n\n\n\n\n\n", "6 \n\nCourse Notes, Week 1: Proofs \n\n4  Proving an Implication \n\nAn enormous number of mathematical claims have the form “If P , then Q” or, equivalently, “P \n\nimplies Q”. Here are some examples: \n\n•  (Quadratic Formula) If ax2  + bx  + c  = 0 and a  = 0, then x  = (−b  ±\n\nb2  − 4ac)/2a. \n\n√\n\n•  (Goldbach’s Conjecture) If n  is an even integer greater than 2, then n  is a sum of two primes. \n\n•  If 0 ≤ x  ≤ 2, then −x3  + 4x  + 1 >  0. \n\nThere are a couple standard methods for proving an implication. \n\n4.1  Method #1 \n\nIn order to prove that P  implies Q: \n\n1.  Write, “Assume P .” \n\n2.  Show that Q  logically follows. \n\nExample \n\nTheorem 4.1.  If 0 ≤ x  ≤ 2, then −x3  + 4x  + 1 >  0. \n\nBefore we write a proof of this theorem, we have to do some scratchwork to ﬁgure out why it is \n\ntrue. \n\nThe inequality certainly holds for x  =  0; then the left side is equal to 1 and 1 >  0. As x  grows, the \n\n4x  term (which is positive) initially seems to have greater magnitude than −x3  (which is negative). \n\nFor example, when x  =  1, we have 4x  =  4, but −x3  =  −1 only.  In fact, it looks like −x3  doesn’t \n\nbegin  to  dominate  until  x  >  2.  So  it  seems  the  −x3  + 4x  part  should  be  nonnegative  for  all  x \n\nbetween 0 and 2, which would imply that −x3  + 4x  + 1 is positive. \n\nSo  far,  so  good.  But  we  still  have  to  replace  all  those  “seems  like”  phrases  with  solid,  logical \n\narguments.  We can get a better handle on the critical −x3  + 4x  part by factoring it, which is not \n\ntoo hard: \n\n−x 3  + 4x  = x(2 − x)(2 + x) \n\nAha!  For x  between 0 and 2, all of the terms on the right side are nonnegative.  And a product \n\nof nonnegative terms is also nonnegative. Let’s organize this blizzard of observations into a clean \n\nproof. \n\n2\n\nProof.  Assume 0 ≤ x  ≤ 2.  Then x, 2 − x, and 2 + x  are all nonnegative.  Therefore, the product of \n\nthese terms is also nonnegative. Adding 1 to this product gives a positive number, so: \n\nMultiplying out on the left side proves that \n\nx(2 − x)(2 + x) + 1 >  0 \n\n−x 3  + 4x  + 1 >  0 \n\nas claimed. \n\n\n\n\n\n\n�\n\n", "Course Notes, Week 1: Proofs\t\n\n7 \n\nThere are a couple points here that apply to all proofs: \n\n•\t You’ll often need to do some scratchwork while you’re trying to ﬁgure out the logical steps \n\nof a proof. Your scratchwork can be as disorganized as you like— full of dead­ends, strange \n\ndiagrams,  obscene words,  whatever.  But keep your scratchwork separate from your ﬁnal \n\nproof, which should be clear and concise. \n\n•\t Proofs typically begin with the word “Proof” and end with some sort of doohickey like � or \n\n“q.e.d”. The only purpose for these conventions is to clarify where proofs begin and end. \n\n4.2  Method #2 ­ Prove the Contrapositive \n\nAn implication (“P  implies Q”) is logically equivalent to its contrapositive “not Q  implies not P ”; \n\nproving one is as good as proving the other.  And often proving the contrapositive is easier than \n\nproving the original statement. If so, then you can proceed as follows: \n\n1.  Write, “We prove the contrapositive:” and then state the contrapositive. \n\n2.  Proceed as in Method #1. \n\nExample \n\nTheorem 4.2.  If r  is irrational, then \n\nr  is also irrational. \n\n√\n\nRecall that rational numbers are equal to a ratio of integers and irrational numbers are not. So we \n\nmust show that if r  is not a ratio of integers, then \n\nr  is also not a ratio of integers.  That’s pretty \n\nconvoluted!  We can eliminate both “not”’s and make the proof straightforward by considering \n\nthe contrapositive instead. \n\n√\n\nProof.  We prove the contrapositive: if \n\nr  is rational, then r  is rational. \n\nAssume that \n\nr  is rational. Then there exists integers a  and b  such that: \n\n√\n\n√\n\n√\n\nr  = \n\na\n\nb \n\nr  = \n\n2a\n\nb2 \n\nSquaring both sides gives: \n\nSince a2  and b2  are integers, r  is also rational. \n\n5  Proving an “If and Only If” \n\nMany mathematical theorems assert that two statements are logically equivalent; that is, one holds \n\nif and only if the other does. Here are some examples: \n\n•\t An integer is a multiple of 3 if and only if the sum of its digits is a multiple of 3. \n\n•\t Two triangles have the same side lengths if and only if all angles are the same. \n\n•\t A positive integer p  ≥ 2 is prime if and only if 1 + (p − 1) · (p − 2)\n\n· · ·\n\n3 2 1 is a multiple of p.\n\n·\n\n·\n\n\n\n\n\n\n\n\n\n\n\n\n", "8 \n\nCourse Notes, Week 1: Proofs \n\n5.1  Method #1: Prove Each Statement Implies the Other \n\nThe  statement  “P  if  and  only  if  Q”  is  equivalent  to  the  two  statements  “P  implies  Q”  and  “Q \n\nimplies P ”. So you can prove an “if and only if” by proving two implications: \n\n1.  Write, “We prove P  implies Q and vice­versa.” \n\n2.  Write, “First, we show P  implies Q.” Do this by one of the methods in Section 4. \n\n3.  Write, “Now, we show Q implies P .” Again, do this by one of the methods in Section 4. \n\nExample \n\nTwo sets are deﬁned to be equal if they contain the same elements; that is, X  =  Y  means z  ∈ X if \n\nand only if z  ∈  Y .  (This is actually the ﬁrst of the ZFC axioms.)  So set equality theorems can be \n\nstated and proved as “if and only if” theorems. \n\nTheorem 5.1 (DeMorgan’s Law for Sets).  Let A, B, and C be sets. Then: \n\nA ∩ (B ∪ C) =  (A ∩ B) ∪ (A ∩ C) \n\nProof.  We show z  ∈ A ∩ (B ∪ C) implies z  ∈ (A ∩ B) ∪ (A ∩ C) and vice­versa. \n\nFirst, we show z  ∈ A ∩ (B ∪ C) implies z  ∈ (A ∩ B) ∪ (A ∩ C). Assume z  ∈ A ∩ (B ∪ C). Then z is in \n\nA and z is also in B or C. Thus, z is in either A ∩ B or A ∩ C, which implies z  ∈ (A ∩ B) ∪ (A ∩ C). \n\nNow, we show z  ∈ (A ∩ B) ∪ (A ∩ C) implies z  ∈ A ∩ (B ∪ C).  Assume z  ∈ (A ∩ B) ∪ (A ∩ C). \n\nThen z is in both A and B or else z is in both A and C. Thus, z is in A and z is also in B or C. This \n\nimplies z  ∈ A ∩ (B ∪ C). \n\n5.2  Method #2: Construct a Chain of Iffs \n\nIn order to prove that P  is true if and only if Q is true: \n\n1.  Write, “We construct a chain of if­and­only­if implications.” \n\n2.  Prove P  is equivalent to a second statement which is equivalent to a third staement and so \n\nforth until you reach Q. \n\nThis method is generally more difﬁcult than the ﬁrst, but the result can be a short, elegant proof. \n\nExample \n\nThe standard deviation of a sequence of values x1, x2, . . . , xn  is deﬁned to be: \n\n� \n\n(x1  − µ)2  + (x1  − µ)2  + . . . + (xn  − µ)\n\n2\n\nwhere µ is the average of the values: \n\nµ = \n\nx1  + x2  + . . . + xn \n\nn \n\n\n\n\n\n\n\n", "Course Notes, Week 1: Proofs \n\n9 \n\nTheorem 5.2.  The standard deviation of a sequence of values x1, . . . , xn  is zero if and only if all the values \n\nare equal to the mean. \n\nFor example, the standard deviation of test scores is zero if and only if everyone scored exactly the \n\nclass average. \n\nProof.  We construct a chain of “if and only if” implications.  The standard deviation of x1, . . . , xn \n\nis zero if and only if: \n\n� \n\n(x1  − µ)2  + (x1  − µ)2  + . . . + (xn  − µ)2  = 0 \n\nwhere µ is the average of x1, . . . , xn. This equation holds if and only if \n\n(x1  − µ)2  + (x1  − µ)2  + . . . + (xn  − µ)2  =  0 \n\nsince zero is the only number whose square root is zero.  Every term in this equation is nonnega­\n\ntive, so this equation holds if and only every term is actually 0. But this is true if and only if every \n\nvalue xi  is equal to the mean µ. \n\nProblem 1.  Reformulate the proof DeMorgan’s Law for Sets as a chain of if­and­only­if implica­\n\ntions. \n\n6  How to Write Good Proofs \n\nThe  purpose  of  a  proof  is  to  provide  the  reader  with  deﬁnitive  evidence  of  an  assertion’s  truth. \n\nTo serve this purpose effectively, more is required of a proof than just logical correctness:  a good \n\nproof  must  also  be  clear.  These  goals  are  usually  complimentary;  a  well­written  proof  is  more \n\nlikely to be a correct proof, since mistakes are harder to hide. \n\nIn practice, the notion of proof is a moving target.  Proofs in a professional research journal are \n\ngenerally unintelligible to all but a few experts who know all the lemmas and theorems assumed, \n\noften without explicit mention, in the proof.  And proofs in the ﬁrst weeks of a beginning course \n\nlike 6.042 would be regarded as tediously long­winded by a professional mathematician.  In fact, \n\nwhat we accept as a good proof later in the term will be different from what we consider good \n\nproofs in the ﬁrst couple of weeks of 6.042. But even so, we can offer some general tips on writing \n\ngood proofs: \n\nState your game plan.  A good proof begins by explaining the general line of reasoning, e.g. “We \n\nuse case analysis” or “We argue by contradiction”.  This creates a rough mental picture into \n\nwhich the reader can ﬁt the subsequent details. \n\nKeep a linear ﬂow.  We sometimes see proofs that are like mathematical mosaics, with juicy tidbits \n\nof reasoning sprinkled across the page. This is not good. The steps of your argument should \n\nfollow one another in a sequential order. \n\nA proof is an essay, not a calculation.  Many students initially write proofs the way they compute \n\nintegrals.  The  result  is  a  long  sequence  of  expressions  without  explantion.  This  is  bad. \n\nA  good  proof  usually  looks  like  an  essay  with  some  equations  thrown  in.  Use  complete \n\nsentences. \n\n\n\n\n\n\n", "10 \n\nCourse Notes, Week 1: Proofs \n\nAvoid excessive symbolism.  Your  reader  is  probably  good  at  understanding  words,  but  much \n\nless skilled at reading arcane mathematical symbols.  So use words where you reasonably \n\ncan. \n\nSimplify.  Long, complicated proofs take the reader more time and effort to understand and can \n\nmore easily conceal errors. So a proof with fewer logical steps is a better proof. \n\nIntroduce notation thoughtfully.  Sometimes an argument can be greatly simpliﬁed by introduc­\n\ning a variable,  devising a special notation,  or deﬁning a new term.  But do this sparingly \n\nsince you’re requiring the reader to remember all that new stuff. And remember to actually \n\ndeﬁne the meanings of new variables, terms, or notations; don’t just start using them! \n\nStructure long proofs.  Long programs are usually broken into a heirarchy of smaller procedures. \n\nLong proofs are much the same.  Facts needed in your proof that are easily stated, but not \n\nreadily  proved  are  best  pulled  out  and  proved  in  preliminary  lemmas.  Also,  if  you  are \n\nrepeating essentially the same argument over and over,  try to capture that argument in a \n\ngeneral lemma, which you can cite repeatedly instead. \n\nDon’t bully.  Don’t use phrases like “clearly” or “obviously” in an attempt to bully the reader into \n\naccepting something which you’re having trouble proving.  Also, go on the alert whenever \n\nyou see one of these phrases is someone else’s proof. \n\nFinish.  At some point in a proof, you’ll have established all the essential facts you need. Resist the \n\ntemptation to quit and leave the reader to draw the “obvious” conclusion.  What is obvious \n\nto you as the author is not likely to be obvious to the reader. Instead, tie everything together \n\nyourself and explain why the original claim follows. \n\nThe analogy between good proofs and good programs extends beyond structure.  The same rig­\n\norous  thinking  needed  for  proofs  is  essential  in  the  design  of  critical  computer  system.  When \n\nalgorithms and protocols only “mostly work” due to reliance on hand­waving arguments, the re­\n\nsults can range from problematic to catastrophic. An early example was the Therac 25, a machine \n\nthat provided radiation therapy to cancer victims, but occasionally killed them with massive over­\n\ndoses due to a software race condition.  More recently, in August 2004, a single faulty command \n\nto a computer system used by United and American Airlines grounded the entire ﬂeet of both \n\ncompanies— and all their passengers! \n\nIt is a certainty that we’ll all one day be at the mercy of critical computer systems designed by \n\nyou and your classmates. So we really hope that you’ll develop the ability to formulate rock­solid \n\nlogical arguments that a system actually does what you think it does! \n\n7  Propositional Formulas \n\nIt’s really sort of amazing that people manage to communicate in the English language.  Here are \n\nsome typical sentences: \n\n1.  “You may have cake or you may have ice cream.” \n\n2.  “If pigs can ﬂy, then you can understand the Chernoff bound.” \n\n", "Course Notes, Week 1: Proofs \n\n11 \n\n3.  “If you can solve any problem we come up with, then you get an A for the course.” \n\n4.  “Every American has a dream.” \n\nWhat precisely do these sentences mean? Can you have both cake and ice cream or must you choose \n\njust one desert?  If the second sentence is true, then is the Chernoff bound incomprehensible?  If \n\nyou can solve some problems we come up with but not all, then do you get an A for the course? \n\nAnd can you still get an A even if you can’t solve any of the problems?  Does the last sentence \n\nimply that all Americans have the same dream or might they each have a different dream? \n\nSome  uncertainty  is  tolerable  in  normal  conversation.  But  when  we  need  to  formulate  ideas \n\nprecisely—  as  in  mathematics—  the  ambiguities  inherent  in  everyday  language  become  a  real \n\nproblem.  We can’t hope to make an exact argument if we’re not sure exactly what the individual \n\nwords mean. (And, not to alarm you, but it is possible that we’ll be making an awful lot of exacting \n\nmathematical arguments in the weeks ahead.)  So before we start into mathematics, we need to \n\ninvestigate the problem of how to talk about mathematics. \n\nTo  get  around  the  ambiguity  of  English,  mathematicians  have  devised  a  special  mini­language \n\nfor  talking  about  logical  relationships.  This  language  mostly  uses  ordinary  English  words  and \n\nphrases such as “or”, “implies”, and “for all”.  But mathematicians endow these words with def­\n\ninitions more precise than those found in an ordinary dictionary.  Without knowing these deﬁni­\n\ntions, you could sort of read this language, but you would miss all the subtleties and sometimes \n\nhave trouble following along. \n\nSurprisingly, in the midst of learning the language of logic, we’ll come across the most important \n\nopen problem in computer science— a problem whose solution could change the world. \n\n7.1  Combining Propositions \n\nIn  English,  we  can  modify,  combine,  and  relate  propositions  with  words  such  as  “not”,  “and”, \n\n“or”, “implies”, and “if­then”. For example, we can combine three propositions into one like this: \n\nIf all humans are mortal and all Greeks are human, then all Greeks are mortal. \n\nFor  the  next  while,  we  won’t  be  much  concerned  with  the  internals  of  propositions—  whether \n\nthey involve mathematics or Greek mortality— but rather with how propositions are combined \n\nand related.  So we’ll frequently use variables such as P  and Q  in place of speciﬁc propositions \n\nsuch  as  “All  humans  are  mortal”  and  “2 + 3 =  5”.  The  understanding  is  that  these  variables, \n\nlike propositions, can take on only the values T(true) and F(false).  Such true/false variables are \n\nsometimes called Boolean variables after their inventor, George— you guessed it— Boole. \n\n7.1.1  “Not”, “And” and “Or” \n\nWe  can  precisely  deﬁne  these  special  words  using  truth  tables.  For  example,  if  P  denotes  an \n\narbitrary proposition, then the truth of the proposition “not P ” is deﬁned by the following truth \n\ntable: \n\nP  not P \n\nT \n\nF \n\nF \n\nT \n\n\n\n\n\n", "12 \n\nCourse Notes, Week 1: Proofs \n\nThe  ﬁrst  row  of  the  table  indicates  that  when  proposition  P is  true,  the  proposition  “not  P ”  is \n\nfalse (F). The second line indicates that when P is false, “not P ” is true. This is probably what you \n\nwould expect. \n\nIn general, a truth table indicates the true/false value of a proposition for each possible setting of \n\nthe variables.  For example, the truth table for the proposition “P and Q” has four lines, since the \n\ntwo variables can be set in four different ways: \n\nAccording to this table, the proposition “P and Q” is true only when P and Q are both true. This \n\nis probably reﬂects the way you think about the word “and”. \n\nThere is a subtlety in the truth table for “P or Q”: \n\nThis says that “P or Q” is true when P is true, Q is true, or both are true.  This isn’t always the \n\nintended meaning of “or” in everyday speech, but this is the standard deﬁnition in mathematical \n\nwriting.  So if a mathematician says, “You may have cake or your may have ice cream”, then you \n\ncould have both. \n\n7.1.2  “Implies” \n\nThe least intuitive connecting word is “implies”.  Mathematicians regard the propositions “P im­\n\nplies  Q”  and  “if  P then  Q”  as  synonymous,  so  both  have  the  same  truth  table.  (The  lines  are \n\nnumbered so we can refer to the them later.) \n\nP Q\n\n\nT  T \n\nT  F \n\nF  T \n\nF  F \n\nP and Q\n\n\nT \n\nF \n\nF \n\nF \n\nP Q\n\nT  T \n\nT  F \n\nF  T \n\nF  F \n\nP or Q \n\nT \n\nT \n\nT \n\nF \n\nP implies Q,\n\n\nif P then Q\n\n\nP Q\n\n\n1. T  T \n\n2. T  F \n\n3. F  T \n\n4. F  F \n\nT \n\nF \n\nT \n\nT \n\nLet’s experiment with this deﬁnition. For example, is the following proposition true or false? \n\n“If Goldbach’s Conjecture is true, then x2  ≥ 0 for every real number x.” \n\nNow, we told you before that no one knows whether Goldbach’s Conjecture is true or false.  But\n\n\nthat doesn’t prevent you from answering the question!  This proposition has the form P −→  Q\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "Course Notes, Week 1: Proofs \n\n13 \n\nwhere P is “Goldbach’s Conjecture is true” and Q is “x2  ≥ 0  for every real number x”. Since Q is \n\ndeﬁnitely true, we’re on either line 1 or line 3 of the truth table.  Either way, the proposition as a \n\nwhole is true! \n\nOne of our original examples demonstrates an even stranger side of implications. \n\n“If pigs ﬂy, then you can understand the Chernoff bound.” \n\nDon’t take this as an insult;  we just need to ﬁgure out whether this proposition is true or false. \n\nCuriously,  the answer has nothing to do with whether or not you can understand the Chernoff \n\nbound.  Pigs  do not ﬂy,  so  we’re on either line 3  or line 4  of the truth  table.  In both cases,  the \n\nproposition is true! \n\nIn contrast, here’s an example of a false implication: \n\n“If the moon shines white, then the moon is made of white cheddar.” \n\nYes, the moon shines white.  But, no, the moon is not made of white cheddar cheese.  So we’re on \n\nline 2 of the truth table, and the proposition is false. \n\nThe truth table for implications can be summarized in words as follows: \n\nAn implication is true when the if­part is false or the then­part is true. \n\nThis  sentence  is  worth  remembering;  a  large  fraction  of  all  mathematical  statements  are  of  the \n\nif­then form! \n\n7.1.3  “If and Only If” \n\nMathematicians commonly join propositions in one additional way that doesn’t arise in ordinary \n\nspeech. The proposition “P if and only if Q” asserts that P and Q are logically equivalent; that is, \n\neither both are true or both are false. \n\nP Q\n\n\nT  T \n\nT  F \n\nF  T \n\nF  F \n\nP if and only if Q\n\n\nT \n\nF \n\nF \n\nT \n\nThe following if­and­only­if statement is true for every real number x: \n\n“x2  − 4  ≥ 0  if and only if  x ≥ 2” \n\n| \n\n|\n\nFor some values of x, both inequalities are true. For other values of x, neither inequality is true . In \n\nevery case, however, the proposition as a whole is true. \n\nThe phrase “if and only if” comes up so often that it is often abbreviated “iff”. \n\n\n\n\n\n\n\n", "14 \n\nCourse Notes, Week 1: Proofs \n\n7.2  Propositional Logic in Computer Programs \n\nPropositions and logical connectives arise all the time in computer programs.  For example, con­\n\nsider the following snippet, which could be either C, C++, or Java: \n\nif  (  x  >  0  ||  (x  <=  0  &&  y  >  100)  )\n\n\n. . . \n\n(further instructions) \n\nThe symbol || denotes “or”, and the symbol && denotes “and”. The further instructions are carried \n\nout only if the proposition following the word if is true. On closer inspection, this big expression \n\nis  built  from  two  simpler  propositions.  Let  A be  the  proposition  that  x > 0,  and  let  B be  the \n\nproposition that y  >  100. Then we can rewrite the condition this way: \n\nA truth table reveals that this complicated expression is logically equivalent to “A or B”. \n\nA or ((not A) and B) \n\nA B\n\nT T \n\nT F \n\nF T \n\nF F \n\nA or ((not A) and B)  A or B \n\nT \n\nT \n\nT \n\nF \n\nT \n\nT \n\nT \n\nF \n\nThis means that we can simplify the code snippet without changing the program’s behavior: \n\nif  (  x  >  0  ||  y  >  100  )\n\n\n(further instructions) \n\nRewriting a logical expression involving many variables in the simplest form is both difﬁcult and \n\nimportant. Simplifying expressions in software might slightly increase the speed of your program. \n\nBut, more signiﬁcantly, chip designers face essentially the same challenge.  However, instead of \n\nminimizing && and || symbols in a program, their job is to minimize the number of analogous \n\nphyscial  devices  on  a  chip.  The  payoff  is  potentially  enormous:  a  chip  with  fewer  devices  is \n\nsmaller, consumes less power, has a lower defect rate, and is cheaper to manufacture. \n\n7.3  A Cryptic Notation \n\nProgramming languages use symbols like &&  and ! in place of words like “and” and “not”. Math­\n\nematicians have devised their own cryptic symbols to represent these words, which are summa­\n\nrized in the table below. \n\nEnglish \n\n“not P ” \n\n“P and Q” \n\n“P or Q” \n\n“P\n\nimplies Q” or “if P then Q” \n\n“P if and only if Q” \n\nCryptic Notation \n\n¬ P \n\n(alternatively, P ) \n\nP ∧ Q \n\nP ∨ Q \n\nP −→ Q \n\nP ←→ Q \n\n\n\n\n\n\n\n\n\n\n\n\n\n", "Course Notes, Week 1: Proofs \n\n15 \n\nFor example, using this notation, “If P and not Q, then R” would be written: \n\n(P ∧ ¬ Q) −→ R \n\nThis symbolic language is helpful for writing complicated logical expressions compactly.  But in \n\nmost  contexts  ordinary  words  such  as  “or”  and  “implies”  are  much  easier  to  understand  than \n\nsymbols such as ∨ and −→ .  So we’ll use this symbolic language sparingly, and we advise you to \n\ndo the same. \n\n7.4  Logically Equivalent Implications \n\nAre these two sentences saying the same thing? \n\nIf I am hungry, then I am grumpy.\n\n\nIf I am not grumpy, then I am not hungry.\n\n\nWe can settle the issue by recasting both sentences in terms of propositional logic.  Let P be the \n\nproposition “I am hungry”, and let Q be “I am grumpy”.  The ﬁrst sentence says “P implies Q” \n\nand the second says “(not Q) implies (not P )”.  We can compare these two statements in a truth \n\ntable: \n\nQ P implies Q (not Q) implies (not P ) \n\nP\n\nT T \n\nT F \n\nF T \n\nF F \n\nT \n\nF \n\nT \n\nT \n\nSure enough, the two statements are precisely equivalent. In general, “(not Q) implies (not P )” is \n\ncalled the contrapositive of “P implies Q”.  And, as we’ve just shown, the two are just different \n\nways of saying the same thing.  This equivalence is mildly useful in programming, mathematical \n\narguments, and even everyday speech, because you can always pick whichever of the two is easier \n\nto say or write. \n\nIn contrast, the converse of “P implies Q” is the statement “Q implies P ”. In terms of our example, \n\nthe converse is: \n\nThis sounds like a rather different contention, and a truth table conﬁrms this suspicion: \n\nIf I am grumpy, then I am hungry. \n\nQ P implies Q Q implies P \n\nP\n\nT T \n\nT F \n\nF T \n\nF F \n\nT \n\nF \n\nT \n\nT \n\nThus, an implication is logically equivalent to its contrapositive, but is not equivalent to its con­\n\nverse. \n\nOne ﬁnal relationship: an implication and its converse together are equivalent to an if and only if \n\nstatement, speciﬁcally, to these two statements together. For example, \n\nT \n\nF \n\nT \n\nT \n\nT \n\nT \n\nF \n\nT \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "16 \n\nCourse Notes, Week 1: Proofs\n\n\nIf I am grumpy, then I am hungry. \n\nIf I am hungry, then I am grumpy. \n\nare equivalent to the single statement: \n\nI am grumpy if and only if I am hungry. \n\nOnce again, we can verify this with a truth table: \n\nQ (P implies Q) and (Q implies P )  Q if and only if P \n\nP\n\nT T \n\nT F \n\nF T \n\nF F \n\nT \n\nF \n\nF \n\nT \n\nT \n\nF \n\nF \n\nT \n\n8  Logical Deductions \n\nLogical deductions or inference rules are used to prove new propositions using previously proved \n\nones. \n\nA fundamental inference rule is modus ponens.  This rule says that a proof of P together with a \n\nproof of P −→ Q is a proof of Q. \n\nInference rules are sometimes written in a funny notation. For example, modus ponens is written: \n\nWhen the statements above the line, called the antecedents, are proved, then we can consider the \n\nstatement below the line, called the conclusion or consequent, to also be proved. \n\nA key requirement of an inference rule is that it must be sound:  any assignment of truth values \n\nthat makes all the antecedents true must also make the consequent true.  So it we start off with \n\ntrue axioms and apply sound inference rules, everything we prove will also be true. \n\nThere are many other natural, sound inference rules, for example: \n\nP, P −→ Q \n\nQ \n\nP −→ Q, Q −→ R \n\nP −→ R \n\n¬ P −→ Q, ¬ Q \n\nP \n\n¬ Q\n\n¬  −→ \n\nP\n\nQ −→ P \n\nRule. \n\nRule. \n\nRule. \n\nRule. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "Course Notes, Week 1: Proofs\n\n\n17 \n\nSAT \n\nA proposition is satisﬁable if some setting of the variables makes \n\nthe  proposition  true.  For  example,  P  ∧ ¬Q  is  satisﬁable  because  the \n\nexpression is true when P  is true and Q  is false.  On the other hand, \n\nP  ∧ ¬P  is not satisﬁable because the expression as a whole is false for \n\nboth settings of P . But determining whether or not a more complicated \n\nproposition is satisﬁable is not so easy. How about this one? \n\n(P  ∨ Q ∨ R) ∧ (¬P  ∨ ¬Q) ∧ (¬P  ∨ ¬R) ∧ (¬R  ∨ ¬Q) \n\nThe general problem of deciding whether a proposition is satisﬁable \n\nis  called  SAT .  One  approach  to  SAT  is  to  construct  a  truth  table  and \n\ncheck whether or not a T  ever appears.  But this approach is not very \n\nefﬁcient; a proposition with n  variables has a truth table with 2n  lines. \n\nFor a proposition with just 30 variables, that’s already over a billion! \n\nIs there an efﬁcient solution to SAT? Is there some ingenious proce­\n\ndure  that  quickly  determines  whether  any  given  proposition  is  satiﬁ­\n\nable or not? No one knows. And an awful lot hangs on the answer. An \n\nefﬁcient solution to SAT would immediately imply efﬁcient solutions \n\nto many, many other important problems involving packing, schedul­\n\ning,  routing,  and circuit veriﬁcation.  This sounds fantastic,  but there \n\nwould also be worldwide chaos.  Decrypting coded messages would \n\nalso  become  an  easy  task  (for  most  codes).  Online  ﬁnancial  transac­\n\ntions would be insecure and secret communications could be read by \n\neveryone. \n\nAt present, though, researchers are completely stuck.  No one has a \n\ngood idea how to either solve SAT more efﬁciently or to prove that no \n\nefﬁcient solution exists.  This is the outstanding unanswered question \n\nin computer science. \n\n\n\n\n\n", "18 \n\nRule. \n\nOn the other hand, \n\n¬ P  −→ ¬ Q \n\nP  −→ Q \n\nCourse Notes, Week 1: Proofs \n\nis not sound: if P  is assigned T and Q is assigned F, then the antecedent is true and the consequent \n\nis not. \n\nProblem 2.  Prove that a propositional inference rule is sound iff the conjunction (AND) of all its \n\nantecedents implies its consequent. \n\nAs with axioms, we will not be too formal about the set of legal inference rules.  Each step in a \n\nproof should be clear and “logical”; in particular, you should state what previously proved facts \n\nare used to derive each new conclusion. \n\n\n"]], ["c:\\users\\mjols\\documents\\ds uwec courses\\740 data mining\\Course-Syllabus-for-DS-740.pdf", ["Course Syllabus for DS 740: Data Mining \n\nNOTE: This syllabus document contains the basic information of this course. The most \n\ncurrent syllabus is available in the full course. \n\n \n\nCourse Description \n\nData mining methods and procedures for diagnostic and predictive analytics. Topics \n\ninclude association rules, clustering algorithms, tools for classification, and ensemble \n\nmethods such as bagging and boosting. Computer implementation and applications will \n\nbe emphasized. \n\n \n\nCourse Objectives By the end of this course, you will be able to: \n\n●  Compare and decide among methods of data mining. \n\n●  Use multiple linear regression for prediction. \n\n●  Use k-nearest neighbors for prediction. \n\n●  Use methods based on extending linear models for classification. \n\n●  Use data to honestly assess predictive ability and precision of data-mining \n\nprocedures. \n\n●  Use trees for classification and prediction. \n\n●  Conduct an analysis using unsupervised learning. \n\n●  Plan and execute an analysis using data mining.  \n\nYour mastery of course content is assessed using a variety of methods: \n\nGrading Policy \n\n \n\nActivity \n\nMidterm Project \n\nFinal Project \n\nTotal \n\nHomework Assignments \n\nParticipation in required learning activities \n\n(WebWork) \n\nPercentage of Grade \n\n60% \n\n10% \n\n10% \n\n20% \n\n100% \n\n \n\nFinal grades are assigned using the following scale: \n\n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "A  90-100% \n\nB  80-89% \n\nC  70-79% \n\nD  60-69% \n\n \n\n \n\n \n\nF  At or below 59% \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"]], ["c:\\users\\mjols\\documents\\ds uwec courses\\740 data mining\\ds740_lesson1_KNN classification.pdf", [" \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nImportant note: Transcripts are not substitutes for textbook assignments. \n\n \n\n \n\n\n\n", " \n\n \n\n \n\n \n\n \n\n\n", " \n\n \n\nK nearest-neighbors is a simple but effective method of supervised learning that can be \n\nused for classification or regression problems. It's non-parametric, which means it \n\ndoesn't assume a particular shape of the model for the relationship between the \n\npredictor and response variables. Here's how it works. \n\n \n\nSuppose we have two categories of data, blue squares and red triangles, and we'd like \n\nto predict which category the new data point, the black circle, falls into. If we're using 1 \n\nnearest-neighbors, then we would just look at the one neighbor, the data point that's \n\nclosest to that black circle. It's in the red triangle category, so that's the category that \n\nwe would predict for the black circle. \n\n \n\nFor 3 nearest-neighbors, we would simply look out a bit farther and look at the three \n\ndata points that are closest to the black circle. Of these, two are red triangles and only \n\none is a blue square. So we would still assign the black circle to the red triangle \n\ncategory. When you have two categories, it's generally a good idea to choose K to be \n\nsome odd number so that you can avoid having ties. \n\n \n\n \n\n \n\n \n\n\n", " \n\n \n\n \n\n \n\n \n\n\n", " \n\n \n\nThis slide represents a video/screencast in the lecture. The transcript does not \n\nsubstitute video content. \n\n \n\nLet's use k nearest neighbors to predict whether people will default on their credit card \n\ndebt. I've already installed the ISLR package, which contains the data set we'll be using, \n\nas well as the FNN package, which contains the function for k nearest neighbors. So I'm \n\ngoing to start by loading those libraries. And I'll also load dplyr and ggformula, which \n\nwe'll use to manipulate the data and make some graphs later on.  \n\n \n\nNext, let's look at the data to get a sense of what we're working with. The head function \n\nwill show us the first six rows of data. And we can see we have four columns. It looks \n\nlike default with a lowercase D and student are categorical variables, and balance and \n\nincome are quantitative variables.  \n\n \n\nThe summary function will give us some basic summaries of the data. So for the \n\ncategorical variables, we can see how many nos and yeses there are, and for the \n\nquantitative variables, we can see their minimum, their maximum, and the median, \n\nmean, and first and third quartiles. Looking at this information shows us that there are \n\nno NAs, so no missing data. And the quantitative values look like-- don't have any \n\nunusual values, such as people with a negative income, or anything like that. So we \n\ndon't have any outliers to deal with. So let's proceed with preparing our data set for the \n\nanalysis.  \n\n \n\nWe want to start by making sure that all of our predictor variables are quantitative \n\nbecause that's what KNN expects. So we need to convert the categorical variable \n\nstudent into a dummy variable or indicator variable, where a 0 will indicate that no, a \n\n \n\n \n\n\n", "person is not a student, and a 1 will indicate that, yes, they are a student. There are \n\nmany different ways to do this. I'm doing it using the mutate function from dplyr and \n\nthe ifelse function.  \n\n \n\nSo the way to interpret this is we first to check if the value student equals no. \n\nRemember to use a double equals sign to check for equality. If that's true, then we put \n\nthe value 0 in. And if it's false, then we put the value 1 in. And in doing this, I'm creating \n\na copy of the data frame called Default2, just so that it's easy to go back to the original \n\nversion of the data in case I don't like my work. So we can look at the first six lines of \n\nthis new version of the data set, and we see that student is indeed changed into zeros \n\nand ones.  \n\n \n\nThe next thing to do is to split the data into a training set and a test set. We'll use the \n\ntraining set to build or create the model, and then we'll use the test set to make \n\npredictions and check the model's accuracy. In this way, we're not checking the model's \n\naccuracy on the exact same data that we used to create the model. So we get a more \n\nhonest sense of how well the model would do on new data.  \n\n \n\nIn order to do this, I'm starting by setting a random seed using the set.seed function. \n\nAnd then you can put any positive integer in as an argument. I just used 123. Then I'm \n\ncreating a groups vector, where I'll use the number 1 to indicate that a data point is in \n\nthe training set and a number 2 to indicate that it's in the test set. It's pretty common to \n\nuse about 2/3 of your data for the training set. And our data set here had 10,000 rows, \n\nwhich we could have seen by using the dim function, D-I-M.  \n\n \n\nSo here I'm repeating the number 1 6,666 times, and I'm repeating the number 2 3,334 \n\ntimes. So this is great, but now the vector groups has all of the first 2/3 of the data in \n\nthe training set, and all of the last 1/3 in the test set. We probably don't want to do that. \n\nIt's better to mix things up and have a random sample in the training set. So now we can \n\nmix up our groups variable to create the variable random_groups using the sample \n\nfunction.  \n\n \n\nSo here we're sampling 10,000 elements, or all of them, from the vector groups. And by \n\ndefault, the sample function does this without replacement. So this is simply creating a \n\nrandom permutation or rearrangement of the groups vector. Then we'll let in_train be \n\nequal to random groups double equal to 1. So this is going to be equal to true for all of \n\nthe elements where random groups was equal to 1. So it's equal to true for all of the \n\ndata points that should be in the training set. And it'll be equal to false for all the data \n\npoints in the test set.  \n\n \n\nSo looking at the first six elements of in_train, we see that the first two elements or first \n\ntwo rows are in the training set, and then the third row is in the test set, and so on.  \n\n \n\n \n\n \n\n", " \n\n \n\nThis slide represents a video/screencast in the lecture. The transcript does not \n\nsubstitute video content. \n\n \n\nIntuitively, a difference of, say, 1,000 means a different thing for each of our two \n\nquantitative variables. For example, two people who have incomes that are $1,000 \n\napart have pretty similar incomes. But if one person has a credit card balance of $500 \n\nand another person has a credit card balance of $1,500, that's a pretty big difference. \n\nThe second person's credit card balance is three times the credit card balance of the \n\nfirst person.  \n\n \n\nSo we have this sense that we shouldn't treat $1,000 as being the same for each of \n\nthese two variables. And we can verify that by looking at the standard deviations of \n\nthese two variables. We see that the standard deviation of balance is just under $500. \n\nSo a difference of $1,000 would be a little over two standard deviations. And the \n\nstandard deviation of income is over $13,000. So a difference of $1,000 in income is less \n\nthan one standard deviation-- not a very big difference.  \n\n \n\nHowever, k nearest neighbors uses Euclidean distance to define how similar two points \n\nare. So it would use a difference of 1,000 as being the same, regardless of which variable \n\nit came from. We don't want that. We instead want to rescale these variables so that \n\nour k nearest neighbors will work in terms of standard deviations, not in terms of raw \n\ndollar amounts.  \n\n \n\nWe can do that by scaling or standardizing the data. In other words, we're subtracting \n\nthe mean of each variable and dividing by the standard deviation. So each variable will \n\nnow have a mean of 0 and a standard deviation of 1. Here I'm using the scale function \n\n \n\n \n\n\n", "on columns 3 and 4, so that's balance and income, of the training set of Default2. And \n\nI'm going to store that in a new variable, quant_train_std. So in other words, the \n\nstandardized version of the quantitative variables in the training set.  \n\n \n\nWe want to standardize the training set by itself based on its mean and standard \n\ndeviation, because the most honest way of assessing our model is not to let the training \n\nset know anything about the test set before we run the model. So in other words, we're \n\nnot even being affected by the values from the test set in determining the mean and \n\nstandard deviation we use to scale the training set.  \n\n \n\nBut we do want a mean of 0 to represent the same thing in both the training set and \n\ntest set. So we want to scale the test set, which I'm denoting by !in_train to get the \n\nvalues where in_train is not true. In other words, it's false. I'm scaling that based on the \n\ncenter equal to the center attribute of the training set, and the scale or standard \n\ndeviation based on the scale attribute of the training set.  \n\n \n\nSo this whole second line, or lines 3 through 4, is simply using the original mean and \n\nstandard deviation from the training set from before we did the scaling in order to do \n\nthe scaling of the test set. That way a value of 0 in the training set after it's standardized \n\nwill mean the same thing as a value of 0 in the test set after it's standardized.  \n\n \n\nAll right. Now we're ready to build the model. I've started by creating a variable called \n\nx_train by simply combining the value of student in the training set with the \n\nstandardized quantitative variables from the training set using cbind to call and bind \n\nthose into a single matrix. And similarly, x_test contains the value of student and the \n\nstandardized quantitative variables for the test set.  \n\n \n\nThen we can use the knn function from the fnn package with the arguments train equal \n\nto the x values or predictor values of the training set, and test equal to the predictor \n\nvalues of the test set. cl is the classifications, or response values of the training set. So \n\nthat's the first column of the Default2 data frame was whether the person defaulted on \n\ntheir credit card balance.  \n\n \n\nAnd then the final argument is k, the number of nearest neighbors we want to consider \n\nin order to classify each point from the test set. And here we'll just use k equal to 1, so \n\none nearest neighbor. I'm storing those results in predictions, and then we'll look at the \n\nfirst few entries in that vector.  \n\n \n\nSo here we see that predictions is a vector with levels no and yes. And it looks like the \n\nfirst six people in that vector all had a prediction of, no, they will not default on their \n\ncredit card debt. \n\n \n\n \n\n \n\n", " \n\n \n\nWe can compute the error rate of our model on the validation or test set by using the \n\ntable function in R to compute the confusion matrix. Here I've used predictions as the \n\nfirst argument in the table function. So the predicted response values will be \n\nrepresented in the rows of the confusion matrix. The actual observed response values \n\nare represented in the columns.  \n\n \n\nIn the confusion matrix, the elements on the main diagonal are the number of \n\nobservations that were correctly predicted. The elements on the off diagonal, the 74 \n\nand 71, are the elements that were misclassified. We said the prediction was no, but the \n\ntruth was yes, or we said, yes, and the truth was no. That means that the overall error \n\nrate is the sum of these two values, 74 plus 71, divided by the total number of \n\nobservations in the validation set, which is also the total number of observations in the \n\nconfusion matrix. In this case, we get 0.043, so an overall error rate of about 4%, which \n\nis a pretty good overall error rate.  \n\n \n\nWe can also compute the error rate on subsets of the data. For example, in this case, \n\nthe Yes column represents people who really did default on their credit card debt. So we \n\ncan compute the proportion of people who actually defaulted who were misclassified-- \n\nthat is incorrectly predicted as not defaulting-- by taking the 74 divided by the total in \n\nthe Yes column. That gets us 0.692, or about 69%, which is a much worse error rate.  \n\n \n\n \n\n \n\n\n", " \n\n \n\nQuestion 1 \n\n \n\n \n\n \n\n \n\n\n", " \n\n \n\nAn important question when using K nearest-neighbors is what value of K to choose. As \n\nyou increase K-- the number of neighbors used to classify each point-- the flexibility of \n\nthe model decreases. That means that the variance decreases and the bias increases \n\nuntil, in the extreme case, when K equals the entire size of the training data set, every \n\npoint is classified as belonging to the same category, whichever category is most \n\ncommon in the training data. Because of the trade-off between bias and variance, we \n\nexpect the error to decrease and then increase as we increase K. So we expect there to \n\nbe some middle point between K equals 1 and K equals the entire training data set \n\nwhen the overall error rate is at a minimum. \n\n \n\n \n\n \n\n \n\n\n", " \n\n \n\nThis slide represents a video/screencast in the lecture. The transcript does not \n\nsubstitute video content. \n\n \n\nLet's test a variety of different values of k to find which one gives us the best model for \n\nthis data. Here we have the same code as we had before running k nearest neighbors \n\nwith k equal to 1. And we're computing the confusion matrix for our results. The only \n\ndifference is that we're storing the confusion matrix in a variable, which I've called \n\nconf_mat.  \n\n \n\nThen on the next line, I'm taking the diagonal elements of the confusion matrix and \n\nfinding their sum. So this will tell me the total number of correctly classified \n\nobservations. And I'm dividing that by the total number of observations in the validation \n\nset. So overall, this line of code is giving me the accuracy on the validation set, which is 1 \n\nminus the overall error rate.  \n\n \n\nNow we want to take this whole chunk of code and iterate it over many different values \n\nof k. Let's say values of k from 1 up to 150. And because our response variable is binary-- \n\nthat is the response values of default can be either yes or no, two possible values-- we'd \n\nlike to avoid tie votes between the number of nearest neighbors voting for yes versus \n\nthe number voting for no.  \n\n \n\nSo let's go ahead and test all the odd numbers of neighbors from 1 up to 149. We can do \n\nthat using the seq function to create a sequence of numbers from 1 up to 150 by \n\nincrements of two. And we're storing that in a vector of k_vals. I've also created a vector \n\nhere to store the accuracy values for each value of k. And that's just an empty vector \n\nwith a numeric type. And its length is going to be equal to the length of the k_vals \n\nvector.  \n\n \n\n \n\n\n", " \n\nThen we want to iterate our code over all the values in k_vals. We can do that using a \n\nfor loop, for ii in 1 colon the length of k_vals. So for every element in k_vals, we'll do the \n\ncode once. And here I'm using a double letter, ii, for my index of which iteration I'm on. \n\nI've learned that using double letters, say ii, instead of just the letter i, for my iteration \n\nindices makes it much easier to find where in my code I've done iterations.  \n\n \n\nAll right. So now I'm going to take the code that we wrote previously and Control-C, \n\nControl-V to copy it into my for loop, and I can indent it to make it more obvious that it's \n\ninside the for loop. And there are two things we want to change here. First, we want to \n\nchange the value of k from 1 to the ii-th entry in k_vals. Then we want to be able to \n\nstore the accuracy from each iteration. So we'll store that in the ii-th position of \n\naccuracy.  \n\n \n\nAll right. And now we'll run this code. This will take a little bit of time because we do \n\nneed to compute the nearest neighbors model for each value of k from 1 up to 149. All \n\nright. And now we can scroll down and plot the results. Here I'm using the gf_line \n\nfunction from the ggformula package.  \n\n \n\nAnd I want to plot the accuracy on the y-axis as a function of the value of k. And this \n\nmakes sense to have accuracy on the y-axis and k on the x-axis, because it makes more \n\nsense to think about k influencing the accuracy, rather than the other way around. And \n\nthen I'm just using the argument lwd equal to 2 to set the line width equal to 2 to make \n\nit a little easier to read.  \n\n \n\nSo here we have our graph of the accuracy. It looks like by increasing k from 1 up to \n\nsomething a little bit bigger than 1, we get a big increase in accuracy. We then have a \n\npeak in accuracy around 25 for k, and again around 40. So it looks like any value of k \n\nbetween about 13 and about 50 would be reasonable.  \n\n \n\nIf we want to find the exact maximum on this validation set, we can use + function max \n\nof accuracy to find that the maximum accuracy was 97.4%. And then we can use \n\nwhich.max(accuracy) to tell us which position in the k values, which value of ii, gave the \n\nmaximum. And then k_vals square bracket which.max of accuracy tells us the value of k \n\nthat corresponds to that.  \n\n \n\nSo in this case, we had k equal to 41 nearest neighbors gave us the best accuracy. But \n\nremember that this might change somewhat if we repeated this analysis with a different \n\nrandom seed that gave us a different validation set. \n\n \n\n \n\n \n\n \n\n \n\n", " \n\n \n\n \n\n \n\nNow that we know which model is the best, it's time to interpret it. K nearest neighbors \n\nis a modeling technique that's easy to understand. It simply looks at the K nearest \n\nneighbors of a point, and lets them vote on the predicted response value of that point. \n\nBut it doesn't produce a list of coefficients that we can use to interpret the effect of \n\neach of the predictor variables in the way that, say, linear regression does.  \n\n \n\nSo we need to be a little bit creative in order to understand how each of the predictor \n\nvariables contributes to the prediction. One strategy I really like for this is to make \n\npredictions for a set of example points. For example, here I'm creating vectors of values \n\nof the balance, income, and student, ranging from a nice round number close to the \n\nminimum up to a nice round number close to the maximum for each of balance and \n\nincome.  \n\n \n\nThen I'm using the function expand.grid to create a grid of all combinations of balance, \n\nincome, and student within these vectors. So here's what the first few rows of my \n\nexample data data frame looks like. We have all the possible combinations of student \n\nand balance being combined with the possible values of income.  \n\n \n\nOne caveat to this strategy is that not all of these combinations are necessarily realistic. \n\nIt may be that there simply aren't any students in the data set who have incomes equal \n\nto 73,500. And so we need to take our interpretations based on this with something of a \n\ngrain of salt.  \n\n \n\n \n\nNotes: \n\n \n\n\n", "balance_to_check = seq(0, 2600, by = 100) \n\nincome_to_check = seq(500, 73500, by = 1000) \n\nstudent_to_check = c(0, 1) \n\n \n\n   \n\nexample_data = expand.grid(student_to_check,  \n\n                           balance_to_check, \n\n                           income_to_check)  \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n", " \n\n \n\nNext, we need to make the predictions for our example data. So here I'm scaling the \n\nincome and balance of the example data using the mean and standard deviation from \n\nthe training set. So this is exactly the same thing as we did for the test set. And then I'm \n\nappending the column of the student values to make the data frame x_example.  \n\n \n\nSo we'll treat this like our test set to make the predictions. So here we're making the \n\npredictions using k equal to 41, our best model. So the only thing that has changed \n\ncompared to the analysis we did before is that now we have test equals x_example, our \n\nmade up data frame of example points.  \n\n \n\nNotes: \n\nexample_std = scale(example_data[ , 2:3], \n\ncenter = attr(quant_train_std, \"scaled:center\"), \n\nscale = attr(quant_train_std, \"scaled:scale\")) \n\n \n\n# Be sure the columns are in the same \n\n# order as the training data \n\nx_example = cbind(example_data[ ,1], \n\n         example_std) \n\n \n\nset.seed(123) \n\npredictions = knn(train = x_train, \n\n                  test  = x_example, \n\n                  cl = Default2[in_train, 1], \n\n                  k = 41) \n\n \n\n \n\n \n\n \n\n\n", " \n\n \n\nNext, we need to graph the predictions. So here I'm using the mutate function from the \n\ndplyr package to append the predictions from the k nearest neighbors model to the \n\nexample data data frame. And then I'm renaming the columns to make them a little bit \n\neasier to read in the graph.  \n\n \n\nHere I'm filtering to just the rows where student is equal to 0, so just the non-students. \n\nAnd using the gf_point function to create a scatter plot of balance versus income and \n\ncoloring the points based on the predictions. And remember that when you're using \n\nggformula and adding a color that varies based on the value of a variable, you use the \n\nequals squiggle.  \n\n \n\nNotes: \n\nexample_data <- example_data %>% \n\n  mutate(pred = predictions) %>% \n\n  rename(student = Var1, \n\n         balance = Var2, \n\n         income = Var3) \n\n \n\nexample_data %>% \n\n  filter(student == 0) %>% \n\n  gf_point(balance ~ income, color =~ pred) %>% \n\n  gf_labs(title = \"Non-students\") \n\n \n\n \n\n \n\n \n\n \n\n \n\n\n", " \n\n \n\nAnd here is what that graph looks like for non-students. And here's what it looks like \n\nwhen we filter to just the students. So right away looking at these graphs, we see that \n\nhigher up on the graph when balance is higher, we have more blue dots, indicating that \n\nthe prediction is, yes, the person will default. This makes sense because a higher \n\nbalance is harder to pay off. So this agrees with our intuitive understanding of the real \n\nworld context of the data.  \n\n \n\nWe also see that there's very little difference between the graph for students and the \n\ngraph for non-students. So it's looking like whether a person is a student or not is not a \n\nvery important variable in this model.  \n\n \n\n \n\n \n\n \n\n \n\n \n\n\n", " \n\n \n\nWe also see that income has a weak curved relationship with the balance required for a \n\nprediction of default. As income increases-- that is as we move from left to right on the \n\ngraph-- the balance that's required for a prediction of default first decreases, and then \n\nincreases. This might be somewhat surprising because you might expect that as people's \n\nincome increases, it should be easier to pay off a given amount of debt. So you might \n\nexpect that the relationship would just be increasing.  \n\n \n\nSo this raises additional questions for future models to be built or future data that we \n\ncould gather. Perhaps you want to gather information on people's social structures. \n\nPerhaps people with lower incomes are more likely to have family who can help them \n\npay off debt, or their reasons for incurring debt. Perhaps people with low incomes who \n\nincur high amounts of debt are only doing so under very specialized circumstances that \n\nmake them easier to pay off.  \n\n \n\n \n\n \n\n\n", " \n\n \n\n \n\n \n\nWhen you're using k nearest neighbors for classification with two categories, it's a good \n\nidea to choose k to be an odd number to avoid having tie votes. However, when you're \n\ndoing classification with more than two categories in your response variable, it can be \n\nhard to avoid having tie votes. In this case, the version of knn that's in the fnn package \n\nin R will choose a winner alphabetically.  \n\n \n\nFor example, if we're using k nearest neighbors with k equal to 5 in the example shown \n\nhere to classify the black circle, then we have a tie between blue triangles and yellow \n\nsquares. The knn function would choose blue triangles because that's the one that \n\ncomes first alphabetically.  \n\n \n\nYou can see the threshold that knn is using for its decision making by adding the \n\nargument prob=true. This will let you see that the probability attribute of the result was \n\n0.4, or two out of five of the nearest neighbors voted for blue.  \n\n \n\nTie votes are not a problem for knn for regression problems. That is predicting a \n\nquantitative response, because in that case, we're not having a tie between multiple \n\nquantitative values. Instead, all of the quantitative values from the nearest neighbors \n\nget averaged.  \n\n \n\nNotes: \n\ntrain.x = data.frame(x1 = c(1,3,1,3,2), x2 = c(1,1,5,5,2)) \n\ntrain.y = c(\"blue\", \"yellow\",\"blue\",\"yellow\",\"red\") \n\ntest.point = data.frame(x1 = c(2.1), x2 = c(2.1)) \n\n \n\n \n\n\n", "result = knn(train.x, test.point, \n\n    cl = train.y, k = 5, prob = TRUE) \n\nresult[1] \n\nattr(result, \"prob\") \n\n \n\n \n\n \n\n \n\n", " \n\n \n\nOne issue with using K nearest-neighbors for classification or regression is that it tends \n\nnot to perform very well when the number of predictor variables is large relative to the \n\nsize of your data set. This is common among nonparametric methods. Because they \n\ndon't assume a particular shape for your model, they tend to require more information \n\nin the form of more data points in order to make good predictions. \n\n \n\nFor K nearest-neighbors the issue is that the more variables there are, the fewer data \n\npoints there are that look similar to or nearby each point for which we want to make a \n\nprediction. If you think about each variable as being a dimension of the data set, then \n\nthree variables would correspond to three dimensional space, with each data point \n\nbeing a star. There can be a lot of empty space between two different stars. \n\n \n\n \n\n \n\n \n\n\n", " \n\n \n\nThere is no one answer to how many predictor variables is too many for K nearest-\n\nneighbors. However, if you are interested in performing K nearest-neighbors on a data \n\nset with lots of predictor variables, it might be a good idea to do an exploratory analysis \n\nor use cross-validation to help you choose a subset of the predictor variables, or \n\nconsider another method, such as logistic or linear regression. \n\n \n\nNotes: \n\nHere’s an example that uses linear regression as an exploratory analysis to choose \n\npredictor variables for KNN: \n\nhttps://www3.nd.edu/~steve/computing_with_data/17_Refining_kNN/refining_knn.ht\n\nml \n\n \n\n \n\n \n\n \n\n\n\n\n", " \n\n \n\n \n\n \n\n \n\n \n\n\n", "Answer to Question 1 \n\n \n\n \n\n \n\n \n\n\n"]], ["c:\\users\\mjols\\documents\\ds uwec courses\\740 data mining\\ds740_lesson1_KNN regression.pdf", ["DS 740\n\nData Mining\n\nK-Nearest Neighbors for Regression\n\nand Working With Categorical Predictors \n\n \n\n \n\nImportant note: Transcripts are not substitutes for textbook assignments. \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "Learning Objectives\n\nBy the end of this lesson, you will be able to:\n\n• Use K-nearest neighbors for regression in R.\n\n• Use one-hot encoding for categorical predictors.\n\n• Decide when to treat a discrete, quantitative predictor as \n\ncategorical.\n\n• Explain why we set the random seed in R.\n\n \n\n \n\n \n\n \n\n \n\n\n\n\n", "K-nearest Neighbors for Regression\n\n> install.packages(\"FNN\")\n\nTake average value of response variable from k nearest points\n\nk does not need to be odd\n\n \n\n \n\n \n\n \n\n \n\n\n\n", " \n\nNotes: \n\n \n\nDataset: “Ames, IA Real Estate Data,\" submitted by Dean De Cock, Truman State University. Dataset \n\nobtained from the Journal of Statistics Education (http://www.amstat.org/publications/jse). \n\nAccessed 26 May 2016. Used by permission of author. \n\n \n\n \n\n \n\n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "Dealing with categorical predictors\n\n• Lot shape is a categorical variable.\n\n• In the Default data set, we used 1 \n\nto represent students and 0 for \n\nnon-students.\n\n• Could use 0, 1, 2, 3 for IR1, IR2, IR3, \n\nReg…\n\n• But this implies an ordering\n\n \n\n \n\nIn the Ames Housing dataset, lot shape is a categorical variable. We need to converge it \n\ninto numbers in order to use K-nearest neighbors. Previously, in the default dataset, we \n\nused 1 to represent students and 0 to represent non-students. So we could do \n\nsomething similar here, using 0 to represent IR1, 1 to represent IR2, 2 to represent IR3, \n\nand 3 to represent Reg. But that implies an ordering that IR1 is closer to IR2 than it is to \n\nIR3 or to Reg. And we may not want to assume that ordering. \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n\n\n\n\n", "One-hot encoding\n\n• Each level of the categorical variable becomes an indicator variable\n\n• Good for KNN, because distance between two points with different \n\nlot shapes is the same\n\nOriginal \n\nLot Shape\n\nIR1\n\nIR2\n\nIR3\n\nReg\n\n1\n\n0\n\n0\n\n0\n\nis_IR1\n\nis_IR2\n\nis_IR3\n\nis_Reg\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n1\n\n \n\nAn alternative is one-hot encoding, in which each level of the categorical variable \n\nbecomes its own 0-1 indicator variable. In this case, we have four possible values of lot \n\nshape. So we would create four indicator variables, where a 1 for, say, is IR1 would \n\nindicate that the lot shape of that house is IR1 and a 0 would indicate that it's not. This \n\nis a good strategy for K-nearest neighbors because the distance between two points \n\nwith different lot shapes is the same no matter which two lot shapes we're talking \n\nabout. \n\n \n\n \n\n \n\n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "A variation of one-hot encoding\n\n• One variable can be perfectly predicted based on the others\n\n• Perfect multicollinearity\n\n• Bad for linear regression\n\n• Variation: n categories → n-1 indicator variables\n\n• First value in alphabetical order is the “default”\n\nOriginal \n\nLot Shape\n\nIR1\n\nIR2\n\nIR3\n\nReg\n\nLotShapeIR2 LotShapeIR3 LotShapeReg\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n1\n\n \n\n \n\nA potential disadvantage of one-hot encoding is that any one predictor variable that we \n\ncreate in this way can be perfectly predicted based on the others. If a particular house \n\nhas zeros for IR1, IR2, and IR3, then we know it must have a one for is Reg. This means \n\nthat the indicator variables have perfect multicollinearity. And this creates problems for \n\nmethods such as linear regression, which rely on matrices to compute the models.  \n\n \n\nSo if we're using one of those machine learning methods, we instead use a variation of \n\none-hot encoding in which a categorical predictor variable with n levels is turned into n-\n\n1 indicator variables. In this case, we have four possible values of lot shape. So we \n\nwould turn that into three indicator variables. The first value of the categorical variable \n\nin alphabetical order is the default value.  \n\n \n\nSo in this case, IR1 is represented by zeros in all three of the indicator variables, saying \n\nthat a house with a lot shape of IR1 is neither lot shape IR2, nor lot shape IR3, nor lot \n\nshape Reg.  \n\n \n\n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", " \n\n \n\nMany machine learning methods will do this variation of one-hot encoding for us \n\nautomatically. For example, if we do linear regression to predict a house's sale price \n\nbased on its year built and its lot shape, R will produce output that looks like this. This \n\ntells us that if we want to predict the price for a home on an IR1 lot, we would use the \n\nformula shown here, taking the y-intercept and the slope times the year it was built.  \n\n \n\nBut if we want to predict the price for a home on an IR2 lot, we would have an \n\nadditional term of the effect size of the IR2 lot shape. This has the effect of changing the \n\ny-intercept of the model for IR2 homes. \n\n \n\n \n\n \n\n \n\n\n", "One-hot encoding Lot Shape\n\n• For KNN, use 4 indicator variables for 4 categories\n\n• 3 indicator variables:\n\nPoint 1\n\nReg, 2000\n\nReg, 2000\n\nPoint 2\n\nIR1, 2000\n\nIR2, 2000\n\nDistance\n\n1\n\n2\n\n \n\n \n\nIn this case, because we're doing KNN, we want to use four indicator variables to \n\nrepresent the four categories of lot shape. If we only had three indicator variables, then \n\nwe would end up with different distances between pairs of points with different shapes \n\nsimply depending on whether those lot shapes included the default value of IR1 or not. \n\nSo to do our one-hot encoding, we can do this in a variety of ways.  \n\n \n\nI chose to use mutate from the dplyr package along with the ifelse function.  \n\n \n\nNotes: \n\names <- ames %>% \n\n  mutate(is_IR1 = ifelse(`Lot Shape` == \"IR1\", 1, 0), \n\n        is_IR2 = ifelse(`Lot Shape` == \"IR2\", 1, 0), \n\n        is_IR3 = ifelse(`Lot Shape` == \"IR3\", 1, 0), \n\n        is_Reg = ifelse(`Lot Shape` == \"Reg\", 1, 0)) \n\n \n\n \n\n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", " \n\n \n\nCategorical variables represented by numbers\n\nQuality\n\n1\n\n2\n\n3\n\nMeaning\n\nGood\n\nMedium\n\nBad\n\nZip code\n\n54701\n\n53715\n\n90210\n\nMeaning\n\nEau Claire, WI\n\nMadison, WI\n\nBeverly Hills, CA\n\n \n\n \n\nAnother thing to watch out for as you're preparing your dataset for analysis is \n\ncategorical variables represented by numbers. For example, maybe your dataset has a \n\nvariable called quality, which ranges from 1, representing good, to 3, representing bad. \n\nEven though this variable looks like a number, it's actually categorical because numerical \n\ncomparisons don't make sense here.  \n\n \n\nIt doesn't make sense to say that medium is twice as much as good or the distance \n\nbetween good and bad is twice as much as the distance between good and medium. \n\nThis is what's known as an ordinal categorical variable. So we do have an ordering to the \n\ncategories. In this case, you might want to leave this represented by numbers for the \n\npurposes of K-nearest neighbors. But if you wanted to remove that sense of ordering \n\nand simply treat the qualities as being three different possibilities, then you might want \n\nto one-hot encode this. You could try it both ways and see which way gave you the \n\nbetter predictions.  \n\n \n\nAnother example would be a data set which includes people's zip codes as one of the \n\nvariables. Again, even though zip code is something that looks numeric, it's actually \n\ncategorical because it doesn't make any sense to say that the distance between my zip \n\ncode and your zip code is 200. This is a nominal categorical variable, one where ordering \n\ndoesn't really make any sense. So we would definitely want to one-hot encode people's \n\nzip codes.  \n\n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", " \n\n \n\nShould Year be categorical?\n\n• Discrete, quantitative\n\nCan’t have a year \n\n2019.5317\n\nNumerical comparisons \n\nmake sense\n\n• Treating as quantitative will give better predictions if 2020 is more \n\nsimilar to 2015 than it is to 2000\n\n \n\n \n\nA common question that people have is, should year be treated as categorical? Year is a \n\ndiscrete, quantitative variable. It's quantitative because numerical comparisons make \n\nsense. It makes sense to say that the year 2020 is twice as far from the year 2000 as it \n\nwas from the year 2010. But it's discrete because it only comes in certain specified \n\nvalues-- in this case, whole numbers.  \n\n \n\nYou can't have a year 2019.5317. When you have a predictor variable that's discrete and \n\nquantitative, treating it as quantitative will tend to give you better predictions if the \n\nnumerical value of the variable is informative about the similarity of the response \n\nvariable-- for example, if 2020 is more similar to 2015 in terms of the sale prices of \n\nhomes than it is to the year 2000. So if you have some sort of gentle trend in the sale \n\nprices of homes as year progresses, then you want to treat year as quantitative.  \n\n \n\n \n\n\n\n\n\n\n", "Treating Year as categorical\n\n• Treating as categorical will give better predictions if the years are all \n\nequally dissimilar\n\n \n\n \n\nOn the other hand, treating year as categorical will tend to give better predictions if the \n\nyears are all equally dissimilar. In other words, if the fact that 2020 and 2019 are close \n\ntogether and 2000 is far different from both of them-- doesn't give us any information \n\nthat we can use to predict house prices. If we could just as easily replace the numbers of \n\nthe years by the shapes, year circle, year square, and year triangle, then treating year as \n\ncategorical makes sense.  \n\n \n\nIf you plan to do this, the number of distinct values of your predictor variable should be \n\nsmall compared to the number of data points. That's because with one-hot encoding \n\nyou'll be creating a new indicator variable for every level of that predictor variable. And \n\nyou don't want your total number of variables to get too large compared to the number \n\nof data points in your dataset. Another thing to keep in mind before you treat a discrete \n\nquantitative variable as categorical is that for some machine learning methods, it can be \n\nimpossible to make predictions for unseen values of a categorical variable.  \n\n \n\nThis isn't a problem with K-nearest neighbors. But it is a problem with linear regression. \n\nSo if you're doing linear regression to predict house prices and using year as categorical, \n\nif your training dataset only contains data from the year 2000 to 2020, then you're not \n\ngoing to be able to use it to predict the sale price of a house in 2021 or 2025 because \n\nyou won't have been able to use your training set to estimate the effect size of that new \n\nyear.  \n\n \n\n \n\n \n\n\n\n\n\n", " \n\nSelf-assessment Question 1 \n\n \n\n \n\nAnswer is at the end of the transcript. \n\n \n\n \n\n \n\n \n\n\n", " \n\n \n\nSetting the random seed\n\n• sample( ) generates pseudorandom numbers\n\n• Result is determined by current value of the random seed\n\n• By a complicated algorithm\n\n• Setting the seed makes your results reproducible\n\n \n\n \n\nNow that we've one-hot encoded our categorical variable, it's time to randomly divide \n\nthe dataset into a training set and validation set. But why do we use the function \n\nset.seed before we do this. We do this because the sample function isn't really dividing \n\nup our dataset randomly. It's not rolling a die or flipping a coin behind the scenes. \n\nInstead, the sample function generates pseudo-random numbers. These are numbers \n\nthat are determined by a complicated algorithm so they look random to us.  \n\n \n\nBut they're actually determined by the current value of the random seed. This is great \n\nbecause it means that we can make our results reproducible by setting the seed. For \n\nexample, if you set the seed equal to 200 and then do an analysis today, you can come \n\nback tomorrow and say, gee, I'd really like to take another look at that analysis with the \n\nsame training and validation sets and maybe make some new graphs. You can do that by \n\nsetting the seed equal to 200 and repeating the same code that you did before. You'll \n\nget the same set of random numbers in the same order.  \n\n \n\nNotes: \n\nset.seed(300) \n\ngroups = c(rep(1, 2000), rep(2, 930)) \n\n      # 1 represents the training set \n\nrandom_groups = sample(groups, 2930) \n\n \n\n \n\n \n\nset.seed(200) \n\nsample(1:10, 1) \n\n \n\n\n\n\n\n\n\n\n\n\n\n", "set.seed(200) \n\nsample(1:10, 1) \n\n \n\n \n\n \n\n \n\n", " \n\n \n\nRandom seeds and the autograder\n\n• Random seed changes internally each time you generate a random \n\nnumber\n\n \n\n \n\nIn addition to changing when you use the function set.seed, the random seed also \n\nchanges internally each time you generate a random number. This means that you can \n\nwrite code like this, where you set the seed and then call the sample function twice to \n\nget two different random numbers. In this course, we'll often tell you to set the random \n\nseed to a specific value so that you get the same answers on your homework as we got.  \n\n \n\nBut this feature of the random seed changing when you generate random numbers \n\nmeans that your answers might get off of what the autograder is expecting if you reset \n\nthe seed-- back to 200-- at a place when we're not expecting you to or if you generate \n\nextra random numbers, for example, by running a code cell multiple times in the \n\nprocess of debugging it. So for this class, you should set the seed where the homework \n\nor web work tells you to and no other places.  \n\n \n\nWhen you have your code working the way you want it, it's a good idea to clear your R \n\nenvironment and then run your code from top to bottom or knit the RMD file. This make \n\nsure that you didn't run any code sections twice to generate additional random \n\nnumbers and throw off your random seed.  \n\n \n\n \n\n\n\n\n\n", " \n\nScreencast \n\n \n\n \n\n \n\nLet's use k nearest neighbors to predict the price of homes in Ames, Iowa, using their lot \n\nshape and the year they were built. I've already loaded in my packages, the FNN \n\npackage, which contains the function knn.reg, as well as the readr package to read in \n\nthe data, dplyr for some data manipulation, and ggformula for some graphing for \n\ninterpretation later.  \n\n \n\nSo next I want to read in the data. I'll do that using the read_csv function in the readr \n\npackage. And my data set is already saved in the same folder where this RMD file is \n\nsaved. So I don't need to include the whole path to the file, just the name of the file. I'm \n\nalso including an additional argument, guess_max equals 1,064. I did this because when \n\nI tried using just read_csv CSV of AmesHousing, I got some warning messages saying \n\nthat r wasn't able to interpret several of the rows in the data set, starting with row \n\nnumber 1,064.  \n\n \n\nThis happened because it turned out that none of the first thousand rows of the data \n\nhad pools. So they all had NA for the quality of the pool variable. So that meant that \n\nread_csv was guessing the incorrect data type for that variable. So by using guess_max \n\nequals 1,064, I'm telling read_csv to wait until it reads that row of the data to guess the \n\ndata type for each of the variables.  \n\n \n\nAll right, we can check what's in the Lot Shape variable and verify that it has four \n\npossible values as a categorical variable. The IR2 and IR3 values are fairly infrequent. So \n\nwe expect to get better predictions for houses with Lot Shape equal to Reg or IR1.  \n\n \n\n \n\n \n\n\n", "So now we can One-hot encode the lot shape using four indicator variables. And then \n\ncreate the training and validation sets. Using the dim function, we see that this data set \n\ncontains 2,930 rows. And we typically want to have the training set be about 2/3 of the \n\ndata. So I'll create a training set with 2,000 rows and let the other 930 rows be in my \n\nvalidation set. So this code is exactly the same as what we did for the default data set, \n\njust with a different number of data points in the training and validation sets.  \n\n \n\nNext, we'll create the x_train and x_test data frames containing the predictor variables, \n\nthe x variables, for each of the training and validation sets. And I'm using the filter \n\nfunction to select the rows that are in the training set and exclamation point \n\nrepresenting not, so not in the training set for the test set.  \n\n \n\nNow I want to scale the Year Built variable. In this case, we only have one quantitative \n\nvariable. So I did consider simply not scaling it. But I did want a value of 1, difference in \n\nyear built, to be similar to the difference in 1 versus a 0 for each of the last shapes. So I \n\nam going to scale this so that a value of 1 in the year built represents 1 standard \n\ndeviation, rather than one year. And just like we did for the default data set, we're \n\nscaling the test set according to the mean and standard deviation of the training set \n\nfrom before we did the scaling.  \n\n \n\nNext, we'll build the model. So this looks very similar to what we did for knn for \n\nclassification. The differences are that we're using the function knn.reg instead of just \n\nknn. And the response values, instead of being cl for the classifications are called y. \n\nAnother small change is that the predictions output contains some additional \n\ninformation, not just the predictions. So to get the actual numerical predictions of the \n\nsale prices, we want to look at predictions, whatever variable we call that, $pred, so the \n\npred component of the object that we got out of knn.reg.  \n\n \n\nAll right, to look at the accuracy of our model, we can compute the mean squared error. \n\nSo the mean squared error is computed in exactly the way it sounds. We first compute \n\nthe error of each data point by taking the predictions and subtracting the observed \n\nvalues of the sale prices from the test set, so remember the !in_train. So that's our \n\nerror. Then we square it, and finally, we take the mean. One thing to be aware of is \n\nmake sure that you're doing the squaring inside of taking the mean.  \n\n \n\nSo here we get quite a large mean squared error. This isn't too surprising because sale \n\nprice has a large standard deviation. So it's not surprising that it's difficult to predict. \n\nAnd we're only using two predictor variables, the lot shape and the year it was built. So \n\nit's not surprising that it would be difficult to predict. However, the mean squared error \n\nis difficult to put into context because the units of this are dollars squared. So what \n\nreally counts as a large value of dollars squared isn't something that's really intuitive.  \n\n \n\nTwo things that we can do to make this easier to interpret. Our first, if we were testing \n\nout multiple models, for example, if we had a for loop to iterate over different values of \n\n \n\n \n\n", "k, then we would be interested in the model that had the lowest mean squared error. So \n\nthat could help put it into context. The other thing we could do is, instead of just \n\ncomputing the mean squared error, we can compute the mean absolute error or MAE. \n\nSo this is very similar to the mean squared error. We start out by computing the error, \n\nthe difference between the predictions, and the observed values of the response \n\nvariable. But then instead of squaring them, we take the absolute value, so simply \n\nmaking them all positive. And then we take the mean.  \n\n \n\nSo this result now has units of dollars, which is much easier to interpret. So we can see \n\nthat, on average, our predictions are off by plus or minus about $56,000. We can further \n\nput this into context by comparing our error to the values in the data. So here we had a \n\nmedian sale price of $160,000 and a maximum sale price of $755,000. So we can \n\ncompare the MAE to the median or the maximum and see that our error is about 35% of \n\nthe median home price or about 7% of the maximum home price. So we can see that \n\nthis is still a fairly large error. That we could probably do better by testing out different \n\nmodels, either by including more of the predictor variables or by changing our value of \n\nK.  \n\n \n\n \n\n \n\n", " \n\n \n\nInterpreting the model\n\n• Make predictions for a grid of \n\nexample points\n\n• 1 quantitative + 1 categorical \n\npredictor → use a line graph \n\nwith colors\n\n• Newer homes cost more, but \n\nonly since about 1960\n\n• Interestingly, irregular lots \n\ncost more\n\n \n\n \n\nTo interpret our model, we can make predictions for a grid of example points like we did \n\nfor the default data set. In this case, we only have one quantitative and one categorical \n\npredictor. So we can use a line graph to plot the predicted house prices as a function of \n\nthe year built with different colors to represent the lot shape. In this case, the line graph \n\nturned out to be pretty noisy. So I'm using a loess smoothing spline instead.  \n\n \n\nFrom this graph, we can see that newer homes do tend to cost more, which agrees with \n\nmy intuitive expectation. But this is only true since about 1960. Before that, it seems like \n\nthe house prices were relatively flat, perhaps even with a peak right around World War \n\nII. So this could mean that once we get past 1960, people simply think of the house as \n\nold and don't put any more of a discount on how much it should cost.  \n\n \n\nInterestingly, we see that irregular shaped lots of lot shape IR1 tend to cost more than \n\nthe regular shaped lots. This is worth investigating. Perhaps the irregular shaped lots \n\n \n\ntend to be larger so they end up being more desirable.  \n\n \n\n\n\n\n\n", "Summary\n\n• KNN for regression problems: knn.reg in FNN library in R\n\n• Standardize predictor variables, same as for classification problems.\n\n• Use MSE or MAE to measure error, unlike for classification problems.\n\n• Treating a discrete, quantitative predictor (like Year) as categorical \n\ncan improve predictions if different values of the predictor are all \n\nequally dissimilar.\n\n• For some ML methods, can be impossible to make predictions for \n\nunseen values.\n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n\n\n\n", "Answer to Self-Assessment 1 \n\n \n\n \n\n \n\n \n\n\n"]], ["c:\\users\\mjols\\documents\\ds uwec courses\\740 data mining\\ds740_lesson1_preview+tradeoffs.pdf", [" \n\n \n\n \n\n \n\n \n\nImportant note: Transcripts are not substitutes for textbook assignments. \n\n \n\n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", " \n\n \n\nLearning Objectives\n\nBy the end of this lesson, you will be able to:\n\n• Explain the 2 main types of data mining approaches.\n\n• Explain the 2 types of supervised learning.\n\n• Explain 2 reasons why the method that best fits the data is not \n\nalways the best method to use.\n\n• Estimate the error on new data points for regression and \n\nclassification problems.\n\n• Explain why interpreting a model is important, and some key \n\nquestions to start the interpretation.\n\n \n\n \n\n\n\n\n", " \n\n \n\n \n\n \n\nMethods of data mining fall into two main categories. The first is called supervised \n\nlearning. This is where you have a training dataset with a known response variable, or y \n\nvariable. And you want to estimate, or predict, the values of that response variable in \n\nanother dataset called the test set, or validation set. Supervised learning can also be \n\nused to do inference, if you want to understand the model that describes the \n\nrelationship between the predictor variables, x variables, and the response variable.  \n\n \n\nSome examples of supervised learning approaches include linear regression, logistic \n\nregression, linear discriminant analysis, and decision trees.  \n\n \n\n \n\n\n", " \n\n \n\n \n\n \n\nThe other main category is unsupervised learning. In unsupervised learning, you have no \n\nresponse variable. Instead, your main goal is to understand any relationships between \n\nthe variables or between the observations. For example, suppose we have two variables \n\nrepresented on the x and y axes here. We're not thinking about the y variable as being a \n\nresponse, but we want to understand its relationship with the x variable. And in this \n\ncase, we could use cluster analysis to identify three main clusters in the data. \n\n \n\nSome other examples of unsupervised learning and data mining are principal \n\ncomponents analysis and association rules.  \n\n \n\n\n", " \n\n \n\n \n\nSupervised learning can be further broken down into regression methods and \n\nclassification methods. Regression methods are used when the response variable is \n\nquantitative. So linear regression is an example of a regression method. Classification \n\nmethods are used when the response variable is categorical, or qualitative. So we want \n\nto classify or categorize each data point. Despite its name, logistic regression is an \n\nexample of a classification method.  \n\n \n\n \n\n\n", " \n\n \n\n \n\n \n\nHowever, the boundary between these two approaches can be fuzzy. Many \n\nclassifications methods-- including logistic regression-- can be used to estimate a \n\nprobability that a particular data point falls into a particular category. Those \n\nprobabilities can be thought of as a quantitative response variable. Or you might have a \n\nquantitative response variable, for example, sales volume. But maybe you're mainly \n\ninterested in whether it's high or low. In that case, you might turn your regression \n\nproblem into a classification problem. \n\n \n\nAnd finally, many methods of supervised learning-- including k nearest neighbors-- apply \n\nto both regression and classification problems.  \n\n \n\n\n", " \n\n \n\n \n\n \n\nAfter you know which general category of method you want to use, you'll want to \n\nconsider the trade off between flexibility and interpretability. More flexible methods \n\nhave a bigger variety of model shapes that they can produce, which means they can fit \n\nthe data better. For example, here we have a single data being fit by two different \n\nmethods. On the left, it's being fit by linear regression, which is only fitting a model \n\nshape that looks like a line. On the right, the same dataset is being fit by a lowest \n\nsmoothing spline, which can fit a variety of curved shapes to the data. That means it can \n\ncurve around and fit the data better. \n\n \n\n \n\nNOTES: \n\n \n\nR Code: \n\nfit = loess(y ~ x) \n\nxvals = seq(1, 20, .05) \n\npredvals = predict(fit, xvals) \n\nlines(xvals, predvals, lwd=2, col=\"red\") \n\n \n\n \n\n \n\n\n\n\n\n\n", " \n\n \n\n \n\nFitting the data well is good. However, less flexible methods are often easier to \n\ninterpret. This can be important for communicating your model to others, as well as for \n\nunderstanding the process that underlies your data. For example, suppose that in this \n\ndataset, the y-axis represents profit and the x-axis represents the amount of time that a \n\nparticular store has been open. It might be more useful to know the general trend that \n\nas the store has been open longer, the profit tends to increase, as shown by the linear \n\nregression. Compared to the picture on the right, knowing that the profit started out \n\ndecreasing but then increased and then squiggled around some.  \n\n \n\n \n\n\n", " \n\n \n\n \n\n \n\nEven if your highest priority is the accuracy of the model's predictions, rather than \n\ninterpreting the model, you might not want the most flexible method. That's because of \n\nthe trade off between bias and variance. I like to think of this as the balance between \n\nfitting the data versus overfitting the data.  \n\n \n\nThe idea is that if you have a very flexible model, like the interpolating spline shown \n\nhere, it will fit the data very well, which gives you low bias. However, it's also very \n\nsensitive to the specific values in this data set. That means it has high variance, and it \n\nmeans it's likely to be bad for estimating the value of a new data point. \n\n \n\nNOTES: \n\nR Code: \n\nlibrary(splines) \n\nfit = interpSpline(y~x) \n\nplot(x, y, pch=19, yaxt=\"n\", xaxt=\"n\", xlim=c(1,20), \n\nylim=c(49,52.5)) \n\npar(new = T) \n\nplot(fit, xlim=c(1,20), ylim=c(49,52.5)) \n\n \n\n \n\n \n\n \n\n\n\n\n\n\n\n\n", " \n\nQuestion 1 \n\n \n\n \n\nAnswer is at the end of this transcript \n\n \n\n \n\n \n\n \n\n\n", " \n\nQuestion 2 \n\n \n\n \n\n \n\nAnswer is at the end of this transcript \n\n \n\n \n\n \n\n\n", " \n\nComputing Error\n\nMean Squared Error: for regression problems\n\n!\"# =  \n\n!! −   ! !!\n\n!\n\n\t\n\nPredicted y-value\n\nfor the ith point\n\nError rate for classification problems\n\n!  !!   ≠   !!\n\n\t\n\nEquals 1 if the ith\n\npoint is misclassified\n\nEquals 0 if classified correctly\n\n1\n\n!\n\n1\n\n!\n\n!!!\n\n!\n\n!\n\n!!!\n\n \n\n \n\nTo compute the error for regression-type problems, where the response variable is \n\nquantitative, we use the mean squared error. This should look familiar from linear \n\nregression. Here, f hat could also be called y sub i hat. It's the predicted y value for the i-\n\nth data point. \n\n \n\nTo compute the error rate for classification problems where the response variable is \n\ncategorical, we use this formula. Here, i is an indicator variable that equals 1 if y sub i \n\ndoes not equal y sub i hat. That is, if the i-th point is misclassified, and equals 0 if the i-th \n\npoint is classified correctly.  \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", " \n\n \n\n \n\n \n\n \n\nWhen assessing a model, we want to estimate its error on new data points that weren't \n\nused to build the model. One way to do this is with the validation set approach. We \n\ndivide the data into two parts, a training set, which we use to build the model, and a \n\nvalidation set, which we use to estimate its error. It's important that the training set and \n\nvalidation set be similar in terms of the distribution of the values of the predictor and \n\nresponse variables. \n\n \n\nFor example, if we're modeling the profits of an ice cream store, we wouldn't want the \n\ntraining set to contain all of the hot days, and the validation set to contain all of the cold \n\ndays. So we frequently choose a random subset of the data to be in the training set, and \n\nthe rest of the data to be in the validation set. \n\n \n\n \n\nNOTES: \n\nSee section 5.1.1 in Introduction to Statistical Learning. \n\n \n\n\n", " \n\n \n\n \n\n \n\nThe validation set approach has two major drawbacks. The first is that the error can vary \n\nbased on which points are in the training set versus the validation set. The second is that \n\nyou're not using all of the data to train the model. Perhaps only 1/2 or 2/3 of your data \n\nare in the training set. This means you're not using all of the information available, so \n\nyou can overestimate the error in your model. We'll see how to mitigate both of these \n\ndrawbacks later on when we discuss cross validation. \n\n \n\n \n\n \n\n \n\n\n", " \n\nInterpreting the model\n\n• Black box model:  Cannot be understood by humans\n\n• Too complex\n\n• Proprietary\n\n• Creates problems with\n\n• Knowing when to trust results\n\n \n\n \n\nAfter you found a model with a low error rate, it's important to interpret the model. You \n\nmight be familiar with the term black box, which refers to a model that can't be \n\nunderstood by humans. That might be because it's too complex or simply because it's \n\nproprietary, but black box models create problems with knowing when to trust the \n\nresults.  \n\n \n\nFor example, maybe a model predicts that a particular patient has a high probability of \n\nsurviving surgery. But did it actually account for this patient's rare blood disorder? You \n\ndon't know because you don't know what variables the model used or how it used \n\nthem.  \n\n \n\nNOTES: \n\n \n\nExample of bias in a mortgage lending model: \n\nhttps://www.cnbc.com/2018/11/27/online-lenders-are-charging-minority-borrowers-\n\nmore-study-concludes.html  \n\n  \n\nArticles about problems with black box models: \n\nhttps://arxiv.org/pdf/1811.10154.pdf  \n\nhttps://hdsr.mitpress.mit.edu/pub/f9kuryi8/release/6  \n\n \n\n\n\n\n\n\n\n\n", " \n\nInterpreting the model\n\n• Black box model:  Cannot be understood by humans\n\n• Too complex\n\n• Proprietary\n\n• Creates problems with\n\n• Knowing when to trust results\n\n• Risk of bias\n\n• Troubleshooting\n\n• Black box models aren’t necessarily more accurate!\n\n• Look for a good interpretable model first, and interpret it.\n\n \n\n \n\nAfter you found a model with a low error rate, it's important to interpret the model. You \n\nmight be familiar with the term black box, which refers to a model that can't be \n\nunderstood by humans. That might be because it's too complex or simply because it's \n\nproprietary, but black box models create problems with knowing when to trust the \n\nresults.  \n\n \n\nFor example, maybe a model predicts that a particular patient has a high probability of \n\nsurviving surgery. But did it actually account for this patient's rare blood disorder? You \n\ndon't know because you don't know what variables the model used or how it used \n\nthem.  \n\n \n\nNOTES: \n\n \n\nExample of bias in a mortgage lending model: \n\nhttps://www.cnbc.com/2018/11/27/online-lenders-are-charging-minority-borrowers-\n\nmore-study-concludes.html  \n\n  \n\nArticles about problems with black box models: \n\nhttps://arxiv.org/pdf/1811.10154.pdf  \n\nhttps://hdsr.mitpress.mit.edu/pub/f9kuryi8/release/6  \n\n \n\n\n\n\n\n\n\n\n", " \n\nGood interpretations provide insight\n\n• Reveal information about variables/underlying process\n\n• Non-data scientists can understand and provide suggestions\n\n• Suggest strategies for influencing the response variable\n\n• Suggest hypotheses for future research\n\n• Interpretation may be useful even if predictions are insufficiently \n\naccurate\n\n \n\n \n\nGood interpretations can reveal information about the variables and the underlying \n\nprocess that may have generated the data. They allow non data scientists to understand \n\nyour model so they can provide suggestions to make it better, like, hey, did you think \n\nabout including a variable that tells whether the patient has a rare blood clotting \n\ndisorder?  \n\n \n\nGood interpretations also can suggest strategies for influencing the response variable. \n\nFor example, maybe we don't just want to predict the profit of a store, but we want to \n\nimprove the profit. How might we do that?  \n\n \n\nAnd good interpretations can suggest hypotheses for future research or data collection. \n\nThis means that a good interpretation can be useful even if the predictions are \n\ninsufficiently accurate for predicting individual observations. They can still tell us \n\nsomething useful about the overall process.  \n\n \n\n \n\n \n\n\n\n\n", " \n\nHow to write a good interpretation\n\n• What predictor variables are \n\nmost important?\n\n• Do they have a positive, \n\nnegative, or curved \n\nrelationship with the \n\nresponse variable?\n\n• Does the direction of the \n\nrelationship depend on the \n\nvalue of another variable?\n\n \n\n \n\nEach predictor variable-- to write a good interpretation, start by looking at which \n\npredictor variables are most important in the model and what kind of relationship do \n\nthey have with the response variable. Is it positive, negative, curved? Or does the \n\ndirection of the relationship between predictor variable A and the response variable \n\ndepend on the value of predictor variable— \n\n \n\nNOTES: \n\nset.seed(101) \n\nx = runif(100) \n\ny = 3*x + rnorm(100) \n\ngf_point(y ~ x) %>% \n\ngf_smooth(y ~ x) \n\n \n\n   \n\nset.seed(102) \n\nx = runif(100) \n\ny = -7*x + rnorm(100) \n\ngf_point(y ~ x) %>% \n\ngf_smooth(y ~ x) \n\n \n\n   \n\nset.seed(103) \n\nx = runif(100, -1, 1) \n\ny = x^2 + rnorm(100, 0, .5) \n\ngf_point(y ~ x) %>% \n\ngf_smooth(y ~ x) \n\n \n\n \n\nlibrary(ggformula) \n\nmydf = data.frame(A=factor(c(0,0,1,1)), B=c(1,2,1,2), response=c(3,4,4,3)) \n\n\n\n\n\n\n\n\n", " \n\ngf_point(response ~ B, col =~ A, data = mydf) %>% \n\ngf_smooth(response ~ B, col =~ A, method = \"lm\", data = mydf) \n\n \n\n \n\n", " \n\nQuestions about the relationships\n\n• Do the relationships make sense based on the context of the \n\ndata?\n\n• Do they agree with previous studies?\n\n• What do the relationships suggest about how to improve the \n\nresponse variable?\n\n \n\n \n\nThen to take the interpretation deeper you can ask, do the relationships make sense \n\nbased on the context of the data? Do the directions of the relationships agree with \n\nprevious studies about similar data sets? And what do the relationships suggest about \n\nhow to improve the response variable?  \n\n \n\nYou have to take these suggestions with a grain of salt because you don't know if any \n\nassociations in your data actually represent causal relationships or not, but these \n\nsuggestions can still provide good hypotheses that you can then use for future \n\ninvestigations.  \n\n \n\n \n\n \n\n\n\n\n", " \n\nGood examples\n\n“Higher quality wines tend to have \n\nhigher alcohol percentages than \n\naverage quality wines. Additionally, the \n\nlevels of volatile acidity are 20% higher \n\nin average quality wines compared to \n\nhigh quality wines. This would make \n\nsense given higher volatile acidity \n\nlevels can contribute to an unpleasant, \n\nvinegar taste in the wine.”\n\n \n\nHere's an example of an interpretation that does a good job of putting its results into a \n\nreal world context explaining why this makes sense.  \n\n \n\n \n\n \n\n \n\n\n\n\n\n\n", " \n\nGood examples\n\n“The employees most likely to leave are newer \n\nemployees who are likely not yet very \n\nestablished within the company. Second, stock \n\noptions could possibly be explored as a \n\nretention tool, as it appears that those without \n\nstock options are more likely to leave. Third, \n\nthere is evidence that employees who work \n\novertime are more likely to leave the \n\ncompany.”\n\n \n\nHere's an interpretation that does a good job of explaining how these results could be \n\nused to affect a change in the response variable, in this case, improving employee \n\n \n\nretention.  \n\n \n\n\n\n\n\n\n", " \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n\n", " \n\nSummary Two\n\nThe error of a method can be computed using: \n\n• the MSE: for regression problems.\n\nor\n\n• the proportion of misclassified points: for classification problems.\n\nThe validation set approach is one way to estimate a method’s error \n\non new data points.\n\nTo interpret a model, describe the relationship between the response \n\nand the most important predictors, and relate it to the context of the \n\ndata.\n\n \n\n \n\n \n\n \n\n\n\n\n", " \n\nQuestion 1 Answer \n\n \n\n \n\n \n\nQuestion 2 Answer \n\n \n\n \n\n \n\n \n\n\n\n"]], ["c:\\users\\mjols\\documents\\ds uwec courses\\740 data mining\\Understanding standardization.pdf", ["Why do we standardize the validation set according to the \n\ntraining set’s mean and standard deviation? \n\nAbra Brisbin \n\n1/29/2021 \n\nlibrary(dplyr) \n\nlibrary(ggformula) \n\nWhy do we standardize the validation (test) set according to the \n\ntraining set’s mean and standard deviation (sd)? \n\nWhen we make the training/validation set split randomly (though there are reasons why \n\nwe don’t always do this), then typically the mean and sd will be similar in the two groups. \n\nWhen this happens, you’ll get similar results regardless of whether you scale the validation \n\nset according to the original mean and sd of the training set (correctly) or the validation set \n\n(incorrectly). \n\nThe reason why it’s important to scale the validation set based on the training set mean \n\nand sd is most apparent when the validation set happens to have a different mean than the \n\ntraining set. So, let’s simulate a test and training set with different means. Then we can see \n\nwhat goes wrong when you scale the validation set according to its own mean and sd. \n\nSimulating the raw (unscaled) data \n\nThis data is simulated to have the same standard deviation ((cid:1) = 2) for each of the predictor \n\nvariables (cid:4)1 and (cid:4)2, because this is the case where standardization isn’t really necessary–\n\nboth variables are already weighted equally, based on their standard deviations, so we \n\nshould get the same predicted classifications regardless of whether we scale the data or \n\nnot. \n\nset.seed(111) \n\ntrain = cbind(rnorm(100, 2, 2), rnorm(100, 3, 2)) \n\ntest = cbind(rnorm(15, 3, 2), rnorm(15, 5, 2)) \n\ncombo = data.frame(rbind(train, test)) \n\nnames(combo) = c(\"x1\", \"x2\") \n\ncombo <- combo %>% \n\n  mutate(group = c(rep(\"train\",100), rep(\"validation\",15))) \n\nAdd a column of random numbers to facilitate creating random classifications. \n\nset.seed(111) \n\ncombo <- combo %>% \n\n  mutate(random = runif(115)) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "combo <- combo %>% \n\n  mutate(classification = case_when(group == \"validation\" ~ \"unknown\", \n\n                                    x1 + x2 > 5 & random < .9 ~ \"A\", \n\n                                    x1 + x2 <= 5 & random < .1 ~ \"A\", \n\n                                    TRUE ~ \"B\")) \n\nGraph the raw data \n\nNotice that most of the data points in the validation set are predicted to belong to class A. \n\ncombo %>% \n\n  gf_point(x2 ~ x1, shape =~ group, col =~ classification) %>% \n\n  gf_refine(scale_shape_manual(values = c(19, 6))) %>% \n\n  gf_labs(title = \"Raw data\", \n\n          subtitle = \"Most validation set points are predicted to be class \n\nA\") \n\nCorrectly scaling based on mean and sd of training set \n\nLet’s scale the data correctly: both the training and validation sets will be scaled based on \n\nthe original mean and sd of the training set. \n\n \n\ntrain_stats <- combo %>% \n\n  filter(group == \"train\") %>% \n\n  summarise(train_mean_x1 = mean(x1), \n\n            train_mean_x2 = mean(x2), \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "            train_sd_x1   = sd(x1), \n\n            train_sd_x2   = sd(x2)) \n\nTo make it more clear what’s happening, I’m doing the scaling by explicitly subtracting the \n\nmean and dividing by the standard deviation. (This is what the scale function does behind \n\nthe scenes.) \n\ncombo <- combo %>% \n\n  mutate(x1_std = (x1-train_stats$train_mean_x1)/train_stats$train_sd_x1, \n\n         x2_std = (x2-train_stats$train_mean_x2)/train_stats$train_sd_x2) \n\nHere’s what it looks like after we scale all the data points (training and validation) by the \n\ntraining set mean and sd. The whole data set is shifted to be centered at (0, 0), and the \n\nvertical and horizontal spread have changed, because we rescaled to have a standard \n\ndeviation of 1. But the validation set points are still in the same positions relative to the \n\ntraining set. This version still predicts that most of the validation set belongs to class A. \n\ncombo %>% \n\n  gf_point(x2_std ~ x1_std, shape =~ group, col =~ classification) %>% \n\n  gf_refine(scale_shape_manual(values = c(19, 6))) %>% \n\n  gf_labs(title = \"Scaled based on training set\", \n\n          subtitle = \"Most validation set points are predicted to be class \n\nA\") \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "Incorrectly scaling the test set based on the test set’s mean and sd \n\nNow let’s scale the data incorrectly: the training set will be scaled based on its mean and sd, \n\nand the validation set will be based on the validation set’s mean and sd. \n\ntest_stats <- combo %>% \n\n  filter(group == \"validation\") %>% \n\n  summarise(test_mean_x1 = mean(x1), \n\n            test_mean_x2 = mean(x2), \n\n            test_sd_x1   = sd(x1), \n\n            test_sd_x2   = sd(x2)) \n\ncombo <- combo %>% \n\n  mutate(x1_std_bad = case_when(group == \"train\" ~ x1_std, \n\n                                TRUE ~ (x1-\n\ntest_stats$test_mean_x1)/test_stats$test_sd_x1), \n\n         x2_std_bad = case_when(group == \"train\" ~ x2_std, \n\n                                TRUE ~ (x2-\n\ntest_stats$test_mean_x2)/test_stats$test_sd_x2)) \n\nHere’s what it looks like if we incorrectly scale the validation set according to the validation \n\nset’s mean and sd. The training and validation sets have been shifted separately, so they are \n\neach centered at (0, 0). This changes the position of the validation set relative to the \n\ntraining set. As a result, about half the validation set is now incorrectly predicted to belong \n\nto class B. \n\ncombo %>% \n\n  gf_point(x2_std_bad ~ x1_std_bad, shape =~ group, col =~ classification) \n\n%>% \n\n  gf_refine(scale_shape_manual(values = c(19, 6))) %>% \n\n  gf_labs(title = \"Training and validation sets scaled based on their own \n\nmean and sd\", \n\n          subtitle = \"Shifts validation set, predicts half the points to be \n\nclass B\") \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", " \n\n\n"]], ["c:\\users\\mjols\\documents\\ds uwec courses\\740 data mining\\lesson2\\ds740_lesson2_presentation1.pdf", [" \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nImportant note: Transcripts are not substitutes for textbook assignments. \n\n \n\n1 \n\n\n\n", " \n\n \n\n \n\n \n\n \n\n2 \n\n\n", " \n\nWe assess a model by predicting data and summarizing the error in prediction. \n\nEstimating the error of a model on the same data that were used to build or train the \n\nmodel can give an underestimate of the error. We are not able to get an honest picture \n\nof how well the model can actually predict new data when we reuse the data used to fit \n\nthe model. \n\n \n\n \n\n \n\n \n\n3 \n\n\n", " \n\nLet's take a look at an example. Consider a simple linear regression model in which we \n\nare estimating the intercept and slope of the best fitting line to xy data. Our purpose is \n\nto produce a line which can be used to predict the response based on the observed \n\npredictor, x. Thus, we choose the line to minimize the distance that the points fall from \n\nthe line, or more specifically, a summary of the distance that the points fall from the \n\nline. We work to minimize the SSE, which stands for Sum of Squared Errors, or \n\nequivalently, the Mean Square Error, MSE. \n\n \n\n \n\n \n\n \n\n4 \n\n\n", " \n\nYou've seen that the validation set approach lets us estimate what the error of a model \n\nwill be on new data points in a test or validation set. However, the error varies based on \n\nwhich points get included in that validation set, and this can be an overestimate of the \n\nerror because the model is being trained on a smaller data set. That is, we are producing \n\na worse model fitted to the data because it's based on fewer data. In this lesson, we'll \n\nlearn how to avoid those problems. \n\n \n\n \n\n \n\n \n\n5 \n\n\n", " \n\n \n\nWe illustrate this point by applying a validation set approach on the previous data. We \n\nsplit the data in half with n equals 6, training observations, and 6, testing observations. \n\nIn this particular example, we are able to fit a line very closely to the training set. \n\nHowever, the prediction of the test set is much worse, and we can see this from the \n\nmuch higher MSE computed on the testing set.  \n\n \n\nWhy is this? Occurs because the test set data were not used to fit the model. And this is \n\nthe real situation that we wish to mimic when we assess how well a model can do future \n\nprediction of new data. That is, how well can it predict data that were not used to fit the \n\nmodel. \n\n \n\n \n\n \n\n6 \n\n\n", " \n\n \n\nThe example in the previous slide was rather extreme, but it suggests some basic \n\nprinciples about model fitting and model assessment that we wish to highlight. One way \n\nto illustrate this is via simulation. We consider repeating the process on the previous \n\nslides many, many times.  \n\n \n\nSo each time, we sample a total of n equals 12 data points, we fit a simple linear \n\nregression model to the full 12 data points, and we then split the data into a training set \n\nof size 6, and a testing set of size 6 on which we fit a simple linear regression model to \n\nthe 6 training observations and predict the 6 testing observations. We repeat this whole \n\nprocess many, many times, say, 100,000, and in doing so, we get 100,000 simple linear \n\nregression models fit to n equals 12 observations with 100,000 slopes, that is, a \n\ndifferent slope for each of the different sample fits. This is the histogram displayed in a \n\nlighter orangish color with a taller peak. We also get 100,000 fitted models to the \n\nsmaller training set, which again, produces 100,000 slopes, one for each of the fits to \n\neach training set. And this is visualized in the reddish wider histogram displaying those \n\n100,000 estimated slopes.  \n\n \n\nThe purpose of this illustration is to show that the smaller sized training data set tends \n\nto be less accurate for estimating the slope than is the full data set with n equals 12. \n\nWhile this is intuitively apparent, this visually affirms that intuition. That is, more data \n\nproduces estimates that are closer, that is, less spread out, to the true value of the \n\nslope. Hence, we would like to fit the model on as much data as possible to get our best \n\nestimates. \n\n \n\n \n\n \n\n7 \n\n\n", " \n\n \n\nWe revisit the simulations that were described in the previous slide, looking at a more \n\nrelevant summary to the topic of discussion, and that is the mean square error. We first \n\ntake a look at the mean square error values where each is computed on a sample of size \n\nn equals 12 by repredicting that full data set using the model fit to the same data. These \n\nmean square error values over 100,000 such samples are shown in the yellow lighter \n\nhistogram, which has a center around 0.7.  \n\n \n\nWe also make a histogram of mean square values computed on the test sets where the \n\nmodel was fit to a training set of 6 values. This is shown in the histogram in the bluish \n\ncolor, which is centered around 1.5. The purpose of comparing these histograms is to \n\nshow that reusing data, that is, repredicting the same data used to fit the model, will \n\nunderestimate error, as shown in the lower center for the yellow histogram.  \n\n \n\nThe honest prediction is the one we want, and that is illustrated as being centered right \n\naround 1.5. And this shows a more reliable reflection of the true error and estimation. \n\nHence, we would like to assess the model with truly new data for accuracy. However, \n\nwe also note that the mean square error values from the testing sets tend to be more \n\nvariable than if we could use more data. So is there a way to obtain a more precise, but \n\nstill accurate estimate? \n\n \n\n \n\n \n\n8 \n\n\n", " \n\n \n\nSuppose we were to use all the data to fit the model. We now are left with no data to \n\nassess unless we are clever about reusing the data. If we are willing to do some \n\nadditional model fitting on subsets, we can use these data for assessment as well. Such \n\nan assessment process could be accomplished as follows.  \n\n \n\nSplit the data sets into approximately equally sized subsets. In turn, hold each subset \n\nout and fit the model on the remaining subsets. Predict the observations in the holdout \n\nset.  \n\n \n\nWe are then using a model fit on training set data to predict testing set data. That is, we \n\npredict data not used to fit the model. Of course, this does not come for free. We must \n\nfit the model additional times, the same number of times as we split the data. \n\n \n\n \n\n \n\n9 \n\n\n", " \n\n \n\nWe now consider Leave One Out Cross-Validation, or LOOCV. This is the simplest \n\napplication of multiple splits of the data, as we use n to denote number of data points. \n\nWhen we split the data into n sets or folds, we are making splits into subsets of size 1. A \n\nvery simple example with size n equals 4 illustrates the concept.  \n\n \n\nWe could fit the model on data points 2, 3, and 4. We then use the model to predict \n\ndata point 1 to get a prediction from a model fit without data point 1, giving us a y hat \n\nvalue. The subscript of 1 in parentheses on the estimated function denotes the \n\nprediction from a model fit without data point one. Subsequently leaving out each of \n\nthe remaining data points, we obtain predictions of each of the data points in an honest \n\nway, that is, based on a model fit without that data. Further comparison of these \n\npredictions-- y hats-- to the actual observed responses-- y's-- will allow us to assess how \n\nwell the model can predicts new observations. \n\n \n\n \n\n \n\n10 \n\n\n", " \n\n \n\nWhen we have a numeric response, assessment for leave one out cross-validation uses a \n\nvery similar measure to one which we have already examined. The only distinction is \n\nthat we denote each predicted data point as having been made from a model fit without \n\nthe use of that data point by using a subscript i in parentheses to identify the \n\nobservation that is left out. The formula is as displayed.  \n\n \n\nThe leave one out cross-validation measure for assessment based on these n splits \n\naverages out these squared errors over the n splits. This computation is denoted as CV \n\nsubscript n in parentheses. Note that this formula looks extraordinarily similar to the \n\nbasic MSE formula with the key difference that the responses are predicted using \n\nmodels fit without that data point.  \n\n \n\nThe cross-validation assessment measures for classification problems are very similar to \n\nthose using numeric responses, with the only adjustment being that the indicator for \n\nmisclassification of observations is used in place of the squared error computation. Note \n\nthat the key difference, again, is that the predictions are performed based on a fitted \n\nmodel using data except for the observation that we are eventually predicting. That is, \n\nwe have an honest predicted categorization for our new data point. \n\n \n\n \n\n \n\n11 \n\n\n", " \n\nCorrect answer is on the last page. \n\n \n\n \n\n \n\n \n\n12 \n\n\n", " \n\n \n\nWith k fold cross-validation, or k CV, we use k to denote the number of splits. When we \n\nsplit the data into k sets or folds, we are making splits into subsets of approximately the \n\nsame size, each of which will be n over k. A very simple example of size n equals 8 \n\nillustrates the concept. With 4 folds, this means all subsets are of size 2.  \n\n \n\nWe begin by fitting the model on all data points except 1 and 7. We then use the model \n\nto predict data points 1 and 7 to get a valid or realistic prediction from a model fit \n\nwithout these data. Subsequently leaving out each of the remaining folds of data, we \n\nobtain predictions for each of the data points in an honest way. The subscript in \n\nparentheses on the estimated functions still denotes the prediction from a model fit \n\nwithout the data point. \n\n \n\n \n\n \n\n13 \n\n\n", " \n\n \n\nAssessment for k fold cross-validation uses essentially the same measure as that used \n\nfor leave one out cross-validation. However, since the folds may be of slightly different \n\nsizes, unlike leave one out cross-validation, we use a weighted average of the squared \n\nerrors computed from within those splits. Thus, the k fold cross-validation measure for \n\nassessment based on k splits results in a CV sub k in parentheses.  \n\n \n\nNote that the final version of the formula looks exactly like the previous one when we \n\nwork it out to the end. The key difference is that the responses are predicted using \n\nmodels fit without any of the observations within fold j, which is the fold-containing \n\ndata point i. Conceptually though, once we have the predicted values, the computation \n\nof the cross-validation measure for assessment is the same as with leave one out cross-\n\nvalidation. And this applies for both numeric y and categorical y as responses. \n\n \n\n \n\n \n\n14 \n\n\n", " \n\n \n\nFirst step in programming cross-validation is obtaining an appropriate split of the data. \n\nFor leave-one-out cross-validation, this is as simple as cycling through the observations \n\nand leaving each out in turn as the test set. Hence, this can be done by simply labeling \n\nthe cross-validation groups from 1 up to n, leaving each one out in turn, and then using \n\nthe rest of the data as the training set.  \n\n \n\nFor k-fold cross-validation, however, we need to do a random assignment of labels. So \n\nwe begin with a set of all possible labels corresponding to the length of the \n\nobservations, n, that we have in our data set, and labeling with 1 up to number of folds \n\nas our labels. We then randomize by using the sample function, and we will do so in \n\nconjunction with set.seed typically, to allow for a consistent starting point.  \n\n \n\nOnce we have our cross-validation groups, or stored-in CV groups, we then split the data \n\nby iteratively designating each group and using that designation to split into train and \n\ntest sets. And we'll see this inside a loop on the next slide.  \n\n \n\n \n\n15 \n\n\n", " \n\n \n\nTo integrate the designated splits stored in CV groups, the next step in the programming \n\nprocess requires that we cycle through each fold of data, which we do so using for-\n\nloops. Even before starting the for-loop, we need to set up storage space to contain the \n\npredicted values from inside each iteration of the loop. We then-- in the next segment \n\nof lines of code, the first segment inside the loop-- iterate through designating each \n\ngroup in turn by using a logical vector, CV groups equals some value, and we're going to \n\nuse II to iterate through the loop since it's an easier search string.  \n\n \n\nOnce we have the logical vector, we use that to designate our train set, not including \n\ngroup I, and our test set, which does include group I data. The next line of code fits the \n\nmodel to whatever train set we've split off. We store that in, say, model fit, and we then \n\nuse that model fit to predict our test data-- that is, the data in the test set-- and store \n\nthat in predicted. Since that is just the set of predicted values for this subset of data-- \n\nthis particular test set-- we then need to further put that into our all-predicted storage \n\nin the correct locations as designated buyer, logical vector group I.  \n\n \n\nA brief comment, looking ahead we'll discuss a wrapper function that does much of this \n\nfor a wide variety of different modeling techniques, and we'll be talking about that in \n\nlesson 8.  \n\n \n\n \n\n16 \n\n\n", " \n\n \n\nLeave one out cross-validation is a very useful tool for assessment since it can predict all \n\ndata in the entire set, yet does so based on models fit without that data. In addition, \n\nthese models are fit using nearly all the data. The CV measure as an estimate for \n\nvariability is close to unbiased, and this is the lowest possible bias among all possible \n\nchoices for splits.  \n\n \n\nHowever, the variance of the estimate is increased due to the overlap among the data \n\nsets used to fit the models. The splits of the data are simple and deterministic. There is \n\nonly one possible way to split the data into n folds. This has the downside of making the \n\nmethod computationally intensive with n plus model fits, especially if the model fitting \n\nprocess is time-consuming. \n\n \n\n \n\n \n\n17 \n\n\n", " \n\n \n\nK fold cross-validation, again, predicts all data in the data set based on models fit \n\nwithout that data. In addition, those models are fit using a high proportion of the data. \n\nHowever, since each training set is smaller than with leave one out cross-validation, the \n\nCV measure will be slightly biased when estimating the variability. Since there is less \n\noverlap among the data sets used to fit the models, the variance is decreased, resulting \n\nin a trade-off between bias and variance.  \n\n \n\nAlong with more mathematical evaluations of the cross-validation procedures, this \n\nsuggests that a good compromise number of splits is a k of something between 5 to 10 \n\nfolds. The splits of the data are typically done at random. Multiple ways of splitting the \n\ndata will lead to different CV values. However, this method can be potentially much less \n\ncomputationally intensive with k plus 1 model fits, depending on data size, as well as \n\nmodel-fitting process. This again suggests the use of k fold CV. \n\n \n\n \n\n \n\n18 \n\n\n", " \n\nCorrect answer is on the last page. \n\n \n\n \n\n \n\n \n\n19 \n\n\n", " \n\n \n\nSuppose we have multiple models of which we would like to select the best model for \n\npredicting the response. That is, we want to get predictions as close as possible to the \n\ntrue response values. We also want to get honest predictions.  \n\n \n\nThese dual goals suggest using cross-validation and picking the model for which the \n\ncorresponding measure is minimized. We do this by splitting the data as we typically \n\nwould for k fold cross-validation, then each model is fit to the training data and applied \n\nto the testing data, and a CV k measure is computed for that model. Ideally, a model, or \n\npossibly a group of models, will emerge as best. That is, with lowest value of CV k. And \n\nwe select that as the best model for predicting new data. \n\n \n\n \n\n \n\n20 \n\n\n", " \n\n \n\nWe will be using the body fat data set, with corrections as specified in the reference in \n\nthe notes from the author. The response is a body fat measurement. There's actually \n\nseveral such measurements in the data set. The one we're going to be using is body fat \n\nSiri. It's computed using measurements via a particular equation called series equation.  \n\n \n\nThe predictors that we have available are a set of 14 different variables-- which are very \n\neasy-to-measure physical characteristics-- and regroup these by location on the body as \n\na way of talking about different sets or subsets of measurements that might be the most \n\nmeaningful.  \n\n \n\nSo we will look at five different models-- that include subsets of the variables, and one \n\nmodel that includes all 14 possible predictor variables. We will then apply cross-\n\nvalidation measures to select which of these is the best model, using a CV measure that \n\nis effectively our honest MSE-- or honest mean square error, since our response is \n\nnumeric. \n\n \n\n \n\nNotes: \n\nThe bodyfat.csv file is available in the online course. \n\n \n\nThe data set author’s description and adjustments is available here: \n\nhttp://www.amstat.org/publications/jse/v4n1/datasets.johnson.html \n\n \n\n \n\n \n\n21 \n\n\n\n", " \n\n \n\nThis slide represents a video/screencast in the lecture. The transcript does not \n\nsubstitute video content. \n\n \n\nWe practice the application of cross-validation for model selection using the body fat \n\ndata set as briefly introduced on the previous slide. We use a first sequence of R \n\ncommands to read the data set into R and review the data. We'll begin with a few \n\nlibraries, and these libraries are really only necessary if you are going to be doing some \n\nof the fancier plotting later on.  \n\n \n\nI'm going to make sure that I've got my working directory set to the correct location, \n\nand then take a look at the body fat data. So I'm going to take a look at the size of the \n\ndata set, as well as the names. I could also open up the data set if I felt so inclined in a \n\nseparate tab. Although that line is commented out because this is not an essential part, \n\nbut it can help you take a look at the type of data you have, specifically that all the \n\nvalues in this data set are given or numeric.  \n\n \n\nAlso, we're going to be using just one of the variables, body fat siri, as our preferred \n\nvariable. We'll have case-- or as our preferred response, we'll have case, which is not a \n\nused variable. Another body fat and density measures, body fat brozek and density, \n\nwhich will not be used, because there are very close analogies of body fat siri.  \n\n \n\nInstead, what we're going to be using are some subset of the remaining variables, age, \n\nbody size, weight, height, BMI, some upper body measurements, some lower body \n\nmeasurements, basically, body measurements that are very easy to obtain. And we're \n\ngoing to take a look, and we could use just base R to view our data. If we did that, we're \n\ngoing to get a readable, but not very visually appealing description of how body fat \n\nrelates to the different variables.  \n\n \n\n22 \n\n\n", " \n\nNow, you'll notice that I organized or arranged the variables-- the order of the variables \n\nso that the response is listed last, and what that does in a scatterplot matrix is it ensures \n\nthat the response is listed on the vertical axis, which is how we think about it, and the \n\npotential predictors on the horizontal axis. We see that a lot of these predictors, at least \n\namong the first 6 or 7 or so of them are strongly correlated to body fat siri but are also \n\nassociated with each other, and that might suggest just being able to use subsets of \n\nthem.  \n\n \n\nMore visually appealing-- visually appealing scatterplot matrix starts with setting the \n\nsize of the text to be smaller so that it shows up on screen here well enough and that \n\nplots the same data but in the upper quadrant is going to show the correlation at a size \n\nthat is going to be visually legible. And so this one takes a little bit longer to process \n\nbecause it's a more complex visual. So we'll take a look at that one.  \n\n \n\nAgain, it'll be some of the similar observations. That is body fat siri on the vertical axis \n\ndown here. The various predictors, age, weight, height, BMI on the horizontal axis of \n\neach plot. This also provides a smooth curve-- a smooth type histogram of each of the \n\nvariable values as well as lists the correlation in this upper quadrant.  \n\n \n\nWe could do something similar for the last 7 variables. The last 7 potential predictors, \n\nand you're welcome to take a look at that on your own. It gives a very similar type of \n\nplot.  \n\n \n\nSo we have used these first 21 lines or so, again, depending on whether or not you use \n\nthe more complex visuals. It could be fewer lines than that just to get a feel for the data. \n\nWe then specified the linear models we're going to be fitting. The body fat siri modeled \n\non three variables, size, effectively, variables, weight, BMI, height, and we're going to \n\nstore that. Call that model 1.  \n\n \n\nSimilarly, we had the torso measurements, the torso and body size measurements, the \n\nupper body measurements, the lower body measurements, and then all the predictors. \n\nNow, this is a model specification that's a little bit more succinct than simply taking the \n\nresponse modeled on all 14 predictors. We could certainly do that, but it's more \n\nsuccinct to say, I want to model this on the rest of the predictors except for case, except \n\nfor body fat brozek, except for density.  \n\n \n\nSo now that we have those models specified, we want to apply cross-validation to get \n\nan honest measure of the model's predictive ability. And in order to set up cross-\n\nvalidation, we need to define labels for cross validation groups. If we were performing \n\nleave one out cross-validation, we would define the number of folds to be n and the CV \n\ngroups to simply be the integers 1 up to n.  \n\n \n\n \n\n23 \n\n", "In this case, instead, I'm going to be using 10-fold cross validation, because that is \n\nperhaps the slightly more difficult application of this. So I'm going to define n folds \n\nequals 10. I'm going to set a seed, and that seed will be required so that you produce \n\nthe same random sample eventually that I do using the sample function. So I'm going to \n\nrun those two lines, then I'm going to set up my groups as my repeat. My fold labels up \n\nto as many times as I need to.  \n\n \n\nAnd if we take a look at what's inside groups, it does a whole bunch of repeats of the \n\ndigits 1 through 10 up through 252 total observations. And finally, CV groups randomly \n\nreorders them. Well, again, randomly to produce the same random reorder, we use a \n\nset seed to get consistent results so that you can follow this example and it produces \n\nthe exact same thing that we will see on screen here.  \n\n \n\nSo if you typed in CV groups and you increased this display area from the console, you \n\nshould see this exact same listing of CV groups. And we're going to be using those CV \n\ngroups to run our cross-validation process for just one of the models. Now, how we do \n\nthat is-- well, I'm going to copy directly from the slide in which we saw for loops \n\npseudocode, and we're going to revise that here to include a model fit. We're going to \n\nuse model 1 in this case.  \n\n \n\nSo the point for doing-- the purpose for doing this is to help you see that we are actually \n\njust doing the code with slight adjustments. Now, I mentioned that I copied this all \n\npredicted rep NA n. That's my setup for the storage. I already had that on screen, so we \n\ndon't need to do that twice, and what I want to do next is loop through with index \n\ndouble ii.  \n\n \n\nI'm just going to refer to that as i. So my index i going from 1 to n folds. I'm going to \n\ndefine a logical vector. And if you like, you can add in a comment specifying that. Logical \n\nvector for group ii, and we then use that logical vector not. So that is the negation of the \n\ntruth and forces in there to define our training set.  \n\n \n\nNow, here's the first place where we have to make sure that we've referring to the \n\ncorrect data set. So the data that we're using here is called body fat data set, and there's \n\na couple of ways we could handle this. If I want it to be a little bit bigger picture and \n\nreuse the exact same code, I could say data set equals body fat and then run this code \n\nas is.  \n\n \n\nIn this case, I'm actually just going to, instead of data set, type in body fat, which is my \n\ndata set. Only selecting the rows that are not in group i, and then for my test data, I'm \n\ngoing to select just the rows that are in group i. And so these brackets specify rows and \n\ncolumns, and we are just subsetting the rows and the observations. So those first three \n\nlines are ready to go.  \n\n \n\n \n\n24 \n\n", "Now, I see a little x here, because I don't have an actual model specification. I have to \n\ntype that in, and the type of model that I'm fitting is an LM for linear model. And you'll \n\nnotice that I had previously defined by my models up here, so I can just refer to them by \n\ntheir labels. The model specification in model 1.  \n\n \n\nThe data is going to be my train set data, and then when I make predictions, and it \n\nshould work to apply the generic predict function here on my model fit using new data \n\ntest set. This line will have to be adjusted for certain applications or applications of \n\ncertain methods, depending on how predict is integrated with that method. I'll then \n\ntake those values that are stored in predicted and store them in all my predicted values \n\nbut only in locations where group i is true.  \n\n \n\nNow, good practice is to always try an application of this loop before you actually run \n\nthe full loop. So I'm going to set ii equals 1 and run these commands. So notice that I'm \n\nnot actually doing the four part, I'm just running each inner line. Whoops, except I \n\nforgot to write to run my ii line.  \n\n \n\nI'm going to run each inner line at a time. Now, everything seems to run properly up \n\nuntil this point because I forgot to define my all predicted. Let's see. Wouldn't hurt to \n\njust take a look at everything that's inside here. Group i is a vector of true falses like I \n\nexpected. Check out the dimensions of my train set, the dimension of my test set, and \n\nso on and so forth.  \n\n \n\nSo internal to the loop, you can check out whether each line is operating as you \n\nintended it to. You can also do a summary of models fit, which would have been fit only \n\nto the training set of 226 observations, and of course, that seems to display a summary \n\nof a linear model. I could take a look at the 26 observations that should be stored and \n\npredicted. And now, when I put them in only the group i locations, when I type in all \n\npredicted, I should only have 26 of the 252 locations filled in with values, because I have \n\nonly done the first loop here.  \n\n \n\nAll right, so now that I verified that those inner workings are running correctly, let me \n\nrun the full loop. It doesn't take long at all. I'm going to verify that all predicted actually \n\ncontains values in all 252 locations, and then I would like to compare all honestly \n\npredicted values to my response. And so for simplicity in the coding here, I'm just going \n\nto relabel this as y.  \n\n \n\nMy y is my full set of response values from the body fat data set. I'm going to compute \n\nmy CV value as the mean of the squared errors using honest predicted values and \n\ndisplay that. 34.49. That in and of itself doesn't have an intrinsic meaning, but I'm going \n\nto use that to compare across separate models.  \n\n \n\nAnd if I put the response and predicted values together, I can use some visualizations \n\nfrom GG formula, or I could make a simpler plot. I'll add that in here as a and then \n\n \n\n25 \n\n", "comment it out. I could make a simpler plot that just takes y hat y with the various titles \n\nand labels. So this would be the base R application after, and I will need to correct and \n\nput this in as all predicted because I'm not operating from my data frame.  \n\n \n\nOr if I want to operate from this data frame, that includes my observed and predicted \n\nvalues called y and y hat, respectively, I can make a more palatable visual. The last two \n\nparts of this asks us to add some efficiency. And so if I wanted to redo this process, for \n\nmy different models, I would have to go in and change my model label. I would also \n\nhave to go in and change my storage.  \n\n \n\nSo this could be a little bit meticulous if I'm trying to go through and do this model after \n\nmodel. So an added efficiency could be to simply put the models together in a list, and it \n\nmight also be helpful to make a list to store my all CV values. And then I'm going to pick \n\nout one of the models, say, the first model and run the loop across that.  \n\n \n\nSo it's going to be very similar to this process up above. In fact, I'm going to copy all the \n\ncommands over here, except now, all I have to change is the value of m. And I can do \n\nthis instead of referring to the name of the model by referring to the m-th location in all \n\nmodels. And when I come down here for CV value, instead of calling the CV value 1 and \n\nthen having to relabel that, I can instead just store this in a location on all CV values.  \n\n \n\nSo all CV values location m is going to be the mean of the squared errors. And finally, I \n\ncan change the titles a bit by simply pasting to the end of the designation of the name or \n\nof the title. And so this will add a little bit of streamlining. So let's see this.  \n\n \n\nIf I repeat this process now for m equals 1, and my mistake was that all models here is \n\nactually a list, not a vector, and so I have to specify the list item m. Now that operates as \n\nplanned, and I can see that it went through the same process. The cross validation \n\nprocess using all models one, the first model in my list of models, and then stores my \n\ninformation.  \n\n \n\nAnd if I wanted to get base R to operate correctly, I need to change that to mean. And \n\nmy fancier plotting looks similarly shaped, just a little bit more visually [INAUDIBLE]. OK, \n\nso great. So we streamlined-- or so we made that process a little bit more efficient in \n\nthat we only had to change m here at the beginning.  \n\n \n\nSo one final step that could add some streamlining to this is to make a for loop to cycle \n\nthrough the models. And so I am going to open up a document, where I have done this \n\nand just copied that over. And you can see I now have CV model selection open, and so \n\nthat I don't have to retype all of this.  \n\n \n\nWe're taking realistically the entire process from up here that we applied for a particular \n\nvalue of m. We set up storage, we ran a loop to do the cross validation, and we stored \n\nthe value for our CV as one of the items in all CV values. And so we take those steps, set \n\n \n\n26 \n\n", "up our storage, run the cross validation loop, compute CV and store it, and just getting a \n\nlittle extra fancy here, I am going to store in a list of all plots, I am going to store the plot \n\nfor model m.  \n\n \n\nSo if I set up a list, and then I run this across all models, now, I know that this internal \n\nstuff works already because I practiced that up here for m equals 1. And so for m, \n\ncycling through all my different models, it's going to run all those interior cross \n\nvalidations. At the end of this, I will get all CV values for models 1 through 6, and I will \n\nsee that the model that appears to have minimized is model 3. Has the minimum CV \n\nvalue.  \n\n \n\nAnd so in fact, I am going to want to take a look at the plot of the observed minus \n\npredicted for that model 3. Is best. And several of the others are close in terms of CV \n\nvalues, but model 3 really does the best job. If we wanted to compare that to a model \n\nthat really doesn't do a very good job in predictions corresponding to the observed \n\nvalues, we could take a look at all plots 5.  \n\n \n\nSo all plots for model 5, and we see what happens here is that there are some outlying \n\nvalues that where the predictions do not match up very well with the observed, and that \n\ninflates the mean square. So hopefully this helps you build your own loop for cross-\n\nvalidation and demonstrates how, indeed, we can use that for model selection.  \n\n \n\n \n\n27 \n\n", " \n\nIt is now of paramount importance to recognize that this model selection step has been \n\nincorporated as part of the model-fitting process. That is, to get from the data set to the \n\nfitted model, we must proceed through all steps of the model-fitting process. \n\n \n\n \n\n \n\n \n\n28 \n\n\n", " \n\nSince cross-validation is used to select the model, it has been incorporated as part of the \n\nmodel-fitting process. When fitting a model, we must proceed through all steps that \n\nwere used in the process, including this cross-validation for model selection. That means \n\nevery time we select a model, we must implement a cross-validation. Why is this \n\nimportant? \n\n \n\n \n\n \n\n \n\n29 \n\n\n", " \n\n \n\nThis leads to what is known as the double application of cross-validation. If we are to \n\nuse cross-validation for the model selection process, we need to recognize that there \n\nwill be another level of cross-validation surrounding that used to validate the overall \n\npredictive ability of the model. That is, we will eventually be using two layers of cross-\n\nvalidation. The outer split of the data is used for model validation, while the inner split is \n\nused for model selection.  \n\n \n\nAs this is a more complex application of the methods, we will wait until further \n\napplications and lessons to implement this double usage of cross-validation. So for now, \n\nin the next several lessons, we will first focus on applying cross-validation simply for \n\nmodel selection. And we will practice that through a variety of models. \n\n \n\n \n\n \n\n30 \n\n\n", " \n\n \n\n \n\n \n\n \n\n31 \n\n\n", " \n\n \n\n \n\n \n\n32 \n\n\n", " \n\n \n\n \n\n \n\n \n\n \n\n \n\n33 \n\n\n\n"]], ["c:\\users\\mjols\\documents\\ds uwec courses\\740 data mining\\lesson2\\ds740_lesson2_presentation3.pdf", ["DS 740\n\nData Mining\n\nBootstrapping for Error Estimation\n\nAnother way to reuse data\n\n \n\n \n\nImportant note: Transcripts are not substitutes for textbook assignments. \n\n \n\n1 \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "Learning Objectives\n\nBy the end of this lesson, you will be able to:\n\n• Explain bootstrapping method and understand its application for \n\n• Define standard error,                  , for estimating error of a point \n\n!\"! ! \t\n\nerror estimation.\n\n!\t\n\nestimator     .\n\n• Apply bootstrapping method for error estimation.\n\n \n\n \n\n \n\n \n\n2 \n\n\n\n\n\n\n\n", "Ideal Error Estimation of Parameter a\n\nIMPRACTICAL\n\nTraining \n\nSet 1\n\nTraining \n\nSet 2\n\nFitted \n\nModel 1\n\nFitted \n\nModel 2\n\n•••\n\n•••\n\nTraining \n\nSet B\n\nFitted \n\nModel 2\n\n!!\t\n\n!\"2\t\n\n\t\t\t:\t\n\n!\"# \t\n\n\t\t\t:\t\n\n!\"#\t\n\nVariability of estimator \n\nmeasured as standard \n\ndeviation of      -values\n\n!\"#\t\n\nfrequentist error estimation\n\n \n\n \n\nThe theory behind an ideal error estimation would mean that we select \n\ntraining sets many, many times from the population and obtain a parameter \n\nestimate from each of these training sets, or samples. \n\n \n\nRepeating this estimation many, many times would produce many replications \n\nof the estimator, alpha hat. The variability of these values is measured by the \n\nusual standard deviation. \n\n \n\nThis is the traditional approach known as the frequentist statistical approach, \n\nand is based in sampling distributions of statistics theory. However, this \n\ntheoretical approach is not practical in application since we cannot, in nearly \n\nevery situation, cannot select multiple training sets. \n\n \n\n \n\n \n\n3 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "Practical Error Estimation of parameter a\n\nOne sample, with one fitted model and one estimate.\n\nOne\n\nBig\n\nTraining set\n\nFitted\n\nModel 1\n\n!\"\t\n\nHow to measure\n\nvariability of\n\nestimator     ?!\"\t\n\nHow to get “best estimate” (as above) and also estimate error?\n\n \n\n \n\nFrom a practical perspective, adhering to previous principles, we would like to \n\nuse any and all data we select from the population to get the best possible \n\nestimate of the parameter. So how can we use just one parameter estimate to \n\nmeasure variability of that estimate? Well, the simple answer is we can't. \n\n \n\nThus, somehow we have to devise a method of sampling that will still allow us \n\nto come up with an overall estimate as shown here, but also will produce \n\nmultiple sets of data from which we can obtain multiple parameter estimates \n\nthat will importantly exhibit approximately accurate error. \n\n \n\n \n\n \n\n4 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "Re-Sampling to Produce Bootstrap Sets\n\nBootstrap method: Originally due to Efron (see note below).\n\nAll Data Elements\n\nOriginal sample with n elements\n\nB bootstrap samples\n\nWith replacement\n\neach element\n\ncan appear\n\nmultiple\n\ntimes per\n\nsample\n\n1\n\n2\n\n3\n\nNb\n\n•••\t\t\t\t•••\n\n \n\n \n\nThe bootstrap is an effective method of accurately estimating the error for a \n\nparameter estimation. This method was introduced by Bradley Efron in 1979, \n\nand has recently gained more prominence as a computationally practical way \n\nof estimating error, even within basic statistical methods. \n\n \n\nThe concept is simple. From the initial sample, which was selected from the \n\nmuch larger population, we re-sample a number of times-- let's call that b \n\ntimes-- with replacement, obtaining b bootstrap samples. Sampling with \n\nreplacement means that each individual or object from the original sample \n\nfrom which we're selecting, from which we're re-sampling, can appear \n\nmultiple times in a single bootstrap sample.  \n\n \n\nNotes: \n\n \n\nFor more about Bradley Efron's creation of the bootstrap method, see The \n\nAnnals of Statistics, Volume 7, Number 1 (1979, p. 1-26.) \n\n \n\n \n\n \n\n5 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "Ideal Error Estimation of Parameter a\n\nEASY TO\n\nPRACTICE\n\nTraining \n\nSet 1\n\nTraining \n\nSet 2\n\nFitted \n\nModel 1\n\nFitted \n\nModel 2\n\n•••\n\n•••\n\nTraining \n\nSet B\n\nFitted \n\nModel 2\n\n!∗!\t\n\n!\" ∗2\t\n\n\t\t\t:\t\n\n!\" ∗$\t\n\n\t\t\t:\t\n\n!\"∗$\t\n\nVariability of estimator \n\nmeasured as standard \n\ndeviation of       -values\n\n!∗!\t\n\nbootstrap error estimation\n\n \n\nOnce we get these multiple bootstrap samples, bootstrap error estimation \n\ninvolves taking the bootstrap samples, fitting the model on each sample and \n\nobtaining the estimate of the parameter from each of the samples. We take \n\nthese many estimated values and compute their standard deviation to \n\napproximate the error. This is easy to put in practice, particularly with powerful \n\ncomputation tools. \n\n \n\n \n\n6 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "Error Estimation from Re-samples\n\n• Variability of the \n\n!\"∗$\n\n\t\n\n-values (r = 1, 2, …, B) from the re-samples is \n\napproximately accurate for the standard deviation of the estimates.\n\n• Let the average estimator value be denoted: \n\n!\"#(%&∗) = \t\n\nThen define the standard error of the estimator to be: \n\n1\n\n,\n\n,\n\n- %&∗.\n\n.=1\n\n\t\n\n!\"#(%&) = )\n\n1\n\n# − 1\n\n#\n\n,(%&∗. − /01(%&∗))2\n\n.=1\n\n\t\n\n \n\nTheory, as well as empirical studies, have verified that the bootstrap method \n\nproduces accurate estimates of variability for a wide variety of models. The \n\nformula for computing the so-called standard error based on the b bootstrap \n\nsamples is in fact just our familiar formula for standard deviation. It is applied to \n\nthe bootstrap estimates, the alpha hat stars, for estimating the parameter \n\nalpha. \n\n \n\n \n\n \n\n \n\n7 \n\n\n\n\n\n\n", "Question 1 \n\n \n\n \n\nAnswer is at the end of this transcript \n\n \n\n \n\n \n\n \n\n8 \n\n\n", "Bootstrapping in R\n\nPackage: boot\n\nDownload x1\n\nInput function: fits model and outputs the parameters to be estimated.\n\nApplication: the function, boot, uses data set and input function, \n\nproducing the bootstrap re-samples and the coefficient estimates from \n\nthose samples.\n\n \n\n \n\nThe package boot must be downloaded into R again. As with all packages, it \n\nonly needs to be done once in order to most easily do bootstrapping in R. \n\nWhile one can write their own program to do bootstrapping, boot is a very \n\nintuitive package with a function of the same name that requires simply a data \n\nset and an input function. \n\n \n\nThe input function must fit the model and output the parameters that are \n\ndesired to be estimated. The function then does all the heavy lifting. So \n\nrunning boot will produce the bootstrap re-samples as well as the coefficient \n\nestimates from those samples. And you will see a demonstration of how to \n\nwork with those coefficient tests. \n\n \n\n \n\n \n\n9 \n\n\n\n\n\n", "Body Fat Example Revisited\n\nUsing bodyfat.csv data set, with author’s description and adjustments (errors \n\nin density and height have been corrected). \n\nResponse:  BodyFatSiri (using Siri’s equation)\n\nPredictors: Abs, Weight, Wrist, Forearm, Neck, Biceps, Age, Thigh, Hip, \n\nAnkle, BMI, Height, Chest, Knee.\n\nModel 1 uses first predictor (Abs) only; Model 2 uses all 14 predictors via \n\nmultiple linear regression.\n\nBootstrap used to compute\n\nstandard error based on\n\nbootstrap sample estimates:\n\n!\"#$%&' = )\n\n1\n\n# − 1\n\n#\n\n,$%& ∗. − /01(%& ∗)'\n\n.=1\n\n2\n\n\t\n\n \n\nNotes: \n\n \n\nThe bodyfat.csv file is available in the online course. \n\nThe data set author's description and adjustments are available here: Fitting \n\nPercentage of Body Fat to Simple Body Measurements - Roger W. Johnson, \n\nCarleton College \n\n \n\n \n\n \n\n \n\n10 \n\n\n\n\n\n\n\n\n", " \n\n \n\nThis slide represents a video/screencast in the lecture. The transcript does \n\nnot substitute video content. \n\n \n\nWe're ready to look at our third reuse of data lecture example. Once again, this \n\nexample is based in the body fat data. So if you have not already read in that \n\ndata set, please do so now. You may also make sure that you've changed your \n\ndirectory. We're going to start by fitting the response on all predictors to get a \n\nfull fit of a FOL model. So that's body fat response, calculated by series \n\nequation, fit on all 14 predictors. And we see a summary of that, coefficient \n\nestimates, and standard error, as computed mathematically. We'll also fit a \n\npartial model, as this will give us a little bit easier output to work with on just \n\nfour predictors. \n\n \n\nThe four predictors, selected by stepwise regression, and four predictors \n\nchosen by cross validation. All right, so now that we have these two model fits, \n\na full fit and a partial fit, we're going to take a look at fitting these models and \n\nestimating coefficients on simulated samples selected from the original \n\nsample, with replacement. And so that process is what we've introduced as \n\nbootstrapping. In order to do bootstrap, you need to install the package boot. \n\nOnce you've done that, you can read in the package. And we need to be able \n\nto define functions that output the coefficients that we're aiming to estimate. \n\nSo beta fn FOL stands for beta function FOL. And this is a function that takes \n\ninput data with some sort of index, fits the fol model we previously defined, \n\n \n\n11 \n\n\n", "and fits it to that part, the index part of the input data. We'll then return the LM \n\nfit boot coefficients. \n\n \n\nSo all that is the definition of a function. We'll do something very similar for the \n\npartial fit on the partial model and define that function data beta fn partial. \n\nWe're now ready to run the boot function to simulate the actual re-samples, \n\nand then produce the coefficients for each re-sample. And we're going to go \n\nwith the partial model bootstrap to begin because, as I said, it's a little bit \n\nsimpler. So we'll all start from the same place. We'll set the random seed to be \n\n2. And our partial boot output is running the boot function on the body fat \n\ndata set, with the function as defined up above here. And we're going to do \n\n1,000 bootstrap simulations and see what comes out. \n\n \n\nNow this particular function can take a little bit of time to run, depending on \n\nhow long each model fit takes. It turns out here that we're coming up with \n\nsome summary of our outputs. T1 represents our first term. And in this case, \n\nthat is simply the intercept. T2, the second term, that's the coefficient for the \n\nfirst predictor-- ABS, in this case. So we might want to take a little bit more of \n\na look at some of this information. So the partial boot output T element gives \n\nus all the coefficient estimates. So if I were to type this in up here, I would end \n\nup with a matrix that should be 1,000 values for the five coefficient estimates. \n\nSo let's just not-- rather than printing that out, let's just take a look at the \n\ndimension. So the rows are the 1000 estimates, and each of the columns is \n\nwhich coefficient it is. So partial boot output, element T, picking the first \n\ncolumn, would be all 1,000 coefficient estimates for the intercept from the \n\n1,000 bootstrapped samples. So I will go ahead and just copy and paste that \n\nover. And we see the 1,000 values listed here from the different partial model \n\nfits on the bootstrap sample. \n\n \n\nNot a great summary. So let's actually summarize. Let's take-- calculate the \n\nstandard deviation and make a histogram of those values. When we take a look \n\nat the histogram, it has a-- not surprisingly-- normal shape. Good assumptions \n\nwere met for this model fit. So we expect the coefficient estimates to have a \n\ngood fit. And the standard deviation-- well, the standard deviation is estimated \n\nto be 7.953. Now we might wish to compare this to the standard error as \n\nmathematically computed based on mathematical statistics theory. And so, \n\nthat was output in our original summary of the overall fit on the original data-\n\nset, on the original sample. \n\n \n\nSo we're going to pull off just the standard errors, the first element. Copy and \n\npaste this server. And we see a slightly lower estimate. Again, this is not \n\nsurprising. We get a higher estimate based on the bootstrap samples because \n\nthat's a little bit more realistic. All right, similar sort of thing, when we take a \n\nlook at the coefficient estimates for the ABS term-- we'll copy and paste all of \n\n \n\n12 \n\n", "this over right away. We see all the coefficient estimates for the 1,000 \n\nbootstrap samples. And we obtain a standard deviation estimate-- estimated \n\nstandard deviation. That is, again, higher than mathematical computation. You \n\ncould repeat the same thing for the third term, which is weight. And we see a \n\nsimilar sort of result here. If you'd like, you can continue this, and take a look at \n\nthe full model bootstrap. I'll just run all the commands in one fell swoop. And it \n\ntakes a little bit. It's going to take a little bit longer here to fit the bootstraps. \n\nBut we see a similar sort of summary. \n\n \n\nNotice that the estimated standard deviation for the intercept is much larger \n\nthan the mathematically computed one, as is the standard error, the estimated \n\nstandard error, or the estimated standard deviation from bootstrap for the \n\nweight term. \n\n \n\nNotes: \n\n \n\nSee the online course for a downloadable R file containing the set of \n\ncommands used in this demonstration. \n\n \n\n \n\n \n\n13 \n\n", "Summary\n\nBootstrapping is a method that can be applied to estimate variability\n\nof parameter estimates through re-using the original data.\n\n• Re-samples are selected from the original sample, with replacement\n\n• A fitted model is produced for each re-sample, resulting in new \n\nparameter estimate(s) for each re-sample.\n\n• The boot-strap standard error, calculated as the simple standard \n\ndeviation of these estimates, is approximately correct for the \n\nstandard deviation of parameter estimates from different samples.\n\nThe R function boot (from package boot) is used for applications.\n\n \n\n \n\n \n\n \n\n14 \n\n\n\n", "Question 1 Answer \n\n \n\n \n\n \n\n15 \n\n \n\n\n"]], ["c:\\users\\mjols\\documents\\ds uwec courses\\740 data mining\\lesson3\\Cafe_correlation.pdf", ["l\n\nd\n\no\n\nS\n\n.\n\nd\n\nn\n\na\n\nS\n\n.\n\nd\n\na\n\ne\n\nr\n\nB\n\nl\n\nd\n\no\n\nS\n\n.\n\ns\n\np\n\na\n\nr\n\nW\n\nl\n\nd\n\no\n\nS\n\n.\n\np\n\nu\n\nC\n\n.\n\nt\n\ni\n\nu\n\nr\n\nF\n\nl\n\nd\n\no\n\nS\n\n.\n\ns\n\ne\n\nk\n\no\n\no\n\nC\n\ni\n\np\n\nm\n\ne\n\nT\n\n.\n\nx\n\na\n\nt M\n\ne\n\ne\n\nf\n\nf\n\no\n\nC\n\n.\n\nd\n\nn\n\na\n\n.\n\na\n\nd\n\no\n\nS\n\n.\n\nl\n\na\n\nt\n\no\n\nT\n\ns\n\na\n\nd\n\no\n\nS\n\nl\n\ns\n\ne\n\na\n\nS\n\ne\n\nt\n\ns\n\na\n\nW\n\n.\n\np\n\nu\n\nC\n\n.\n\nt\n\ni\n\nu\n\nr\n\nF\n\ne\n\nt\n\ns\n\na\n\nW\n\n.\n\ns\n\ne\n\nk\n\no\n\no\n\nC\n\ni\n\ne\n\nt\n\ns\n\na\n\nW\n\n.\n\ns\n\np\n\na\n\nr\n\nW\n\nd\n\ne\n\nt\n\ns\n\na\n\nW\n\n.\n\ns\n\nm\n\ne\n\nt\n\nI\n\n.\n\nl\n\na\n\nt\n\no\n\nT\n\ne\n\nt\n\ns\n\na\n\nW\n\n.\n\nd\n\nn\n\na\n\nS\n\n.\n\nd\n\na\n\ne\n\nr\n\nB\n\ne\n\nt\n\ns\n\na\n\nW\n\n.\n\ns\n\nn\n\ni\n\nf\n\nf\n\nu\n\nM\n\nl\n\nd\n\no\n\nS\n\n.\n\ns\n\nn\n\ni\n\nf\n\nf\n\nu\n\nM\n\ne\n\nd\n\no\n\nC\n\n.\n\ny\n\na\n\nD\n\ns\n\ne\n\nc\n\nu\n\nJ\n\ni\n\ns\n\ne\n\ne\n\nf\n\nf\n\no\n\nC\n\ni\n\ns\n\np\n\nh\n\nC\n\nt\n\nMax.Temp\n\nFruit.Cup.Sold\n\nCookies.Sold\n\nBread.Sand.Sold\n\nWraps.Sold\n\nSodas\n\nTotal.Soda.and.Coffee\n\nSales\n\nFruit.Cup.Waste\n\nCookies.Waste\n\nWraps.Waste\n\nTotal.Items.Wasted\n\nBread.Sand.Waste\n\nMuffins.Waste\n\nMuffins.Sold\n\nJuices\n\nDay.Code\n\nChips\n\nCoffees\n\n1\n\n0.75\n\n0.5\n\n0.25\n\n0\n\n−0.25\n\n−0.5\n\n−0.75\n\n−1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"]], ["c:\\users\\mjols\\documents\\ds uwec courses\\740 data mining\\lesson3\\ds740_lesson3_presentation1.pdf", [" \n\n \n\n \n\nDS 740\n\nData Mining\n\nGeneralized Linear Models\n\nLogistic Regression and ROC Curves\n\n \n\n \n\nImportant note: Transcripts are not substitutes for textbook assignments. \n\n1 \n\n \n\n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "Learning Objectives\n\nBy the end of this lesson, you will be able to:\n\n• Perform cross-validation to assess a logistic regression model.\n\n• Create and interpret a ROC curve.\n\n• Decide when logistic regression is an appropriate method.\n\n• Describe when you would use Poisson regression.\n\n \n\n \n\n \n\n \n\n \n\n2 \n\n\n\n\n", "Generalized Linear Model\n\n! ! = !!  +  !!!!  + ⋯ +  !!!!   +  !\t\n\nNoise term\n\n! ! = !!  +  !!!!\n\n! ! = !!  +  !!!!!!   +  !\t\n\n!   +  !\t\n\n \n\n \n\nRecall from data science 705, that a generalized linear model is a model with \n\nthe form shown here. Where the X sub I's are the predictor variables and the \n\nbetas are the coefficients. \n\n \n\nEpsilon is a random term representing noise or the effects of other variables \n\nwhich were unmeasured. \n\n \n\nIt's called a generalized linear model because this model is linear with respect \n\nto the betas, not the X sub I's. That means that we can take powers of various \n\npredictor variables or products of predictor variables and we still have a \n\ngeneralized linear model. \n\n \n\nF of X is a function of the response variable. For a simple linear model, F of X \n\njust represents Y, the value of the response.  \n\n \n\n \n\n \n\n \n\n3 \n\n\n\n\n\n\n\n\n\n", "Logistic Regression\n\nlog\n\n!(!)\n\n1 − !(!)\n\n=   !!  +  !!!!  + ⋯ +  !!!!   +  !\t\n\n! ! =\n\n!!!!!!!!!⋯!!!!!\n\n1 + !!!!!!!!!⋯!!!!!\n\n\t\n\n \n\nIn logistic regression, F of X is the logit or the log odds were P of X represents \n\nthe probability that the point will take on a response value of one. That means \n\nthat the estimated probability, based on the values of the predictors, takes the \n\nform shown here.  \n\n \n\n \n\n \n\n \n\n \n\n4 \n\n\n\n\n\n\n\n\n\n\n", "Logistic Regression in R\n\nlibrary(ISLR)\n\nfit = glm(default ~ student + balance,\n\ndata=Default, family=\"binomial\")\n\nsummary(fit)\n\nLogistic Regression\n\n \n\n \n\nTo perform logistic regression in R, we use the GLM function with the \n\nargument family equals binomial. The GLM function will be loaded \n\nautomatically when you start R. But in this case, we're also loading an \n\nadditional library, ISLR which contains the default data set. So we can do an \n\nanalysis on the probability that someone will default on their credit card debt.  \n\n \n\nNotes: \n\n \n\nlibrary(ISLR) \n\nfit = glm(default ~ student + balance, \n\n          data = Default, family = \"binomial\") \n\nsummary(fit) \n\n \n\n \n\n \n\n \n\n \n\n5 \n\n\n\n\n", "Logistic Regression in R\n\nlibrary(ISLR)\n\nfit = glm(default ~ student + balance,\n\ndata = Default, family=\"binomial\")\n\nsummary(fit)\n\nLogistic Regression\n\n \n\n \n\nThe first argument in the GLM function is a formula for the analysis we want to \n\ndo. The variable before the tilde or squiggle is the response variable. Because \n\nthis is logistic regression, it takes on values of zero or one or yes or no. \n\n \n\nThe plus sign between the two predictor variables indicates that we're not \n\ninterested in analyzing an interaction between these predictors. If we did want \n\nto include an interaction term, then we would use a star or multiplication \n\nsymbol.  \n\n \n\nNotes: \n\n \n\nlibrary(ISLR) \n\nfit = glm(default ~ student + balance, \n\n          data = Default, family = \"binomial\") \n\nsummary(fit) \n\n \n\n \n\n \n\n \n\n \n\n6 \n\n\n\n\n\n", "Specifying the data set\n\nlibrary(ISLR)\n\nfit = glm(default ~ student + balance,\n\ndata=Default, family=\"binomial\")\n\nsummary(fit)\n\nuse this\n\nnot this\n\nnew_point doesn’t contain \n\nDefault$student\n\npredict( ) will ignore newdata\n\n \n\n \n\nThe argument data equals default specifies the name of the data frame where \n\nthe variables are stored. This argument is technically optional. We could omit it \n\nand instead use dollar sign notation, for example, default $student to specify \n\nthat we want our model to include the student column in the default data \n\nframe.  \n\n \n\nHowever, using dollar sign notation when we're specifying a model can create \n\nproblems with predictions later on. For example, suppose we want to make a \n\nprediction for a new data point which I'm storing in the variable newPoint \n\nthat's a student with a balance of $500. Well, the newPoint variable doesn't \n\ncontain a column called default $student, which is what the model is \n\nexpecting.  \n\n \n\nThis means that the predict function will ignore its new data argument and \n\nsimply return predictions for all of the data points in the original default data \n\nset. This can create inaccuracies in your cross-validation that are very hard to \n\ndebug. So to make your life easier, always use data equals instead of dollar sign \n\nnotation to specify the locations of your variables. This is true not just for \n\nlogistic regression, but for all the models that will use this semester. \n\n \n\n \n\n \n\n \n\n \n\n7 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "Summary(fit)\n\n!\t\n\nCoefficients:\n\nEstimate Std. Error z value Pr(>|z|)\n\n(Intercept) -1.075e+01  3.692e-01 -29.116  < 2e-16 ***\n\nstudentYes\n\n-7.149e-01  1.475e-01  -4.846 1.26e-06 ***\n\nbalance      5.738e-03  2.318e-04  24.750  < 2e-16 ***\n\n---\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 '' 1\n\n! ! =\n\n!!!\".!\"!.!\"#$∗!\"#$%&\"!.!!\"#$%∗!\"#\"$%&\n\n1 + !!!\".!\"!.!\"#$∗!\"#$%&\"!.!!\"#$%∗!\"#\"$%&\t\n\n> contrasts(Default$default)\n\nyes\n\nNo    0\n\nYes   1\n\n \n\n \n\nBy using the summary function, we get the following output for the logistic \n\nregression. The first column contains the estimated coefficients, or betas, \n\nwhich we can use to get a formula for the estimated probability of taking on a \n\nresponse value of 1. We can double check how R is interpreting the response \n\nvariable by using the contrast function. \n\n \n\nIn this case, we see that when the variable default, with a lowercase d, is equal \n\nto the word yes, R interprets that as a response value of 1. So, for example, the \n\nnegative coefficient for student yes means that being a student is associated \n\nwith a lower probability of defaulting on credit card debt. \n\n \n\n \n\n \n\n8 \n\n\n\n\n\n\n\n\n\n\n", " \n\nLogistic Regression in R\n\n!\t\n\nCoefficients:\n\np-value of H0: β = 0\n\nEstimate Std. Error z value Pr(>|z|)\n\n(Intercept) -1.075e+01  3.692e-01 -29.116  < 2e-16 ***\n\nstudentYes\n\n-7.149e-01  1.475e-01  -4.846 1.26e-06 ***\n\nbalance      5.738e-03  2.318e-04  24.750  < 2e-16 ***\n\n---\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 '' 1\n\n! ! =\n\n!!!\".!\"!.!\"#$∗!\"#$%&\"!.!!\"#$%∗!\"#\"$%&\n\n1 + !!!\".!\"!.!\"#$∗!\"#$%&\"!.!!\"#$%∗!\"#\"$%&\t\n\n \n\nThe fourth column of the output gives the P value for a hypothesis test where \n\nthe null hypothesis is that that coefficient is equal to zero. And the alternative \n\nhypothesis is that the coefficient is not equal to zero. \n\n \n\nIn this case, the P values for both student yes and balance are very small \n\nindicating that we should reject the null hypothesis. So we have strong \n\nevidence that both being a student and the balance on your credit card is \n\nassociated with the probability of defaulting.  \n\n \n\n \n\n \n\n9 \n\n\n\n\n\n\n\n\n\n\n\n", "Plot of  !(!)\t\n\n \n\n \n\n \n\nHere's what it looks like if we plot the estimated probabilities. The R code to \n\nmake this plot is shown below. \n\n \n\nNotes: \n\n \n\nR Code: \n\nlibrary(ggformula) \n\nlibrary(dplyr) \n\n \n\nDefault2 <- Default %>% \n\n  mutate(pred = predict(fit, type = \"response\"), # Add a column of predictions \n\n         default01 = ifelse(default == \"Yes\", 1, 0) # Make a numeric version of \n\n`default` \n\n         ) \n\n \n\nDefault2 %>% \n\n  gf_line(pred ~ balance, color =~ student, lty =~ student, lwd = 1.3) %>% \n\n  gf_point(default01 ~ balance, color =~ student, shape =~ student, size = 2) \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n10 \n\n\n\n\n\n\n", "Cross-Validation of Logistic Regression 1\n\nn = dim(Default)[1]\n\nngroups = 10\n\ngroups = rep(1:ngroups, length = n)\n\n⬅ Number of observations\n\n⬅ Using 10-fold CV\n\nset.seed(123)\n\ncvgroups = sample(groups, n)\n\nall_predicted = numeric(length = n)\n\n⬅ Ensures replicable results\n\n⬅ places observations in random groups\n\n⬅ space to store predictions\n\nfor(ii in 1:ngroups){\n\n⬅ Iterate for each fold\n\ngroupii = (cvgroups == ii)\n\n⬅ All observations in each  \n\ngroup created previously\n\nWithin Loop: Fit model to training set and make predictions.\n\nCode is different than for linear regression and k-nearest neighbors.\n\n}\n\n \n\n \n\nJust like with linear regression and k-nearest neighbors, we can use cross-\n\nvalidation to estimate the error rate of logistic regression on new data points \n\nwhich were not used to build the model. The structure of the cross-validation \n\nis going to be exactly the same as what you've seen before. We start by setting \n\nn equal to the number of observations or data points. In this case, it's the \n\nnumber of rows of the default data set. n groups is equal to 10 for 10-fold \n\ncross-validation. And we've created a groups vector to store copies of the \n\nnumbers from 1 to 10.  \n\n \n\nThen I have set.seed to ensure that my results are replicable. And I'm using the \n\nsample function to randomly reorder the groups vector. So this is placing the \n\nobservations into random groups.  \n\n \n\nThen I'm using the function numeric with length equal to n to create an empty \n\nvector where I can store the predicted probabilities of each data point \n\nbelonging to category one. Then I have a for loop where I'm going to iterate \n\nover the values of ii from 1 up to 10. Then I'm using group ii equals \n\nparentheses cv groups double equals sign ii. So this is testing, which values of \n\ncv groups are equal to the current iteration index, say, 1, 2, 3, on up to 10, and \n\ncreating a vector of trues and falses. So the variable group ii is equal to true for \n\nall of the observations that are in the ii-th fold.  \n\n \n\nThen we have space within for loop where we're going to fit the model to the \n\ntraining set and make predictions on the test set. This is the place where our \n\ncode is different than for linear regression or k-nearest neighbors. \n\n \n\n \n\n \n\n \n\n11 \n\n\n\n\n\n\n\n\n\n\n\n", " \n\nNotes: \n\n \n\nn = dim(Default)[1] \n\nngroups = 10 # using 10-fold cross-validation \n\ngroups = rep(1:ngroups, length = n) \n\n \n\nset.seed(123) \n\ncvgroups = sample(groups, n) \n\nall_predicted = numeric(length = n) \n\n \n\nfor(ii in 1:ngroups){ \n\ngroupii = (cvgroups == ii) \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n12 \n\n", " \n\n \n\n \n\nInside the for loop, after we create the group ii vector containing trues and \n\nfalses to tell which data points are in the current fold, we use that to create the \n\ntraining set and test set. The training set is all the rows of the default data \n\nframe that are not in the current fold. And the test set is all the rows that are in \n\nthe current fold.  \n\n \n\nNow we're ready to fit the logistic regression model. We do this using GLM just \n\nlike we did before we started using cross-validation. The only thing that \n\nchanges is that we're now fitting the model to just the training set. Then we \n\nuse the predict function to make predictions for the test set.  \n\n \n\nAnd because this is logistic regression, we use the additional argument type \n\nequals response to get our predictions in the form of predicted probabilities \n\nrather than predicted log odds. And finally, we store those predictions in the \n\nvector that we initialized before the start of for loop. Here we're storing it in \n\nthe positions that correspond to the data points that are in the current fold, \n\nsquare bracket group ii.  \n\n \n\nAn alternative way to specify which data we want to use to build the model \n\nwould be to use data equals default, the whole data set, and the additional \n\nargument subset equals exclamation point group ii. This works for logistic \n\nregression as well as several other types of models, such as linear regression. \n\nHowever, there are some types of models that we'll cover in this course, such \n\nas boosted trees, that don't accept subset as an argument. So I think it's easier \n\nto simply always use data equals the training set so I don't have to remember \n\nwhich methods do and don't accept the subset argument.  \n\n \n\n \n\n \n\n13 \n\n\n", " \n\nNotes: \n\n \n\nfor(ii in 1:ngroups){ \n\n  groupii = (cvgroups == ii) \n\n  train_set = Default[!groupii, ] \n\n  test_set = Default[groupii, ] \n\n   \n\n  model_fit = glm(default ~ student + balance, \n\n            data = train_set, family=\"binomial\") \n\n  predicted = predict(model_fit, newdata = test_set, \n\n                                type=\"response\") \n\n  all_predicted[groupii] = predicted \n\n} \n\n \n\n \n\n \n\n \n\n \n\n14 \n\n", "Power and False Positive Rate: Predicted Probabilities\n\n> all_predicted[1:3]\n\n[1] 0.001215781 0.001154122 0.010201310\n\n> table(all_predicted > 0.5, Default$default)\n\nDefaulted?\n\nNo  Yes\n\nFALSE 9629  229\n\nTRUE    38  104\n\nPredicted\n\nto default?\n\n \n\nThe predict function for logistic regression returns predicted probabilities. If \n\nwe want hard and fast predictions of yes or no, zero or one, we could use a \n\nthreshold value. \n\n \n\nFor example, if the probability is greater than 0.5, we could predict that the \n\nperson will default. That produces a confusion matrix like the one shown here. \n\n \n\n \n\n \n\n \n\n \n\n \n\n15 \n\n\n\n\n", "Power and False Positive Rate\n\n> all_predicted[1:3]\n\n[1] 0.001215781 0.001154122 0.010201310\n\n> table(all_predicted > 0.5, Default$default)\n\nPredicted\n\nto default?\n\nDefaulted?\n\nNo  Yes\n\nFALSE 9629  229\n\nTRUE    38  104\n\n⬆ ⬆\n\nFalse positive rate = 38/(38+9629)\n\nTrue positive rate = 104/(104+229)\n\n1-Specificity\n\nAKA: Power, Sensitivity\n\n \n\n \n\nSuppose we had a test that came back positive for each person whose \n\npredicted to default, sort of like a medical test that comes back positive for \n\neach person who is predicted to have cancer. We could measure the success \n\nof such a test using the true positive rate. \n\n \n\nThis is saying out of all the people who really did default, what fraction were \n\nwe successful at predicting? \n\n \n\nAnother way to phrase the true positive rate is the power or the sensitivity of \n\nthe test. \n\n \n\nWe could also look at the false positive rate. This is looking at out of all people \n\nwho did not default, what fraction were we unsuccessful at predicting. It's \n\ncalled a false positive because the test came back positive, predicting that they \n\ndefaulted, but it was false. That is, they really didn't default. \n\n \n\nAnother way to phrase the false positive rate is as one minus the specificity of \n\nthe test.  \n\n \n\n \n\n \n\n \n\n16 \n\n\n\n\n", "Power and False Positive Rates at Different Thresholds\n\n> all_predicted[1:3]\n\n[1] 0.001215781 0.001154122 0.010201310\n\n> table(all_predicted > 0.5, Default$default)\n\nNo  Yes\n\nFALSE 9629  229\n\nTRUE    38  104\n\nFalse positive and true positive \n\nrates at 0.5 threshold.\n\nWhat about rates at multiple thresholds?\n\n \n\n \n\nThe Receiver-Operator Characteristic curve, or ROC curve, lets us summarize \n\nthe trade off between the true positive rate and false positive rate for a given \n\nmethod. Each point on this curve represents a different probability threshold. \n\nFor example, the threshold of 0.5 which we saw in the previous slide has a \n\nhigh specificity, meaning a low false positive rate.  \n\n \n\nThis means that among the people who don't default, we're very unlikely to \n\npredict that they will default. However, it has a mediocre sensitivity, meaning \n\nthat we miss a lot of the people who do default. If we change our probability \n\nthreshold to 0.25, we're now predicting that more people overall will default. \n\nThis increases our sensitivity, because we're catching more of the people who \n\ndefault, but at a cost of decreasing our specificity.  \n\n \n\n \n\n \n\n \n\n17 \n\n\n\n\n\n\n", "ROC Curve\n\nlibrary(pROC)\n\nmy_roc = roc(response=Default$default, \n\npredictor=all_predicted)\n\nplot.roc(my_roc)\n\nEach point represents \n\na different threshold\n\np = 0.25\n\np = 0.5\n\n \n\n \n\nThe receiver operator characteristic curve, or rock curve, lets us summarize \n\nthe trade off between true positive rate and false positive rate for a given \n\nmethod. Each point on this curve represents a different threshold. \n\n \n\nFor example for our logistic regression model, if we wanted a specificity of 0.9, \n\nwe could get it by using a threshold of 0.05. In that case, our sensitivity would \n\nbe 0.84. By changing the threshold, we could increase the sensitivity but at the \n\nexpense of decreasing the specificity or vice versa.  \n\n \n\n \n\n \n\nNotes: \n\n \n\nlibrary(pROC) \n\nmy_roc = roc(response=Default$default,  \n\n \n\n   predictor=all_predicted) \n\nplot.roc(my_roc) \n\n \n\n \n\n \n\n \n\n \n\n \n\n18 \n\n\n\n\n\n\n", "Interpreting ROC curves\n\nBetter models have ROC curves \n\ncloser to upper-left corner\n\n• Area under curve (AUC) close to 1\n\nGray line (\" = $) represents a \n\nmethod that does no better than \n\nrandom guessing\n\n \n\n \n\n \n\n \n\n \n\n19 \n\n\n\n\n\n", "Using ROC Curves to Compare Methods\n\n \n\n \n\nIn practice, ROC curves are rarely used to look at just one model at a time. \n\nThey're especially good for comparing different models or methods. For \n\nexample, when we're using logistic regression to predict whether a person will \n\ndefault on their credit card debt, a model that uses student and balance as \n\npredictor variables has an Area Under the Curve, or AUC, of 0.95, very close to \n\n1. In comparison, a model that uses student and income as predictor variables \n\nhas an area under the curve of 0.54, barely better than random guessing. \n\n \n\nNotes: \n\nR Code: \n\nresults = data.frame(sensitivity = c(my_roc$sensitivities, my_roc2$sensitivities), \n\n                     specificity = c(my_roc$specificities, my_roc2$specificities) \n\n                    ) \n\n \n\n# Add a column to specify the model. \n\n# While we're at it, compute the false positive rate: \n\nresults <- results %>% \n\n  mutate(model = c(rep(\"Student + Balance\", length(my_roc$thresholds)), rep(\"Student + \n\nIncome\", length(my_roc2$thresholds))), \n\n         false_positive_rate = 1 - specificity \n\n         ) \n\n \n\n# Make the graph: \n\nresults %>% \n\n  gf_line(sensitivity ~ false_positive_rate, color =~ model) %>% \n\n  gf_abline(slope = 1, intercept = 0, col = \"gray\") \n\n \n\n  \n\n \n\n \n\n \n\n \n\n20 \n\n\n\n\n\n\n", " \n\nNot just for logistic regression\n\n• May need to do extra work to get the predicted probabilities (not \n\njust classifications)\n\n \n\n \n\nROC curves are not just for logistic regression. They can be used to compare \n\nany methods for classification as long as they result in a continuous \n\nmeasurement such as predicted probabilities. Depending on the method, you \n\nmay need to do some extra work to get the predicted probabilities rather than \n\njust the predicted classifications of yes or no, 1 or 0.  \n\n \n\nFor example, to get the ROC curve for k-nearest neighbors shown here, we \n\nneed to start by using the argument prob equals true to get the predicted \n\nprobabilities from the KNN function. Then we need to extract the probability \n\nattribute from that set of predictions, but these predicted probabilities are the \n\nprobability of belonging to whatever category is the predicted classification. So \n\nwe then need to use an ifelse statement to take 1 minus those probabilities if \n\nthe predicted classification was no. This converts the predicted probabilities \n\ninto the probability of default rather than the probability of either default or \n\npaying off the debt. If you forget to do these steps and just use the predictions \n\nof the classifications, you'll end up with a ROC curve for KNN that looks \n\npiecewise linear and has an area under the curve that's much lower than what \n\nit should be.  \n\n \n\nNotes: \n\npredicted = knn(train = x_train, \n\n                  test  = x_test, \n\n                  cl = train_set$default, \n\n                  k = 41, prob = TRUE) \n\n \n\nprob = attr(predicted, \"prob\") \n\n \n\n \n\n21 \n\n\n\n\n\n\n\n\n\n\n\n\n", " \n\nprob_of_default = ifelse(predicted == \"No\", \n\n                           1-prob, prob) \n\nall_predicted_knn[groupii] = prob_of_default \n\n \n\n \n\n \n\n22 \n\n", " \n\nQuestion 1 \n\n \n\n \n\n \n\nAnswer is at the end of this transcript \n\n \n\n \n\n \n\n \n\n \n\n23 \n\n\n", " \n\nCost functions\n\nDefaulted?\n\nPredicted\n\nto default?\n\nNo\n\nGood (100)\n\nFALSE\n\nTRUE Not good (-10)\n\nYes\n\nVery bad (-1000)\n\nGood (10)\n\n \n\n \n\nAnother way to assess a model for classification is with a cost function. This \n\ncan be especially effective if the two types of errors, false positives and false \n\nnegatives, are not equally bad. For example, suppose we're planning to offer a \n\ncredit-limit increase to all of our customers who are not predicted to default.  \n\n \n\nIn that case, being in the upper left-hand corner of the confusion matrix, \n\npeople who don't default and are not predicted to default, is good. We might \n\nassign that a score of 100 to represent the amount in dollars that we expect to \n\nearn from additional interest on the raised credit limit for those customers. \n\nSimilarly, being in the lower right-hand corner of the confusion matrix is also \n\ngood. This represents customers who do default and who are predicted to \n\ndefault. We won't offer these customers a credit limit increase, but we might \n\nassign this a score of 10, representing the amount in interest we expect to gain \n\non their current debt before they default.  \n\n \n\nBeing in the lower left-hand corner of the confusion matrix is not so good. \n\nThis represents a false negative, meaning the customer isn't going to default, \n\nbut we predict that they will default. We might assign this a score of negative \n\n10 to represent the loss of customer goodwill from refusing a credit-limit \n\nincrease to a good customer.  \n\n \n\nHowever, the upper right-hand corner of the confusion matrix is very bad. This \n\nis a false negative, where we fail to identify a customer who is going to default \n\non their debt. We might assign this a score of negative 1,000 to represent the \n\namount of money we expect to lose if we offer a credit-limit increase and the \n\nperson doesn't pay it back.  \n\n \n\n \n\n \n\n24 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", " \n\nComputing the cost function\n\nDefaulted?\n\nPredicted\n\nto default?\n\nNo\n\nGood (100)\n\nFALSE\n\nTRUE Not good (-10)\n\nYes\n\nVery bad (-1000)\n\nGood (10)\n\n \n\n \n\nTo use our cost function to assess our model, we start by using the matrix \n\nfunction to input the cost function into R. Here, the first argument is a vector \n\nof the numbers we assigned listed in order going down the columns. So 100 is \n\nfollowed by negative 10. And the second argument is nc equal to 2 to say that \n\nwe want two columns in our matrix. Then we compute the confusion matrix as \n\nusual. And we multiply the confusion matrix by the cost.  \n\n \n\nNotice that when we use a regular old star or asterisk to do the multiplication, \n\nthe matrices get multiplied element by element. So the first entry in the cost \n\nmatrix gets multiplied by the first entry in the confusion matrix, which is what \n\nwe want here. The sum of the entries in the resulting matrix gives us the total \n\ncost associated with the predictions that this model made. In this case, we let \n\npositive numbers represent good outcomes. So we want the total cost to be as \n\nlarge as possible.  \n\n \n\nNotes: \n\n \n\ncost = matrix(c(100,-10, -1000, 10), nc = 2) \n\ncost \n\n \n\nconf_matrix = table(all_predicted > 0.5, \n\n \n\n \n\nconf_matrix \n\n \n\n \n\nconf_matrix * cost \n\n \n\nsum(conf_matrix * cost) \n\nDefault$default) \n\n \n\n \n\n25 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", " \n\nOptimize the total cost \n\n \n\n \n\nYou can then test out different models to find the one that optimizes the total \n\ncost based on the numbers you chose for how good or bad each entry in the \n\nconfusion matrix was. Here I'm using a for loop to iterate over possible values \n\nof the probability threshold from 0.01 up to 0.98. In this case, it turns out that \n\nwe should predict a default for anyone with a probability of default greater \n\nthan 0.11. So we would only offer credit-limit increases to people with a \n\nprobability of 0.11 or lower. I did test out 0.99 here as well. But it turned out \n\nthat, in that case, everybody was predicted to not default. So my confusion \n\nmatrix only had one row, which caused some problems with computing the \n\ncost function.  \n\n \n\nNotes: \n\nprob_thresh = seq(.01, .98, by = .01) \n\ntotal_cost = numeric(length = length(prob_thresh)) \n\n \n\nfor(ii in 1:length(prob_thresh)){ \n\n  conf_matrix = table(all_predicted > prob_thresh[ii], \n\n                      Default$default) \n\n  total_cost[ii] = sum(conf_matrix * cost) \n\n} \n\n \n\ngf_point(total_cost ~ prob_thresh) \n\n \n\n \n\n \n\n26 \n\n\n\n\n\n\n\n", "+\n\nLogistic Regression: Advantages\n\n1\n\n• Produces an interpretable model.\n\n• Estimates the influence of each predictor\n\n2\n\n• Typically better than KNN when true relationship between\n\npredictors and                       is linear.\n\nlog\n\n$(&)\n\n1 − $(&)\n\n\t\n\n \n\nOne of the advantages of logistic regression is that it produces an interpretable \n\nmodel that allows us to estimate the influence of each predictor variable based \n\non the size of its coefficient. We can even do a hypothesis test about whether \n\nthat coefficient is different from zero. \n\n \n\nLogistic regression is also typically better than K nearest neighbors when the \n\ntrue relationship between the predictor variables and the logit is linear.  \n\n \n\n \n\n \n\n \n\n \n\n \n\n27 \n\n\n\n\n\n\n\n\n\n\n", "–\n\nLogistic Regression: Disadvantages\n\n1\n\n• KNN typically better when \n\nrelationship is highly non-linear.\n\n2\n\n•\n\n&'‘s unreliable when classes are \n\nseparable.\n\n \n\n \n\nHowever, K nearest neighbors typically performs better than logistic regression \n\nwhen the relationship is highly non-linear. \n\n \n\nIn addition, the estimated coefficients of logistic regression become unreliable \n\nwhen the classes are separable, as in the example shown here. In a case like \n\nthis, it's likely that linear discriminant analysis or a support vector classifier \n\nwould do a better job. \n\n \n\n \n\n \n\n \n\n \n\n \n\n28 \n\n\n\n\n\n\n\n\n", " \n\nGeneralized Linear Models (GLMs) for Count Data\n\nInteger Data: \n\nError terms don’t have\n\nGaussian distribution. \n\nLinear regression\n\ndoesn’t work well.\n\n \n\n \n\nLogistic regression isn't the only kind of generalized linear model. Another \n\ncommon GLN is Poisson regression. \n\n \n\nFor example, here we have a graph of the number of matings a male elephant \n\nexperiences as a function of its age. I've included a jitter on the matings which \n\nadds a small random amount to each point. So that two elephants with the \n\nsame age and number of matings, don't show up as exactly the same circle on \n\nthe graph. \n\n \n\nIn this data set, elephants that are between 30 and 35 years old tend to have \n\nbetween zero and four matings. Whereas elephants that are between 40 and \n\n45 years old, tend to have between zero and nine matings. \n\n \n\nSo as elephants age increases, their mean number of matings increases, but so \n\ndoes the variance on the number of matings. Often an increase in variance as \n\nthe predictor variable increases is a good sign to try log transforming the Y \n\nvariable. \n\n \n\nHowever in this case, the number of matings has to be an integer value. You \n\ncan't have an elephant that mates 3.5 times. That means that our error terms in \n\nthe regression are not going to have a Gaussian distribution. \n\n \n\nSo a simple linear regression, either with or without a log transformation on \n\nthe Y variable, is not going to be a good fit. \n\n \n\n \n\n \n\n \n\n29 \n\n\n\n\n\n\n\n\n", "Notes: \n\n \n\nThe elephant data set can be found in the R package gpk. See the gpk \n\ndocumentation for more information on the data sets it contains. For more \n\nabout how to analyze the elephant data set, see these notes on Applied Linear \n\nRegression (Poisson_Reg_part1_4pp.pdf). \n\n \n\n \n\n30 \n\n \n\n \n\n \n\n\n\n\n\n", "Poisson Regression\n\nFits a straight line to log(y)\n\nAvoids predicting counts < 0\n\n \n\n \n\nInstead, we can use Poisson regression. This fits a straight line to the log of Y \n\nwith errors that are distributed according to a Poisson distribution. This avoids \n\npredicting count values, in this case, the count of the number of matings, as \n\nbeing less than 0.  \n\n \n\nNotes: \n\n \n\nfit_elephant = glm(Number_of_Matings ~ Age_in_Years, \n\n                   data = elephant, family = \"poisson\") \n\n \n\n \n\n \n\n \n\n \n\n31 \n\n\n\n\n\n\n\n", "Summary One\n\nLogistic regression is a good choice for 2-category classification \n\nproblems when: \n\n• you want an interpretable model \n\n• the classes are not separable\n\n• the relationship between predictors and                           could be \n\nlog\n\n\t\n\napproximately linear\n\n!(!)\n\n1 − !(!)\n\n \n\n \n\n \n\n32 \n\n \n\n \n\n \n\n \n\n\n\n\n\n\n\n", "Summary Two\n\n• ROC curves summarize the relationship between true positive rate \n\nand false positive rate for many different values of a threshold.\n\n• Higher ROC curves indicate better methods.\n\n \n\n \n\n \n\n \n\n \n\n33 \n\n\n\n\n", " \n\nQuestion 1 Answer \n\n \n\n \n\n \n\n34 \n\n\n"]], ["c:\\users\\mjols\\documents\\ds uwec courses\\740 data mining\\lesson3\\ds740_lesson3_presentation2.pdf", [" \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nImportant note: Transcripts are not substitutes for textbook assignments. \n\n \n\n\n\n", " \n\n \n\n \n\n\n", " \n\nUsing too many predictor variables in your linear regression results in a model with very \n\nlow bias but very high variance. This means that the model is great at fitting your \n\ntraining data but terrible at predicting new data points. And it's also difficult to \n\ninterpret. \n\n \n\n \n\n \n\n\n", " \n\n \n\nOne simple step to avoid having too many predictor variables is to check for collinearity \n\nor multi-collinearity, which refers to when two or more predictor variables are highly \n\ncorrelated. This can inflate the standard error for your estimated regression coefficients, \n\nwhich results in larger P values for testing whether your individual coefficients are \n\ndifferent from 0. \n\n \n\nOne way to check for collinearity is by looking at a matrix of scatter plots using the pairs \n\nfunction in R. Exam this matrix for any sign of a linear relationship between two of the \n\nvariables. For example, in this data set, the total enrollment at colleges is highly \n\ncorrelated with the number of full-time undergraduates.  \n\n \n\nNotes: \n\n \n\nlibrary(ISLR) \n\nmy_colleges = College[-96,] # Remove an observation with an unrealistic graduation rate \n\n(> 100%) \n\nmy_colleges = my_colleges[ ,c(4, 7,11, 15,18)] # Examine a subset of variables so the \n\nscatterplots aren’t too small to read \n\npairs(my_colleges) \n\n \n\n \n\n \n\n\n", " \n\nA correlation plot is another good way to quickly check for correlations among variables. \n\nLook for predictor variables whose correlations are close to either positive 1 or negative \n\n1.  \n\n \n\n \n\n \n\n \n\n\n", " \n\nSome correlations that you see in the correlation graph are not a cause for concern. For \n\nexample, a variable always has correlation 1 with itself. So you don't need to worry \n\nabout the large number of red circles you see on the main diagonal of the correlation \n\ngraph.  \n\n \n\nIt's also expected to have high correlation between a variable and its transformation. \n\nFor example, if you were using a variable salary and decided to log transform it to \n\nreduce its right skew, then we expect to have high correlation between those two \n\nvariables. This isn't really a surprise, so it's not really a concern. Often, you were going \n\nto remove the original salary variable from the model anyway because of its skew.  \n\n \n\nFinally, strong correlations either positive or negative with the response variable are \n\ngood because those predictors will help us make good predictions. So for example, if \n\nwe're trying to predict enrollment, then we should be happy to see a large number of \n\npredictor variables with strong correlations with enrollment.  \n\n \n\n \n\n\n", " \n\nThe correlation plot and matrix of scatter plots can tell you about the collinearity \n\nbetween two variables, but it can't tell you about multicollinearity among three or more \n\nvariables. For example, the total enrollment at a college might be only weakly correlated \n\nwith the number of data science majors. But if you start to include the number of \n\nbiology majors, business majors, psychology majors, and English majors, then the \n\nassociation starts to increase  \n\n \n\n \n\n\n", " \n\n \n\nTo check for multi-collinearity, we can use the variance inflation factor, which is 1 over 1 \n\nminus the coefficient of determination that we get from regressing a particular \n\npredictor variable as a function of all the other predictor variables, ignoring the \n\nresponse variable. \n\n \n\nA variance inflation factor of 10 or more indicates that 90% or more of the variation in \n\nthis predictor variable can be explained by the other predictor variables. So in this case, \n\nyou may want to remove that predictor variable or replace it by its residuals from that \n\nregression. \n\n \n\n \n\n \n\n\n", " \n\n \n\nFor example, suppose our goal was to predict graduation rate based on a college's \n\nenrollment, number of full-time undergraduates, cost of books, and student-faculty \n\nratio. We would start by using a linear model to predict graduation rate like usual, and \n\nthen we would use the VIF function applied to the output from that linear model.  \n\n \n\nIn this case, we see that both enrollment and the number of full-time undergraduates \n\nhave very high variance inflation factors. That makes sense because we expect that the \n\nnumber of full-time undergraduates is closely correlated with a school's enrollment. This \n\nmeans that enrollment and number of full-time undergraduates are giving us mostly the \n\nsame information. So we could remove one of them from our model.  \n\n \n\nI'd probably remove number of full-time undergraduates because it has a slightly higher \n\nvariance inflation factor. This would decrease our collinearity, which would tend to \n\ndecrease the standard errors on our coefficient estimates in our linear model. And that \n\nin turn is likely to give us smaller p values. However, if we remove the number of full-\n\ntime undergraduates from our model, we are removing information only some of which \n\nis duplicated by including the variable enrollment.  \n\n \n\n \n\n \n\n\n", " \n\n \n\nAn alternative is to regress the number of full-time undergraduates on the other \n\npredictor variables and then replace it by its residuals. What this does is it extracts all of \n\nthe information that's in the number of full-time undergraduates that's not already \n\ncontained in the other predictor variables.  \n\n \n\nThe way to do this is by creating a temporary linear model where the number of full-\n\ntime undergraduates is the response variable and using all the other predictor variables \n\nas the predictor variables, so everything except for graduation rate, which was the \n\noriginal response variable.  \n\n \n\nIf we look at the summary of this temporary linear regression model, we find that it has \n\na multiple R squared of 0.9335, which corresponds to a variance inflation factor of 15, \n\njust as we saw by using the VIF function. We can then take our temporary regression \n\nmodel dollar sign residuals and store them as a new column in our data set and remove \n\nthe original number of full-time undergraduates. Then we can proceed with fitting our \n\nlinear regression model for graduation rate. You'll notice that the variance inflation \n\nfactors of the resulting model are much lower.  \n\n \n\n \n\n \n\n \n\n\n", "Self-Assessment Question 1 \n\n \n\n \n\nAnswer is at the end of this transcript \n\n \n\n \n\n \n\n\n", " \n\n \n\n \n\n\n", " \n\n \n\nThis slide represents a video/screencast in the lecture. The transcript does not \n\nsubstitute video content. \n\n \n\nLet's investigate whether multicollinearity is likely to be an issue when predicting sales \n\nat a student run cafe. I've already read in the data, and I'm using the function \n\nmake.names to take all of the variable names and replace the spaces with periods. That \n\nwill make them easier to refer to later on in our analysis.  \n\n \n\nThis data set contains 22 variables. So a pairs plot of scatter plots ends up being pretty \n\nhard to read. We could make scatter plots of subsets of variables at a time, but I'm going \n\nto make a correlation plot instead. I've started by using the select if function from the \n\ndplyr package to select only the numeric variables for which we're able to compute the \n\ncorrelations. Then I'll use the cor function to compute the correlation matrix of pairwise \n\ncorrelations between each pair of numeric variables. And I'll make the correlation plot \n\nusing the cor plot function from the cor plot library.  \n\n \n\nI'm using the function brewer.pal for Brewer palette to get the color scheme for my \n\ngraph. And this is from the R color Brewer package. I'd like to reverse this color scheme \n\nso that red represents large values, correlations close to positive 1, and blue represents \n\nlarge negative values close to negative 1. To me it's more intuitive for red to be high and \n\nblue to be low.  \n\n \n\nHere's our correlation plot. Here we're trying to predict sales. So we're happy to see \n\nthat sales is reasonably strongly correlated with a number of other variables in the data \n\nset. The other correlations that aren't with sales might be of concern, indicating \n\nmulticollinearity. It looks like coffees is strongly negatively correlated with the maximum \n\n\n", "temperature on a given day. That makes sense that people would tend to want to buy \n\ncoffee when it's cold out.  \n\n \n\nWe also see that the maximum temperature is correlated with t, which represents the \n\nday on which the data was gathered. That makes sense also because this data set was \n\ngathered during one semester, the spring semester, at a university. So day is going to \n\nroughly start in January and end in April. It makes sense that it would be positively \n\nassociated with the temperature.  \n\n \n\nWe do see some groups of variables that are correlated with each other, such as the \n\nnumbers of items sold of different types and the numbers of items wasted of different \n\ntypes. So it's going to be worth looking at the variance inflation factors to see if there's \n\nenough multicollinearity there that we need to be concerned.  \n\n \n\nThe correlation graph and pairs graph of scatter plots can only tell us about relationships \n\nbetween pairs of numeric variables. So it's a good idea to also look at the data dictionary \n\nand think about possible interpretations to get a sense of what relationships might exist \n\namong the categorical variables and among groups of more than two variables at a \n\ntime.  \n\n \n\nFor the cafe data set, right away you might notice that we might have an association \n\ngoing on among these first four variables. Let's check this out using R. We can get a \n\nbetter sense of what's in these variables by looking at the first few entries using the \n\nhead function.  \n\n \n\nSo we see that the first few entries of the t variable are just the numbers from 1 to 6. \n\nAnd by looking at the first few entries of a table of dates, it looks like each date only \n\noccurs once. We can verify this by looking at the length of the date variable and then \n\nreducing that to just the unique elements of date and comparing the length of that \n\nvector. We see that both of those vectors have length 42. So yes, indeed, the date \n\nvector is just a list of 42 different dates with no dates repeated. So it looks like the t \n\nvariable is just a numeric version of that same information. So we don't gain anything by \n\nincluding both of those variables.  \n\n \n\nBetween these two variables, t is definitely the better choice to use in our model. That's \n\nbecause date would be treated as a categorical variable. Recall that linear regression, \n\nlike many other modeling types, will automatically one-hot encode categorical variables, \n\nmeaning that our 42 different dates will be turned into 41 different 0, 1 indicator \n\nvariables. That would massively increase our number of predictor variables, which \n\nwould lead to overfitting. In fact, because we had as many different dates as we had \n\nobservations in our data set, we would end up with a model that perfectly predicted the \n\nsales of the data in our training set but which was useless for predicting the sales on any \n\nnew data.  \n\n \n\n", "Back in the data dictionary, it looked like day code was simply a numeric representation \n\nof the day of the week. We can verify this by making a table of these two variables \n\ntogether. And here we can see that yes, 5 represents Friday, 1 represents Monday, and \n\nso on. So these two variables are giving us the exact same information as well. We don't \n\nneed both of them in our analysis.  \n\n \n\nBetween these two variables, I actually am going to choose to use day of week instead \n\nof day code. That's because this is a linear regression model. So using day code as a \n\nnumeric variable would model the effect of the day as being either linearly increasing or \n\nlinearly decreasing as you go from Monday to Friday throughout the week. In \n\ncomparison, using day of week, which is a categorical variable, will allow us to estimate \n\nthe effect separately of Mondays, Tuesdays, Wednesdays, Thursdays, and Fridays. So \n\nmaybe at this university because of the way classes are scheduled, sales are higher on \n\nMondays, Wednesdays, and Fridays than they are on Tuesdays or Thursdays. Using a \n\ncategorical variable, day of week, would better allow us to model that.  \n\n \n\nSo let's go ahead and fit a linear regression model of sales as a function of all the other \n\nvariables in the data set except for date and day code. When we look at our model, we \n\nsee that two of our variables are listed as NA, meaning that our model was not able to \n\nestimate a coefficient for these two variables. This happened because our model was \n\ncompletely singular. That is, these two variables were perfectly predictable based on the \n\nother predictor variables in our model.  \n\n \n\nIf we look back at our data dictionary, we can see that the total soda and coffee variable \n\nprobably is closely related to the variables sodas and coffee. And similarly, the total \n\nitems wasted is probably closely related to some of these other variables, such as the \n\namount of fruit cup waste, the amount of cookies waste, and so on.  \n\n \n\nLet's investigate whether these are in fact perfectly associated. Here I'm using the \n\nmutate function from dplyr to create a temporary new variable called waste sum. So I'm \n\nadding together all of the other waste variables. And then I'll use gf point to make a \n\ngraph of total items wasted and waste sum. So we can see if total items wasted is, in \n\nfact, exactly the same as the sum of the other wastes.  \n\n \n\nHere we can see that the variables are perfectly on a line with each other. So this \n\nexplains the perfectly singular result we got in our linear regression. To keep things \n\nsimple, let's go ahead and use total items wasted and remove the other waste variables \n\nfrom our analysis.  \n\n \n\nTo investigate total soda and coffee, we could make a graph just like we did for the total \n\nwaste, but I'll try it a little bit differently. I'm still using mutate, but this time I'm creating \n\na variable called is equal, which will be equal to true if sodas plus coffees is exactly equal \n\nto the total soda and coffee. Then I'll group by the value of that new variable is equal, \n\n", "and I'll count how many rows are equal to true, meaning sodas and coffees added \n\ntogether gives the total soda and coffee.  \n\n \n\nHere we see that all 42 rows in the data set give a value of true, meaning that we can \n\nperfectly predict the value of total soda and coffee based on the value of sodas and \n\ncoffees. So for the purposes of our analysis, let's remove total soda and coffee and we'll \n\njust use sodas and coffees. So here's our new linear model where we're predicting sales \n\nbased on all of the other variables except for the ones that we decided to exclude for \n\nreasons of multicollinearity. Linearity  \n\n \n\nHere we can see that we no longer have any NAs in our model. But some of our \n\nstandard errors are still pretty large compared to the size of the coefficient that we're \n\nestimating. This could be a sign of additional multicollinearity that we haven't detected \n\nyet. To investigate this, let's look at the variance inflation factor.  \n\n \n\nHere we see that the variable day of week has a large variance inflation factor of 14.88. \n\nHowever, remember that that was a categorical variable with five possible levels, one \n\nfor each day of the week, meaning that it had four degrees of freedom. This means that \n\na variance inflation factor of 14 isn't quite as bad as it might seem for a variable that \n\nonly had one degree of freedom, such as a quantitative variable.  \n\n \n\nTo get a better comparison among variables with different degrees of freedom, we can \n\nlook at this third column, which contains the variance inflation factor raised to a power \n\nthat depends on the degrees of freedom. Here we can see that day of week isn't looking \n\nso extreme anymore. And sodas has a variance inflation factor of 3.05.  \n\n \n\nGenerally, you want to keep this third column less than the square root of 10, which is \n\nabout 3.16. So the variance inflation factor for sodas isn't quite over that threshold. But \n\nI would be more concerned about sodas than I would be about day of week. If this were \n\nall of the investigation of multicollinearity that I was planning to do with this data set, I \n\nmight consider either excluding sodas or regressing it on the other predictor variables \n\nand then replacing.  \n\n \n\n", " \n\nTo go deeper than checking for multi-collinearity, we need a way of comparing models \n\nto determine where they fall in the trade-off between bias and variance. \n\n \n\n \n\n \n\n\n", " \n\nRecall that the coefficient of determination is 1 minus the residual sum of squares \n\ndivided by the total sum of squares. This is not a good criterion for model comparison, \n\nbecause it can't decrease as predictor variables are added. So if you simply try to choose \n\nthe model that maximizes r-squared, you'll always end up choosing the most \n\ncomplicated model. \n\n \n\n \n\n \n\n\n", " \n\nThe adjusted r-squared takes r-squared and subtracts a penalty based on d, the number \n\nof predictors, not counting the y-intercept. This means that larger values of the adjusted \n\nr-squared indicate a better trade-off between fit to the training data and the number of \n\npredictors. \n\n \n\n \n\n \n\n\n", " \n\n \n\nThe analysis of deviance is a hypothesis test for comparing models. Unlike adjusted r-\n\nsquared and the other criteria for model comparison that we'll discuss in this lesson, the \n\nanalysis of deviance should only be used for comparing nested models, ones in which \n\nthe predictor variables of one model are a subset of the predictor variables of the other \n\nmodel. \n\n \n\nThe null hypothesis is that both models fit the data equally well. The alternative \n\nhypothesis is that the more complex model fits the data significantly better. In this \n\nexample, we get a large P-value, so we retain the null hypothesis that both models fit \n\nthe data equally well. \n\n \n\nSo in this case, there's not any good reason to use the more complex model, which \n\nincreases the variance. So we would prefer to use the simpler model. The analysis of \n\ndeviance can also be used for logistic regression models by including the argument test \n\nequals chi. \n\n \n\n \n\n\n", " \n\nMallows' Cp is another criterion for model comparison. It's based on the residual sum of \n\nsquares plus a penalty term based on d, the number of predictor variables and sigma \n\nsquared hat, the estimated variance of the noise terms in the linear regression. \n\n \n\nA better model will tend to fit the training data well, so it will have a small residual sum \n\nof squares, and it will have a small penalty term. So smaller values of Mallows' Cp \n\nindicate a better model. \n\n \n\n \n\n \n\n\n", " \n\n \n\nAkaike's Information Criterion is equal to 2 times the number of parameters minus 2 \n\ntimes the log likelihood, meaning the probability of observing this data set if the model \n\nwere true. I really like Akaike's Information Criterion, because it's based on the \n\nlikelihood. That means it's easy to apply to other types of models besides linear \n\nregression. \n\n \n\nIf you can write a statistical likelihood for the model, you can compute the AIC. \n\nHowever, AIC also has a special form when it's applied to least squares regression. You'll \n\nnotice that this form is very similar to the form for Mallows' Cp. In fact, in least squares \n\nregression, AIC will always choose the same preferred model as Mallows' Cp. \n\n \n\n \n\n\n", " \n\n \n\nLike Mallows' CP, smaller values of AIC indicate better models. Models with an AIC \n\ndifference of less than about 2 are generally considered to be reasonable alternatives. \n\nFor example, in the models shown here, fit 1 and fit 2 are reasonable alternatives to \n\neach other because their AICs are within 2 of each other, but fit 3 has a much lower AIC. \n\nSo based on AIC, this model is the best trade-off between accuracy and parsimony. \n\nNotes:  \n\n \n\nStyliano, Pickles, and Roberts recommend requiring AIC differences of 6 or more before \n\nrejecting a model: \n\nUsing Bonferroni, BIC and AIC to assess evidence for alternative biological pathways: \n\ncovariate selection for the multilevel Embryo-Uterus model (2013) \n\n \n\n \n\n\n", " \n\nThe Bayesian Information Criterion, or BIC, is similar to AIC except that the penalty for \n\nextra parameters depends on the sample size n. The larger the sample size, the bigger \n\nthe penalty. So BIC tends to select smaller models than either Mallows' Cp or AIC. \n\n \n\n \n\n \n\n\n", " \n\nIt's important to note that all three of these criteria for model comparison involve \n\nconstants which some functions omit. For example, the r functions AIC and extract AIC \n\nproduce very different values for the AIC of the same model. So when you're comparing \n\ntwo models, you should always use the same data set and the same function to \n\ncompute the model comparison criterion for both of the models. \n\n \n\n \n\n \n\n\n", " \n\nBest subset selection refers to a systematic approach for choosing the best possible \n\nmodel. Suppose we have P predictor variables available to us. Then for each possible \n\nnumber of predictors, from 0 up to P, will fit all of the models with that number of \n\npredictors. And we'll choose the one with the lowest residual sum of squares or, \n\nequivalently, the largest coefficient of determination. We'll call this model m sub d. That \n\nthe best model of size d. Then we'll choose the best model from m sub 0 up through m \n\nsub p using one of our criteria for model selection. \n\n \n\n \n\n \n\n\n", " \n\n \n\nWhen d is equal to 0, there's only one model to consider-- the null model, in which each \n\npoint's response value is predicted to equal the mean response value for the whole \n\nsample. When d is equal to p, we also only have one model to consider-- the full model, \n\nin which all of the predictor variables are used. \n\n \n\nAlso notice that Mallows' Cp, AIC, BIC, and adjusted r-squared all rely on the residual \n\nsum of squares. They only differ based on how they penalize the number of predictor \n\nvariables. That means that for any given number of predictor variables d, these four \n\ncriteria will all agree on which model is best. They'll only differ in their decisions in the \n\nlast step of best subset selection when we're comparing models of different sizes. \n\n \n\n \n\n\n", " \n\nTo perform best subset selection in R, we can use the regsubsets function. If we do this \n\nusing the argument method equals exhaustive, regsubsets will consider all possible \n\nmodels with numbers of variables between 1 and nvmax. In fact, exhaustive is the \n\ndefault value for the method argument. So we could omit this argument entirely and \n\nconsider all possible models.  \n\n \n\n \n\n \n\n\n", " \n\n \n\nIf there are more than 50 predictor variables, we can use a stepwise approach either by \n\nchanging the method argument in the regsubsets function or by using the step function, \n\nwhich is based on AIC. With this approach, instead of performing an exhaustive search \n\nof all possible models, at each stage we're doing one of two things-- either adding the \n\nmost useful predictor variable to the model in forward stepwise regression or removing \n\nthe least useful variable in backwards stepwise regression.  \n\n \n\nThe argument seqrep for regsubsets or direction equals both in the step function allows \n\nus to do either forward or backward stepwise regression at each stage. This approach is \n\nnot guaranteed to achieve the best possible model, but it can be useful if you have a lot \n\nof predictor variables. Forward stepwise regression can even be used if the number of \n\npredictor variables is larger than your sample size.  \n\n \n\nNotes: \n\n \n\nregfit.full = regsubsets(Grad.Rate ~ ., data = College, \n\n          method = \"seqrep\", nvmax = 17) \n\n \n\nfit = lm(Grad.Rate ~ ., data = College) \n\nsfit = step(fit, direction = \"both\") \n\n \n\n \n\n\n", " \n\n \n\nThe step function is nice because it automatically compares models of different sizes, \n\nwhereas reg subsets will just report the best model of each size and then leave the \n\ncomparison between different sizes up to you. However, reg subsets has better handling \n\nof missing data, and it also treats each level of a factor as a separate indicator variable. \n\n \n\nThis is nice, for example, if you have sales data from different days of the week, and you \n\nwant to consider the possibility that sales are different on Fridays but the same Monday \n\nthrough Thursdays. \n\n \n\n \n\n\n", " \n\n \n\n \n\n\n", " \n\n \n\nThis slide represents a video/screencast in the lecture. The transcript does not \n\nsubstitute video content. \n\n \n\nLet's perform Best Subset Selection on our cafe data, using an exhaustive search. To do \n\nthis, we'll apply the Regsubsets function, which is in the Leaps library. The syntax of the \n\nRegsubsets function is very similar to the lm function, which we use for linear \n\nregression. The only additional argument here is nvmax. This sets the maximum number \n\nof predictor variables you want to include in any of the models. \n\n \n\nI generally set this equal to the maximum number of predictor variables that you have in \n\nyour dataset, remembering to add additional predictor variables for any additional \n\nlevels of factor variables that you may have. If we simply look at the name of the object \n\nwhere we stored the regsubsets output, we get a summary of the analysis. And it's \n\ntelling us that including all levels of the factor level variables, we had 16 variables. So \n\ndouble checking this against the nvmax that we used, we see that yes, we included all \n\npotential variables. \n\n \n\nIf we want to know more about the models that it considered, we can use plot regfit. \n\nAnd here we get a plot summarizing each of the variables that were possible predictor \n\nvariables along the x-axis and the BIC of different models along the y-axis. And a dark \n\nrectangle indicates that that variable was included in that model. A lower BIC is better, \n\nwhich is indicated up at the top of this graph. \n\n \n\nSo here you can see that the best model with only a single predictor variable included \n\nthe variable wraps.sold. But that model had a fairly high BIC, so it's down at the bottom \n\n\n", "of the graph. The model that was slightly better than that included all the possible \n\npredictor variables, the full model, with a BIC of about negative 16. If we want to change \n\nthe y-axis, we can instead use the scale argument inside our plot function. Here I'll plot \n\nthe adjusted r squared. And here we get that in terms of adjusted r squared, the full \n\nmodel actually didn't do too badly. It's shown up here. We can extract more information \n\nby using the Summary function. And here I'll store the summary results inside a new \n\nobject. \n\n \n\nThen we can extract information from that summary, using the dollar sign to extract \n\npieces of that object. And I'll extract the BIC. Here we get the BIC of each model for one \n\npredictor variable up to 16 predictor variables. So we can plot this as a function of the \n\nnumber of variables. Here we see that as the number of predictor variables included in \n\nthe model increases, the BIC of the best possible model of that size first decreases and \n\nthen increases. We can use the which.min function to pick out which element of this \n\nvector of BICs is the lowest. It's the 10th element, which indicates that the model with \n\n10 variables had the lowest BIC, which matches what we saw on the graph. \n\n \n\nTo extract the best model in terms of Mallows's Cp, we can use which.min again but \n\nextracting the Cp part of the summary object. Or we can extract the best model in terms \n\nof adjusted r squared, using which.max. Based on adjusted r squared, the model with 12 \n\npredictor variables was the best. But Mallows's Cp in this case agreed with BIC that the \n\nmodel with 10 predictor variables was the best. We can see what the coefficients of that \n\nmodel were by using the function coef with arguments regfit, which was our regsubsets \n\nobject, and 10, the number of variables we wanted to be included in that model. So \n\nhere we see the variables included in the model and the estimated coefficients for each \n\nof those. So for example, the number of sodas is positively associated with the total \n\nsales for the day. \n\n \n\nNotes: \n\n \n\nNumber of variables = # of quantitative predictors + (# of levels – 1) for each categorical \n\npredictor. \n\nDataset: \"Student-run Café Business Data,\" submitted by Concetta A. DePaolo and David \n\nF. Robinson, Indiana State University. Dataset obtained from the Journal of Statistics \n\nEducation (http://www.amstat.org/publications/jse). Accessed 2 August 2016. Used by \n\npermission of author. \n\n \n\n \n\n\n", " \n\n \n\nReg subsets returns criteria for model comparison computed on the training data. \n\nHowever, if we want to estimate model performance on new data points, it's a good \n\nidea to use cross-validation. To do this, we need to be able to predict the response value \n\nfor each of the data points in the validation set. Unfortunately, there's not a built-in \n\npredict function for models that are the output from reg subsets. So we need to write \n\nour own. \n\n \n\nThe function shown here will do this by first extracting the appropriate columns from \n\nthe validation set and then multiplying them by the coefficients of the model. \n\n \n\n \n\n\n", " \n\n \n\nA common occurrence is that as we increase the number of variables in the model, the \n\ncross-validation error will drop sharply at first and then level off. So we get a large \n\nnumber of models with very similar cross-validation error rates. In the example shown \n\nhere, the model with nine variables had the lowest cross-validation error, but other \n\nmodels were very similar. \n\n \n\nIt might well be that if we repeated the cross-validation with a different random seed, \n\nwe might find that the model with 8 variables or 10 variables was best. In this case, it \n\ndoesn't really make sense to say that 9 is the absolute for certain optimal number of \n\nvariables. \n\n \n\nInstead, it makes more sense to say that all of the models with a mean squared error \n\nwithin a certain range, say, one standard error of the minimum, are near optimal. So we \n\nmight prefer to choose the most parsimonious model, the one with the fewest \n\npredictors. \n\n \n\n \n\n\n", " \n\nHere's the formula for computing the standard error of the cross-validation error. \n\nNotice that the first formula, cv sub j, is exactly like computing the mean squared error, \n\nbut we're doing it for a single fold of our k-fold cross-validation. \n\n \n\n \n\n \n\n\n", " \n\n \n\n \n\n\n", " \n\n \n\nThis slide represents a video/screencast in the lecture. The transcript does not \n\nsubstitute video content. \n\n \n\nTo perform cross-validation for best subset selection I'm going to start by defining a \n\npredict function that will work for objects that are the output of the regsubsets \n\nfunction. Then, I'll perform cross-validation as usual, setting n equal to the number of \n\nrows in the cafe data set, and setting my value of n groups.  \n\n \n\nThis is a fairly small data set, so I'm going to use leave-one-out cross-validation by \n\nsetting n groups equal to n, the size of the data set. Then I'm creating my groups vector, \n\nsetting my seed, and creating the cvgroups as a random permutation of the groups \n\nvector. Because we're doing leave-one-out cross-validation, we wouldn't really need to \n\nrandomize the order of the groups vector. We could have just said cvgroups equals the \n\nvector from 1 up to n. I've defined the variable nvar equal to 16, the maximum number \n\nof variables that I might want to include as predictors in my model.  \n\n \n\nAnd here, instead of a vector called all_predicted, I'm defining a matrix called \n\ngroup_error. So this is set up so that each row will represent one fold, and each column \n\nwill represent one model size, or number of variables.  \n\n \n\nThen we have our for loop, iterating over the folds as usual. We're setting up groupii, \n\ntrain_data and test_data as usual, and then I'm using regsubsets to fit my model.  \n\n \n\nHere we have data equals train_data, and nvmax equals nvar, which was 16. Here's \n\nwhere the difference comes in. Instead of simply computing the predictions for our test \n\nset, I'm now going to do a second for loop inside the first one, that's going to iterate \n\n\n", "over the different model sizes, from 1 up to 16. Inside that for loop, I'll make the \n\npredictions using my new predict function that I just wrote.  \n\n \n\nRecall that I wrote a function called predict dot regsubsets. Here I'm just calling predict. \n\nThat's because this is a generalized function, where if you call the function predict it will \n\nautomatically look at the object type of the first argument, see that it's a regsubsets \n\nobject, and then look for a function called predict dot regsubsets.  \n\n \n\nSo according to the way the arguments were set up in the function that I wrote, this first \n\nargument is our regsubsets object. The second argument is alldata equals cafe, the \n\nentire data set. Subset equals groupii, so the trues and falses defining which elements \n\nare in the test data set. And id equals jj, so that's the value of how many variables I want \n\nto include as predictors.  \n\n \n\nSo this gives us our predictions for the test set. Normally we would just want to store \n\nthis value in our all_predicted variable. But in this case, we're going to do it a little bit \n\ndifferently, and that's because we're going to want to compute the standard error of \n\nour mean squared errors. And the best way to do that is to compute the mean squared \n\nerror separately for each of the folds.  \n\n \n\nSo here I'm going to take the test data and the sales column, subtract my predictions, \n\nand then square it and take the mean to compute the mean squared error for this \n\nparticular fold and this number of variables, jj. And then I'm storing that value in the row \n\nthat corresponds to this fold, and the column that corresponds to this number of \n\nvariables in my group_error matrix.  \n\n \n\nSo now I'm going to come down here and actually compute the overall mean squared \n\nerror. So our for loop was filling up this group_error matrix with each entry being the \n\nmean squared error from one fold. Now I want to take the mean over all the folds for a \n\ngiven number of variables. So I want to take the mean over all the columns. So that's \n\nindex two of my group_error matrix.  \n\n \n\nThat will give us our mean squared error overall, which should be a vector of 16 \n\nnumbers. So one mean squared error for each number of variables. And then we can \n\ngraph that as a function of the number of variables.  \n\n \n\nIn this case, we see that the mean squared error starts out high, and then drops down to \n\na minimum at two variables. If we want to programmatically determine which number \n\nof variables is best, we can use the function which dot min, which tells us that indeed \n\ntwo variables is the best model in terms of mean squared error.  \n\n \n\nIn this case, it's pretty clear that having two variables both gives us a very low mean \n\nsquared error and a very parsimonious model. But what if the best model had been one \n\nof these up here, with 12 or 14 variables? In that case, we might want to check if there \n\n", "was a more parsimonious model with a mean squared error within one standard error \n\nof the minimum.  \n\n \n\nWe would do that by first computing the standard errors. We do that by taking the \n\nstandard deviation of each column of the mean squared errors, and then dividing by the \n\nsquare root of the number of folds. Then we'll use this to define a threshold by taking \n\nthe mean squared error overall from the lowest mean squared error model, and then \n\nadding the standard error of that model. And we want to take the most parsimonious \n\nmodel whose overall mean squared error is less than one standard error above the \n\noverall mean squared error of the lowest model.  \n\n \n\nSo here we see that models 2, 10, and 12 through 16 all had mean squared errors within \n\none standard error of the minimum. So we would want to pick the most parsimonious \n\nmodel from this list, which is the model with two variables.  \n\n \n\nTo see which two variables are in this best model, and what their coefficients were, we \n\nwant to go back to the model from the full data set, not the models from the cross-\n\nvalidation, which gave us a different model for each fold. So that was our regfit object \n\nfrom the previous video. \n\n \n\nNotes: \n\n \n\nDataset: \"Student-run Café Business Data,\" submitted by Concetta A. DePaolo and David \n\nF. Robinson, Indiana State University. Dataset obtained from the Journal of Statistics \n\nEducation (http://www.amstat.org/publications/jse). Accessed 2 August 2016. Used by \n\npermission of author. \n\n \n\n \n\n\n", " \n\n \n\n \n\n\n", "Question 1 Answer \n\n \n\n \n\n\n"]], ["c:\\users\\mjols\\documents\\ds uwec courses\\740 data mining\\lesson4\\ds740_lesson4_presentation1.pdf", [" \n\n \n\n \n\n \n\n \n\nImportant note: Transcripts are not substitutes for textbook assignments. \n\n1 \n\n \n\n \n\n \n\n \n\n\n\n", " \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n2 \n\n\n", " \n\n \n\n \n\n \n\nBayes' rule is a 253-year-old concept introduced by Thomas Bayes, an English minister \n\nfrom the 1700s. It forms the foundation for a way of thinking about statistics that is an \n\nalternative to the frequentist or traditional viewpoint of statistics. It has gained \n\nprominence, particularly in the last couple decades, due to the increased computational \n\npower that provide the ability to compute many of the posterior distributions.  \n\n \n\nIts basic form can be stated in terms of events or sets of occurrences, which we'll label A \n\nand B. Our goal is to obtain the conditional probability of A given B. And we can do so by \n\nusing other probabilities, including opposite, so to speak, conditional probabilities to \n\ncompute it.  \n\n \n\nThis formula can be thought of as the posterior probability of A-- that is, given or \n\nknowing that B occurs. And it's computed as a ratio, with the top of the ratio being the \n\nconditional probability of B given A, the opposite conditional probability, times the prior \n\nprobability of A. And the denominator is the sum of conditional probabilities times \n\npriors over the possible prior options.  \n\n \n\nNotes: \n\n \n\nFor a great basic introduction of Bayes' rule, see B. \n\nEfron's lecture and paper (with abstract) for MAA Distinguished Lecture, \"A 250-Year \n\nArgument (Belief, Behavior and the Bootstrap).\" \n\n \n\n \n\n \n\n3 \n\n\n", " \n\n \n\n \n\n4 \n\n \n\n \n\nA common example is to consider a test for a rare disease. Such a situation will allow us \n\nnot only to discuss Bayes' rule, but also how observed versus predictive classifications \n\nare characterized. Relabeling appropriate to context, we let D represent the event, \n\nperson has disease, and P represent the test for that disease is positive.  \n\n \n\nPrior probabilities-- that is, not having any test results involved-- tell us that there is a \n\nprobability of 1 in 1,000 of having the disease, and thus a probability of 0.999, or 999 \n\nout of 1,000, of not having the disease.  \n\n \n\nConditional probabilities of the test accuracies are also known and shown in the list. The \n\nprobability of a positive diagnosis for a person, or given that a person has the disease, is \n\n0.995. The probability of a negative diagnosis, given that a person does not have the \n\ndisease, is 0.95. We'll discuss these test accuracies later.  \n\n \n\n\n", " \n\n \n\nQuestion 1 \n\n \n\n \n\nAnswer is at the end of this transcript \n\n \n\n \n\n \n\n \n\n5 \n\n\n", " \n\n \n\n \n\n \n\nWe will now reframe the situation in this example as count data. Consider a population \n\nthat has a million individuals in it. Only a tenth of a percent, or 1,000 of these \n\nindividuals, have the disease, according to the previous probability, since it is a rare \n\ndisease.  \n\n \n\nSo in turn, note that this means that 999,000 individuals do not have the disease. And \n\nthis is where the counts and probabilities of individuals getting the disease based on \n\ntheir true situation come into play. Out of the 1,000 individuals with the disease, 99.5%, \n\nor a count of 995 individuals, will test positive, and five, the remaining five, will test \n\nnegative.  \n\n \n\nSimilarly, out of the 999,000 individuals without the disease, 95%, giving us a count of \n\n949,050 individuals, will test negative. The remainder, for a count of 49,950, will test \n\npositive.  \n\n \n\nAt the bottom of this table, we'll total the numbers of individuals who test negative and \n\npositive. And what is very interesting is taking a look at the numbers who test positive. \n\nSo, note that only about 2% of the individuals who test positive are actually ones with \n\nthe disease. And that's because there are so many people who don't have the disease, \n\nsome of them will test positive, about 5%, and that gives us a big count.  \n\n \n\n \n\n \n\n6 \n\n\n", " \n\n \n\n \n\n7 \n\n \n\n \n\nSo, seeing this in equation form, if we fill in the probabilities to the formula for Bayes' \n\nrule, we calculate that only 2% of those who test positive actually have the disease. This \n\nmay seem surprising. But going back to the rarity of the disease overall, along with the \n\nvarious accuracies of the test, we can help lay out the reason.  \n\n \n\nSo, out of the 1,000 people who have the disease, we have a very accurate test for \n\ndiagnosing the disease in people who actually have it. And so we get almost all of them-- \n\nthat is, 995 people-- added in as positives. And these are the true positives.  \n\n \n\nBut there is a very large group of people who do not have the disease, 999,000. And \n\nsince the test is reasonably accurate, or specific, for diagnosing no disease, most of \n\nthem will test negative. But there's still a decent chunk, about 5%, who test positive. \n\nThis works out to be almost 50,000 people. And this larger count is the set of false \n\npositives. And so note that our probability as computed via the formula works out to be \n\nthe same thing as taking the 995 true positives out of the total number of positives.  \n\n \n\n\n", " \n\n \n\n \n\n \n\nSticking with the idea of classifying into two classes, we connect the relationship \n\nbetween predicted and observed classes through terminology. Letting N denote-- capital \n\nN-- the count of observed negatives, capital N* represent count of predicted negatives. \n\nWe'll also let P and P* be the corresponding counts for the positives.  \n\n \n\nThen inside the table, we can break down the entries as true negatives, or TN, false \n\npositives, false negatives, and true positives. And so we see that the false positives and \n\ntrue positives add up to the total number of predicted positives, as the true negatives \n\nand false negatives add up to the total number of predicted negatives.  \n\n \n\nNotes: \n\n \n\nN: observed negatives \n\nP: observed positives \n\nN*: predicted negatives \n\nP*: predicted positives \n\nn: total population \n\nTN: true negatives from prediction \n\nFN: false negatives from prediction \n\nFP: false positives from prediction \n\nTP: true positives from prediction \n\n \n\n \n\n \n\n8 \n\n\n", " \n\n \n\n \n\n9 \n\n \n\n \n\nThe rates of true identification are the focus in talking about classification models. \n\nNotably, we define specificity to be the true negative rate-- that is, the number of \n\nactually identified negatives over the total number of negatives. And similarly, \n\nsensitivity is the true positive rate.  \n\n \n\nSo to summarize, in this situation, we have a rare disease affecting only 1 in 1,000 \n\npeople with a very high sensitivity in terms of a true positive rate and a high specificity \n\nin terms of a true negative rate. But because it is rare, we wind up with a lot more \n\npositive tests from those who are actually negative. So sensitivity and specificity \n\ncontinue to be very important to know, but results must be interpreted in light of the \n\nnumber of occurrences of each outcome, each class.  \n\n \n\n\n", " \n\n \n\nQuestion 2 \n\n \n\n \n\nAnswer is at the end of this transcript \n\n \n\n \n\n \n\n \n\n10 \n\n\n", " \n\nQuestion 3 \n\n \n\n \n\nAnswer is at the end of this transcript \n\n \n\n \n\n \n\n \n\n11 \n\n\n", " \n\n \n\n \n\n12 \n\n \n\n \n\nHow might such a test be run? A diagnosis that is a positive or negative test result could \n\nbe based off of some physiological measurement, such as white blood cell count, blood \n\nsugar reading, level of pancreatic enzymes, et cetera. For the application of linear \n\ndiscriminant analysis, we will focus on continuous predictors. In the next lecture, we will \n\ndiscuss why the predictors must be continuous, as well as what additional assumptions \n\nare made about the distribution of the predictors.  \n\n \n\nWe will describe extension to more classes in the next lecture. And so we'll state our \n\ngeneric goal here. We wish to classify a response Y into one of K groups or classes based \n\noff one or more continuous predictor variables X.  \n\n \n\n\n", " \n\n \n\n \n\n13 \n\n \n\n \n\nGoing back to our disease example, suppose the situation is such that we are attempting \n\nto diagnose acute pancreatitis as an illness. One indicator of this illness is lipase level in \n\nunits per liter. Suppose the distribution shown in the picture below represent healthy \n\nindividuals, seen in the black line distribution, versus individuals with acute pancreatitis, \n\nas represented by the distribution with the red line.  \n\n \n\nNotice that these two distributions are of the lipase levels. And so we're going to use \n\nthat X predictor variable to classify individuals into either healthy versus acute \n\npancreatitis.  \n\n \n\nNote also that there is not much overlap. For example, looking at the value X equals \n\n250, only a few, a small proportion, of the healthy individuals have a lipase level higher \n\nthan that value. And only a very few individuals with acute pancreatitis have a lipase \n\nlevel below that. So, if these were the distributions, lipase level appears that it would do \n\na good job distinguishing between healthy individuals and those with acute pancreatitis.  \n\n \n\n \n\n\n", " \n\n \n\n \n\n14 \n\n \n\n \n\nWe take a further look at two images to help us understand how this test is performing. \n\nThe first image is the previously introduced ROC curve. And because of the area \n\nunderneath the curve being very close to 1, we note that this is a highly accurate test. \n\nAnd that can be further illustrated by looking at the sensitivity and specificity values that \n\nwe could get for different choices for the X value as a cutoff to classify individuals.  \n\n \n\nAnd if we take an X value anywhere between about 250 to 350, we notice that both the \n\ngreen line, representing sensitivity, and the blue line, representing specificity, will have \n\nvalues quite close to 1 for X's anywhere over that range. There will be a trade-off, of \n\ncourse, as there is between sensitivity and specificity. But over that range, it's a pretty \n\neven match between the two. \n\n \n\n\n", " \n\n \n\n \n\n15 \n\n \n\n \n\n \n\n \n\n\n", "Question 1 Answer \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n16 \n\n \n\n \n\n\n", "Question 2 Answer \n\n \n\n \n\n \n\n \n\n \n\n17 \n\n \n\n \n\n\n", " \n\nQuestion 3 Answer \n\n \n\n \n\n18 \n\n \n\n\n"]], ["c:\\users\\mjols\\documents\\ds uwec courses\\740 data mining\\lesson4\\ds740_lesson4_presentation2.pdf", [" \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nImportant note: Transcripts are not substitutes for textbook assignments. \n\n1 \n\n\n\n", " \n\n \n\n \n\n2 \n\n", " \n\n \n\n \n\n \n\n3 \n\n\n", " \n\nRecalling the goal of discriminant analysis. In this lecture, we will explore the connection \n\nfrom a single predictor used through Bayes' rule and certain assumptions to classify a \n\nqualitative response y into K classes. To do this, we'll use the so-called Bayes' classifier, \n\nwhich looks at the class whose conditional probability, given the predictor is highest. \n\nThis further allows us to define intervals in the x space, in which each class is the \n\nprediction from the classifier. These intervals are marked apart by what are known as \n\nBayes' decision boundaries. \n\n \n\n \n\n \n\n \n\n4 \n\n\n", " \n\n \n\nWe begin with Bayes' rule as expressed in the previous example framework. The \n\noutcome disease is represented with a d and has two possibilities-- has or has not-- and \n\ntest outcome is represented as a p for positive or an n for negative. We rewrite this \n\nmore generically. Rather than the response having necessarily two possible outcomes, \n\nwe let the response-- variable y-- be one of capital K possible classes. \n\n \n\nThen for some continuous variable x, the conditional probability of y being in class K \n\ngiven x can be written as in the equation displayed, simply by substituting in the \n\nresponse values for the outcome of-- or the observed disease value and substituting the \n\nx values in place of the positive or negative. While this starts to get messy walking \n\nthrough the development serves two purposes. First, we can observe the need to have \n\ncertain assumptions based on the process. Second, clarification of the word linear in the \n\nmethod name is found from the form of the classifier decision boundaries. \n\n \n\n \n\n \n\n5 \n\n\n", " \n\nUsing notation that better represents the form of the inputs, we use pi sub K for the \n\nprobability-- the unconditional probability-- of the outcome y. The response y being in \n\nclass K. We let f sub K evaluated at x represent a density for the continuous x. That is, \n\nthe probability density function of x at a given value of y equals K. And Similarly, we let p \n\nsub K of x represent the probability that y equals K given x equals value little x. \n\nSubstituting these in for the original probability notation, we get the form from the book \n\nthat shows p sub K of x as a function of the f sub K and pi sub K. \n\n \n\n \n\n \n\n \n\n6 \n\n\n", " \n\nThe rather typical assumption of a normal distribution for the continuous variable is \n\nemployed here. That is, the density f sub K is identified to be normal. And for each class \n\nK may have a different mean, mu sub K. Initial derivation will additionally assume all \n\nclasses have the same standard deviation sigma. The normal density function can then \n\nbe written out and substituted into the previous formula. \n\n \n\n \n\n \n\n \n\n7 \n\n\n", " \n\nAs a result, we get a final workable form of p sub K of x, the conditional probability that \n\ny is in class K given x. While it is messy to maximize directly, we can equivalently \n\nmaximize the log of p sub K of x. And this works out to a simpler form. That is, maximize \n\nover all K some constant times x plus another constant, where those constants involve \n\nthe mu sub K and pi sub K pieces that vary with K, and sigma squared, which is constant \n\nfor all K. Now, note that this is a linear function in x, which is very simple to maximize \n\nand provides the reason for the name of the method linear discriminant analysis. \n\n \n\n \n\n \n\n \n\n8 \n\n\n", " \n\n \n\nSo we have constants, but we don't know their values. Hence, we must estimate them \n\nfrom sample information. Occasionally, we might have some prior information about \n\nthe pi sub Ks. But typically, we will have to estimate the class proportion pi hat sub K as \n\nsimply the proportion of each class in the sample. And we will come up with K \n\nestimators. \n\n \n\nAdditionally, we will get K more estimators in order to come up with an estimate for \n\neach class mean, which is simply calculated as the sample average of the x sub i's in that \n\nclass. The overall variance is estimated with a typical sample variance formula. But using \n\nmeans corresponding to each x sub i's class and dividing by n minus K to obtain an \n\nunbiased estimator. In total, we are estimating K plus K plus 1 parameters. \n\n \n\n \n\n \n\n9 \n\n\n", " \n\n \n\nWhile such computation can be done by hand, it is painstaking. Happily, there is a very \n\nuser friendly function in the mass package called simply lda, all in lowercase letters. \n\nTypically, only two inputs will be needed. Formula is the predictive model to be used \n\nand data references the appropriate set of information. We will be using the \n\nshapiro.test function to test the assumption of approximate normality of data and the \n\nbartlett.test function to test the assumption of equal standard deviations.  \n\n \n\nNotes: \n\n \n\nR Manual Pages: lda function, shapiro.test function, bartlett.test function. \n\n \n\n \n\n \n\n10 \n\n\n\n\n\n", " \n\n \n\nA very famous historical data set is the Iris dataset. It was analyzed by Fischer all the \n\nway back in 1936 and is used to illustrate LDA, or Linear Discriminant Analysis. The Iris \n\ndata includes 50 flowers from each of the three species and four size measurements-- \n\ntwo of the petal and two of the sepal-- of the plants. We will visually explore \n\nclassification and run the lda function to get decision boundaries and predict \n\nclassification in our video in R. \n\n \n\nNotes: \n\n \n\nLink: Iris data set \n\n \n\n \n\n \n\n11 \n\n\n", "Question 1 \n\n \n\n \n\nAnswer is at the end of this transcript \n\n \n\n \n\n \n\n \n\n12 \n\n\n", "Question 2 \n\n \n\n \n\nAnswer is at the end of this transcript \n\n \n\n \n\n \n\n \n\n13 \n\n\n", " \n\n \n\nThis slide represents a video/screencast in the lecture. The transcript does not \n\nsubstitute video content. \n\n \n\nToday, we're going to be working with the LDA function, which is contained in the MASS \n\nlibrary. And since we'll also be working with a ROC function example, we'll open up the \n\nlibrary for working with ROC functions. We don't have any specialty or additional \n\nlibraries for assumption checking. And for a couple of the visuals, we will work with \n\nggformula and dplyr commands.  \n\n \n\nWe're going to be taking a look at the iris data today, which is already included in the \n\nusual Base R. So we'll begin by, if you have not seen this data set before, taking a look at \n\nthe size, as well as the number of levels of the categorical variable that we'll be using as \n\nthe response species. Species has three levels-- setosa, versicolor, and virginica-- which \n\nare the species of iris plants.  \n\n \n\nWe could take a look at a Base R visual of one of the variables. Here, I chose petal \n\nlength, but you could try some of the other numeric predictor variables. So if we take a \n\nlook at iris, the data set itself, we see that our categorical response variable will be \n\nspecies. And we have four potential predictor variables. We are currently looking at \n\npetal length, which is the most discerning or discriminating.  \n\n \n\nAnother visual that we could use is a ggformula function called gf boxplot. That just \n\nmakes it a little bit easier to fill in colors, for example. And so I've picked out three colors \n\nto denote the levels of species.  \n\n \n\nAnd if you take a look at the values of petal length, setosa, obviously, is very far \n\nseparated in terms of the petal lengths. Versicolor and virginica have some overlap. And \n\nso one question might be, how well can we actually distinguish one of those two, let's \n\n \n\n14 \n\n\n", "say, virginica? Thus, I might define or might be working with an indicator type response. \n\nAnd in this case, if I take a look at virginica, it's a 01 indicator of the virginica species.  \n\n \n\nAnd so for our very first application, in terms of applying the LDA function fit, we might \n\nwish to just work with that, either virginica or not type of response. And part of the \n\nreason for this is I'd like to illustrate an interesting characteristic of the ROC functions \n\nwhen working with LDA. So if I first include a ROC curve with the response as this 01 \n\nindicator value, and my predictor as simply the numeric values of petal length. And I \n\nmake a ROC curve and plot it. When I get the same sort of values from the posterior \n\nprobabilities of an LDA fit, I'm going to wind up with exactly the same curves.  \n\n \n\nSo let's talk through this. For my iris data, I model this 01 virginica response on petal \n\nlength using linear discriminant analysis LDA function. I store that. And then when I take \n\na look at the predictions of that model fit using my iris data, if I just run that code, I get \n\nquite a bit of output. And so sifting through this is really quite important.  \n\n \n\nThe most important piece that we want from this is the posterior probability. And its the \n\nposterior probability for the level zero and one, where one indicates the virginica \n\nspecies. So it's this second column, the posterior probabilities for level one that I want. \n\nSo I'll take a look at just the posterior probabilities and pull off the second column of \n\nthose.  \n\n \n\nWhen I do that, and store those posterior probabilities as ldaprob, using those as my \n\npredictor values is going to give me exactly the same curve because of the linear \n\nrelationship with the predictor values. So if you kept your eye on the plot, it didn't \n\nchange at all. So that's just an example of linear discriminant analysis with a two class \n\ncategorical response variable.  \n\n \n\nWe actually want to apply LDA, linear discriminant analysis, with the three class species \n\ncategorical variable. Not much to adjust here, specifically if we run the LDA function on \n\nthe model only, now using species with the three categories setosa, versicolor, and \n\nvirginica as the response, and petal length is the predictor, we'll still get a fit similar to \n\nwhat we saw before. And if we made a prediction, that is we re-predict using LDA fit the \n\nsame data used to fit that model, we would get the posterior probabilities as we saw \n\nbefore. But now, posterior probabilities for setosa, versicolor, and virginica classes.  \n\n \n\nOK, so I would like to figure out, because as well as part of this prediction, I get the \n\nclassification output. So just taking a look at the class as predicted on the data used to \n\nfit the model, I'm going to store that in cred class and compare that to my original \n\nresponse, which I'm going to label as y. Making a table, we see that there's a very \n\naccurate method here, or at least it's very accurate when applied to the data used to fit \n\nthe model. Two of the actual versicolors are misclassified as virginicas. And six of the \n\nactual virginicas are misclassified as vericolors, as we can see from this table, where the \n\ny, the actual observed response, is in the rows, and the predictions are the columns.  \n\n \n\n15 \n\n", " \n\nIf we summarize this, we get an error of only about 0.0533. Now, we're not comparing \n\nthis to any other models at this time, but that's a pretty low error rate. We should make \n\nsure that we're getting an honest idea of our error rate, though. And so we're going to \n\nuse cross-validation to proceed with this.  \n\n \n\nWe'll set up our storage locations for the predictions. We'll set up our CV groups, as \n\nwe've done in prior lessons. And then we'll loop through those CV groups consecutively \n\ndesignating each group or setting up a logical vector to indicate each group, getting the \n\ntrain set, the test set. And then fitting the LDA models previously specified only to the \n\ntraining set and predicting the outcome.  \n\n \n\nNow, as with any process, I highly recommend going through the steps inside the loop \n\nbefore you try to run the loop to make sure it works. So what I've done is I start by \n\nsetting ii equals one, and running to make sure I'm designating the right things with \n\neach line. If I wasn't sure what was showing up in group ii, that it's a logical vector, I \n\ncould check. Does seem to be a logical vector.  \n\n \n\nI define my train and test set. Might be nice to see that those are of the approximate \n\ncorrect sizes, 90% and 10% of the data, respectively. All that looks good.  \n\n \n\nI store, again inside the loop, eventually LDA fit ii. And I might just want to take a look at \n\nis that actually appear to have the characteristics of an LDA fit? Yeah, seems to work \n\nwell. Interestingly, now the prior probabilities of the groups, because we have a random \n\ntrain set, they're not all quite evenly split, but pretty close to evenly split in the train set.  \n\n \n\nIt is possible to predict and simply store the predicted values for the test set in their \n\nclass four. But to make this compatible with later visuals, we'll actually turn these \n\npredicted values into the text with the AS character function. So if I run that section, it's \n\njust the same values, but as text. And store those as my predictions.  \n\n \n\nAnd then, finally, put those predictions in the correct locations on my full storage vector \n\nfor predictions. So if I take a look at the CV pred class, it should contain about 10% of \n\nthe locations filled in with the corresponding predicted text. Well, it looked like \n\neverything ran correctly. So let me run this through my CV groups.  \n\n \n\nAnd then let's tabulate. So I'm going to table my previously defined labeled y, my \n\nresponse values, my y versus the cross-validated predicted class. And here, we actually \n\nstill get just a 5% error rate. But now, there are three actual versicolors that are \n\nmisclassified as virginica and five actual virginicas that are misclassified as versicolor. So \n\nthe mix is a little bit different when we do this honest prediction, but the \n\nmisclassification rate is right about where we saw it previously when we reclassified the \n\ndata used to fit the model.  \n\n \n\n \n\n16 \n\n", "The last step here is to see if it's reasonable to use LDA. And we know that one of the \n\nassumptions that we're working with in LDA is that we can use a common variance for \n\nthe groups, as well as have normality of the predictor values. When taking a look at the \n\npetal length as my xvar values, I'm going to subset using the brackets only where our \n\nspecies is setosa of the xvar and store that as xSetosa. And do the same sort of thing for \n\nversicolor and virginica.  \n\n \n\nRunning normality tests on the setosa petal lengths, it's close to concerning, but that's \n\nbecause of a slightly more discreet nature of the values. With versicolor, it's very \n\nreasonable. Normality is reasonable. There's no strong evidence against it. And same \n\nthing with virginica. So normality looks great.  \n\n \n\nHowever, if I run a test about looking for evidence of non-constant variance, that is \n\nhighly, highly significant with a very, very small p-value. And piping the iris data set \n\ngrouping by species and then summarizing would be a way to take a look at those \n\nstandard deviations. Equivalently, you could simply take the standard deviation of \n\nxSetosa and of xVersicolor and of xVirginica and find the standard deviations. So we can \n\nsee that setosa's standard deviation is much smaller, and thus, equal variances are not \n\nreasonable. We really want to use QD, and we'll talk about that in the next presentation.  \n\n \n\nThat concludes the application of LDA. As a bonus, I have included some visualizations of \n\nthe boundaries. I won't go through the coding in detail other than to note that I am \n\ncomputing a variety of statistics among the different groups, the different classes of \n\nspecies, and then visualizing the linear decision boundaries.  \n\n \n\nSo if I were to make a histogram of the values in each of those three groups-- setosa, \n\nversicolor, and virginica-- I also plotted a linear decision boundary that we look to \n\nmaximize, so up to a petal length of 2.861. That is about right here. The highest in the \n\nvertical axis value of this linear bound is for setosa. Between 2.861 and 4.906. The \n\nhighest line is that for versicolor. And finally, for petal lengths above 4.906, we would \n\nclassify as virginica because the line for virginica is higher after that point.  \n\n \n\nThe next visual is an attempt to correspond this to our original goal functions, that is the \n\nposterior probabilities that you saw, both as part of the output and which had a \n\ncomputation on a prior slide. This is a little bit more complex, but I believe more visually \n\nunderstandable in terms of what we're actually looking to do. So let's take a look at this \n\nplot, which shows the corresponding cut points. But now, what you see is the posterior \n\nprobabilities.  \n\n \n\nAnd this makes more sense, at least in an understanding in a probabilistic sense, \n\nbecause we're talking about a posterior probability very close to one. And then it \n\nsuddenly drops off for setosa. And suddenly, the posterior probability for versicolor is \n\nhigher and close to one and then drops off a little bit. And the reason it's dropping off \n\nless sharply, more gradually, is because there is some overlap between the versicolor \n\n \n\n17 \n\n", "and virginica species. And after the value 4.906, the posterior probability for virginica is \n\nhighest.  \n\n \n\nThe visuals are also summarized and displayed in the next slide. I hope that that helps \n\nexplain some of the correspondence between maximizing the linear function with the \n\nmaximization of the posterior probability. \n\nNotes: \n\n \n\nSee the online course for a downloadable R file containing the set of commands used in \n\nthis demonstration. \n\n \n\n \n\n \n\n \n\n18 \n\n", " \n\nHere, we show the plots demonstrated in the recording on the previous slide, with the \n\ntop plot showing where the posterior probability values are maximized and identifying \n\nthe cut points, and then showing that the linear functions are maximized with cut points \n\noccurring at the same locations. This explains why the method is called \"linear \n\ndiscriminant analysis\"-- that is, because the functions that are maximized are simply \n\nthought of and worked with as linear functions.  \n\n \n\n \n\n \n\n19 \n\n\n", " \n\n \n\n \n\n \n\n20 \n\n\n", " \n\n \n\n \n\n \n\n21 \n\n\n", "Question 1 Answer \n\n \n\n \n\nQuestion 2 Answer \n\n \n\n \n\n22 \n\n \n\n \n\n\n\n"]], ["c:\\users\\mjols\\documents\\ds uwec courses\\740 data mining\\lesson4\\ds740_lesson4_presentation3.pdf", [" \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nImportant note: Transcripts are not substitutes for textbook assignments. \n\n1 \n\n\n\n", " \n\n \n\n \n\n2 \n\n", " \n\n \n\n \n\n \n\n3 \n\n\n", " \n\n \n\nReferring back to the goal of discriminant analysis, in which we want to predict a \n\nqualitative response y into one of k classes using one or more predictors x, we focus on \n\nthe more than one predictor of x. So we introduce how multiple predictors can be \n\nimplemented into our process. If we have p predictors, each data point now has p \n\npredictor values, which we arrange in a vector x sub i. Each predictor has a mean value \n\nwithin the class k and we arrange these p means in a vector mu sub k, one for each class \n\nk. \n\n \n\nSo we have k vectors of length p describing the means. The covariance is measured by a \n\nmatrix with variances on the diagonal and covariances off diagonal. While we allow the \n\nmu of k's to be different for each class, we begin by assuming the covariance matrices \n\nare the same across all groups. In this case, the estimators we will derive are very similar \n\nto what we saw in the one predictor case. And the forms look very similar. \n\n \n\n \n\n \n\n4 \n\n\n", " \n\n \n\nAs part of the similarity, the goal function which we are trying to maximize, is essentially \n\nthe same simply with involving vectors rather than individual values now. So when we \n\ncome up with a simplified goal function, it looks extremely similar to what we had \n\nderived in the previous lecture, only now instead of a single x, we have a vector x. And \n\ninstead of a single mu sub k, it is a mu sub k as a vector. And instead of a single sigma \n\nsquared, we have a covariance matrix, p by p in dimension. \n\n \n\nThis resulting goal function is, again, a linear combination of elements of the X factor, \n\nand thus this is still accurately described as a linear discriminant analysis. \n\n \n\n \n\n \n\n5 \n\n\n", " \n\n \n\nEstimating the various parameters for the goal function is still necessary, but again takes \n\na very similar form. The x sub i's and mu of k's are now simply vectors of length p. \n\nHowever, this does mean we have quite a few more parameters to estimate. We still \n\nwind up with k estimators, pi hat sub k. But since each mu hat sub k is estimating p \n\nparameters, we have k times p total parameters for the mean functions that need to be \n\nestimated. \n\n \n\nAnd estimating the parameters along the diagonal entries and above on the covariance \n\nmatrix, results in p times p plus 1 divided by 2 parameters to be estimated. In total this \n\ngives us, k plus p divided by 2, the quantity times p plus 1, parameters to be estimated. \n\nThis is decidedly more than we had previously. \n\n \n\n \n\n \n\n6 \n\n\n", " \n\n \n\nAs often is the case, it is unrealistic to assume that the covariance matrices are the \n\nsame, although we can run a test check. What would be the consequence of dropping \n\nthat assumption? The goal, which is to maximize a goal function, remains the same, but \n\nsolving it is more arduous. Luckily, we can simply look at the form of the final goal \n\nfunction. We note it is more involved now within x transpose times a matrix times x \n\nvector involved in the first term. And this results in a linear combination of x squared \n\nvalues. \n\n \n\nAs well as the next term involving a linear combination of the x values. So this is now a \n\nquadratic function of the predictor values. \n\n \n\n \n\n \n\n7 \n\n\n", " \n\nMaximizing the goal function for quadratic discriminant analysis has the same \n\nestimators for parameters pi sub k and vectors mu sub k. It additionally allows separate \n\ncovariance matrices sigma sub k for each class k, where the formula is computed using x \n\nvectors only from within class k. In each of those k matrices, we are estimating p times p \n\nplus 1 over 2 parameters, and so the total number of parameters that must be \n\nestimated are k plus k times p plus k times the number of parameters for each \n\ncovariance matrix.  \n\n \n\n \n\n \n\n8 \n\n\n", " \n\n \n\nWhen comparing LDA and QDA as possible models, computationally there is realistically \n\nno difference, but practically there is a trade off. As a model, QDA is more flexible in \n\nthat it accommodates different covariance structures for the different classes. But this \n\nin turn requires estimating more parameters, which means that we would need more \n\ndata and we add complexity to our model. The mathematical consequence of this is also \n\na trade off- the well-known bias variance trade-off. \n\n \n\nQDA can have a lower bias, but it does have a higher variance because of the added \n\ncomplexity. If you have enough data to estimate all parameters of a QDA, test to see if \n\nthere is significant evidence of a difference between covariance matrices. If there is, you \n\ncan proceed with the QDA. Otherwise, you can go ahead and use LDA. \n\n \n\n \n\n \n\n9 \n\n\n", " \n\n \n\nAs you've already been introduced to the lda function, the use of the qda function in the \n\nMASS package should feel very comfortable. Inputs and values of the function work in a \n\nvery similar manner. We will need to be able to test assumptions for the two tests that \n\nwe are working with.  \n\n \n\nFor the normality assumption, we will be using the Henze-Zirkler's multivariate \n\nnormality test, which was previously introduced in a 705 class. This is applied using the \n\nmhz function from the mvnormalTest package. To test the constant covariance \n\nassumption, we'll be using the BoxM function from the MVTests package. We'll take a \n\nlook at this in an example in today's video. \n\n \n\n \n\nNotes: \n\n \n\nR Manual Pages: lda function, qda function, hzTest function, boxM function. \n\n \n\n \n\n \n\n \n\n10 \n\n\n", " \n\nWe'll be revisiting the iris data set. This time working with all four of the predictors to \n\npredict the species. And will be fitting both LDA and QDA models using those models to \n\npredict classifications. And finally compare the models via cross validation methods.  \n\n \n\nNotes: \n\n \n\nLink: Iris data set \n\n \n\n \n\n \n\n \n\n11 \n\n\n", " \n\n \n\nThis slide represents a video/screencast in the lecture. The transcript does not \n\nsubstitute video content. \n\n \n\nWe next apply QDA, Quadratic Discriminant Analysis function, found in the MASS \n\nlibrary. We're also going to be using ROC curves for visualization. And we'll need to open \n\na couple libraries to conduct tests of our assumptions. Additionally, we'll be doing some \n\ndata organization, as well as some visualization.  \n\n \n\nOur data is set up as previously. And we again observe that we've got three levels of our \n\niris species, so of our response categorical variable. Taking a look at the response values, \n\nor the petal length split up by those response values, it is very clear that there are \n\nsizable differences in petal length split up by species.  \n\n \n\nQuadratic discriminant analysis is advisable, since we do not have constant variance \n\nwithin these groups. And so we're going to proceed in that direction. As we've done \n\npreviously, we could specify our CV groups. In fact, this is the exact same specification of \n\nthose cross-validation groups. And we could apply LDA on one predictor.  \n\n \n\nWe actually did this precisely as will be done in this example in the previous recording. \n\nThe difference here is we're going to generalize this so that we can apply both LDA and \n\nQDA, as well as include different potential models-- that is, using different subsets of \n\npredictors.  \n\n \n\nSo I'm going to set my response and store that in y and then check. And there's various \n\nways you can do this. I'm doing an if-else check. I'm actually doing a double if check to \n\nmake sure that my methodapplied is either LDA or QDA so that the modelfit is the \n\nappropriate model.  \n\n \n\n \n\n12 \n\n\n", "Now, as I specified up here, methodapplied is LDA. If methodapplied is LDA, the output \n\nof this will store in modelfit the LDA fit on the specified model, which is just species, \n\nsplit or fit on petal length for the iris data.  \n\n \n\nSo running this will run this line, since I specified methodapplied to be LDA. Taking a \n\nlook at modelfit, I should see precisely the same output as I saw in the previous \n\npresentation, as well as getting the same output and repredicted error, as well as the \n\nsame cross-validated predictions.  \n\n \n\nThe adjustment has to be made for the cross-validation predictions as well. So if we take \n\na look back at the code from the previous presentation, inside our cross-validation, we \n\nhad simply specified LDA of the particular model we wanted.  \n\n \n\nTo make this a little bit more general purpose, general usage, we now check which \n\nmethod is applied and then apply the corresponding method to the model that we have \n\nchosen. So we are taking this generalization and putting it inside our cross-validation.  \n\n \n\nImportantly, we're only going to fit this to the train set within the cross-validation loop. \n\nEverything else will be the same as before. So it's just this change from one line to these \n\nfour lines to specify the appropriate model. So I run this. I should, again, get the same \n\noutput as we observed in the previous presentation.  \n\n \n\nNow I can store this CV error for model L1. Of course, it seems like a little bit of a level of \n\ncomplexity if that's all we did. But we're actually trying to make this into a-- easier to \n\napply for both QDA and different model specifications. So instead of using LDA, I can \n\nnow use, and in my coding, check for QDA, as well as apply a particular model.  \n\n \n\nSo here, I'm going to apply QDA to the same one-predictor model that we talked about \n\npreviously, run the same lines of code, with an emphasis on getting to the cross-\n\nvalidation error. And it looks very similar in terms of-- we have the same error rate.  \n\n \n\nBut importantly, we do have different classifications here-- that is, different incorrect \n\nclassifications here. And that's because we are fitting or cross-validating a different \n\nmodel. And this was our QDA with one predictor. So I'm going to store that CV error. I'm \n\ngoing to rename it as CVErrorQ1.  \n\n \n\nThe next step would be, instead of just using one predictor, now I'm going to use all four \n\npredictors. So I specify, going back to linear discriminant analysis. And I'm going to fit \n\nspecies on all the remaining predictors. And if you weren't sure what those were, let's \n\ndouble check-- names of iris. We were going to be fitting species on the remaining \n\nvariables. So that is the method and model applied.  \n\n \n\nSo taking iris species, running these same steps as before, again, because we've made \n\nthis generalizable to different models, we can actually just rerun the same set of code. \n\n \n\n13 \n\n", "And it will go through and make sure that we're using the correct model, either LDA or \n\nQDA-- I'm sorry, the correct method, either LDA or QDA, and the correct model on the \n\nappropriate data set.  \n\n \n\nOnce again, store that, in this case as CDErrorL4, and then run through our final method \n\nand model, which is going to be QDA with the response fit on all four predictors. Run \n\nthe same set of code. And make sure we store the result.  \n\n \n\nAnd we now have CV error for each of four models-- LDA with one predictor, QDA with \n\none predictor, LDA with four, and QDA with four. It appears that, because we're looking \n\nat the cross-validated error, or, that is, the honest error, it appears that four predictors \n\nis generally doing a better job-- not dramatically better, but better job at classifying.  \n\n \n\nAnd we might say, well, LDA seems to do slightly better. But it's important to recognize \n\nthat equal covariances are not reasonable. And so we're going to double check \n\nassumptions down here to verify that. So I'm going to define all my xvars by piping iris to \n\nselect just the four x variables.  \n\n \n\nAnd I'm going to pipe iris through filtering-- that is, selecting only the rows that are of \n\nthe species setosa-- and then selecting just the columns that we want-- the columns that \n\nare predictors. So we'll define those four matrices, check for multivariate normality, \n\nwhich, as we said on the previous slide, is reasonable or very close to reasonable. And \n\nthat multivariate normality is needed, or is an assumption, for either LDA or QDA.  \n\n \n\nAnd then we're going to do a check for equal covariance matrices. It is extremely clear, \n\nwhen we take a look at this output with a tiny, tiny p-value, that there is very strong \n\nevidence of a difference in covariance matrices. So QDA is the better option, because \n\nwe do not have equal covariance matrices. And QDA allows for unequal covariance \n\nmatrices.  \n\n \n\nSo that's the application of QDA and multiple predictors. If you'd like, we are now going \n\nto run a ROC with QDA predictions for a two-level response. And that two-level \n\nresponse is going to be an indicator of virginica species. We're going to run both LDA \n\nand QDA on everything except species, of course-- that is, on all the numeric predictors \n\nexcept species.  \n\n \n\nWe can keep track of the probabilities, the posterior probabilities, from each of those \n\nmethods, and then fit a ROC curve for the LDA fit on all four predictors. And that looks \n\nsimilar, but a little bit higher, a little bit area underneath the curve, than on just using \n\none predictor.  \n\n \n\nAnd QDA is fit in the same general way, where our response is the two-outcome \n\nresponse, virginica. And predictor is the posterior probability values. Now, when we plot \n\nthis one, keep an eye on the plots. You'll see that there is an even bigger area under the \n\n \n\n14 \n\n", "curve. And so QDA with all four predictors is the method we would choose to go with in \n\nthis example. \n\n \n\n \n\nNotes: \n\n \n\nSee the online course for a downloadable R file containing the set of commands used in \n\nthis demonstration. \n\n \n\n \n\n \n\n15 \n\n", " \n\n \n\n \n\n \n\n16 \n\n\n", " \n\n \n\n \n\n \n\n17 \n\n\n", " \n\n \n\n18 \n\n\n"]]]